[{"path":"https://ngreifer.github.io/WeightIt/articles/WeightIt.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Using WeightIt to Estimate Balancing Weights","text":"WeightIt contains several functions estimating assessing balancing weights observational studies. weights can used estimate causal parameters marginal structural models. go basics causal inference methods . good introductory articles, see Austin (2011), Austin Stuart (2015), Robins, Hernán, Brumback (2000), Thoemmes Ong (2016). Typically, analysis observational study might proceed follows: identify covariates balance required; assess quality data available, including missingness measurement error; estimate weights balance covariates adequately; estimate treatment effect corresponding standard error confidence interval. guide go steps two observational studies: estimating causal effect point treatment outcome, estimating causal parameters marginal structural model multiple treatment periods. meant definitive guide, rather introduction relevant issues.","code":""},{"path":"https://ngreifer.github.io/WeightIt/articles/WeightIt.html","id":"balancing-weights-for-a-point-treatment","dir":"Articles","previous_headings":"","what":"Balancing Weights for a Point Treatment","title":"Using WeightIt to Estimate Balancing Weights","text":"First use Lalonde dataset estimate effect point treatment. ’ll use version data set comes cobalt package, use later well. , interested average treatment effect treated (ATT). outcome (re78), treatment (treat), covariates balance desired (age, educ, race, married, nodegree, re74, re75). Using cobalt, can examine initial imbalance covariates: Based output, can see variables imbalanced sense standardized mean differences (continuous variables) differences proportion (binary variables) greater .05 variables. particular, re74 re75 quite imbalanced, troubling given likely strong predictors outcome. estimate weights using weightit() try attain balance covariates. First, ’ll start simple, use inverse probability weights propensity scores generated logistic regression. need supply weightit() formula model, data set, estimand (ATT), method estimation (\"glm\" generalized linear model propensity score weights). Printing output weightit() displays summary weights estimated. Let’s examine quality weights using summary(). Weights low variability desirable improve precision estimator. variability presented several ways, important effective sample size (ESS) computed weights, hope close original sample size possible. constitutes “large enough” ESS mostly relative, though, must considered respect constraints, including covariate balance. weights quite high variability, yield ESS close 100 control group. Let’s see weights managed yield balance covariates. nearly covariates, weights yielded good balance. age remained imbalanced, standardized mean difference greater .05 variance ratio greater 2. Let’s see can better. ’ll choose different method: entropy balancing (Hainmueller 2012), guarantees perfect balance specified moments covariates minimizing negative entropy (measure dispersion) weights. variability weights changed much, let’s see gains terms balance: Indeed, achieved perfect balance means covariates. However, variance ratio age still quite high. continue try adjust imbalance, reason believe unlikely affect outcome, may best leave . (can try adding (age^2) formula see changes causes.) Now weights stored W., let’s estimate treatment effect weighted sample. functions lm_weightit(), glm_weightit(), friends make easy fit (generalized) linear models account estimation weights standard errors. can use functions marginaleffects perform g-computation extract treatment effect estimation outcome model. confidence interval treat contains 0, isn’t evidence treat effect re78. Several types standard errors available WeightIt, including analytical standard errors account estimation weights using M-estimation, robust standard errors treat weights fixed, bootstrapping. type described detail vignette(\"estimating-effects\").","code":"library(\"cobalt\") ##  cobalt (Version 4.5.5, Build Date: 2024-04-02) data(\"lalonde\", package = \"cobalt\") head(lalonde) ##   treat age educ   race married nodegree re74 re75    re78 ## 1     1  37   11  black       1        1    0    0  9930.0 ## 2     1  22    9 hispan       0        1    0    0  3595.9 ## 3     1  30   12  black       0        0    0    0 24909.5 ## 4     1  27   11  black       0        1    0    0  7506.1 ## 5     1  33    8  black       0        1    0    0   289.8 ## 6     1  22    9  black       0        1    0    0  4056.5 bal.tab(treat ~ age + educ + race + married + nodegree + re74 + re75,         data = lalonde, estimand = \"ATT\", thresholds = c(m = .05)) ## Balance Measures ##                Type Diff.Un      M.Threshold.Un ## age         Contin.  -0.309 Not Balanced, >0.05 ## educ        Contin.   0.055 Not Balanced, >0.05 ## race_black   Binary   0.640 Not Balanced, >0.05 ## race_hispan  Binary  -0.083 Not Balanced, >0.05 ## race_white   Binary  -0.558 Not Balanced, >0.05 ## married      Binary  -0.324 Not Balanced, >0.05 ## nodegree     Binary   0.111 Not Balanced, >0.05 ## re74        Contin.  -0.721 Not Balanced, >0.05 ## re75        Contin.  -0.290 Not Balanced, >0.05 ##  ## Balance tally for mean differences ##                     count ## Balanced, <0.05         0 ## Not Balanced, >0.05     9 ##  ## Variable with the greatest mean difference ##  Variable Diff.Un      M.Threshold.Un ##      re74  -0.721 Not Balanced, >0.05 ##  ## Sample sizes ##     Control Treated ## All     429     185 library(\"WeightIt\") W.out <- weightit(treat ~ age + educ + race + married + nodegree + re74 + re75,                   data = lalonde, estimand = \"ATT\", method = \"glm\") W.out #print the output ## A weightit object ##  - method: \"glm\" (propensity score weighting with GLM) ##  - number of obs.: 614 ##  - sampling weights: none ##  - treatment: 2-category ##  - estimand: ATT (focal: 1) ##  - covariates: age, educ, race, married, nodegree, re74, re75 summary(W.out) ##                   Summary of weights ##  ## - Weight ranges: ##  ##            Min                                 Max ## treated 1.0000         ||                    1.000 ## control 0.0092 |---------------------------| 3.743 ##  ## - Units with the 5 most extreme weights by group: ##                                             ##               5      4      3      2      1 ##  treated      1      1      1      1      1 ##             597    573    381    411    303 ##  control 3.0301 3.0592 3.2397 3.5231 3.7432 ##  ## - Weight statistics: ##  ##         Coef of Var   MAD Entropy # Zeros ## treated       0.000 0.000   0.000       0 ## control       1.818 1.289   1.098       0 ##  ## - Effective Sample Sizes: ##  ##            Control Treated ## Unweighted  429.       185 ## Weighted     99.82     185 bal.tab(W.out, stats = c(\"m\", \"v\"), thresholds = c(m = .05)) ## Balance Measures ##                 Type Diff.Adj         M.Threshold V.Ratio.Adj ## prop.score  Distance   -0.021     Balanced, <0.05       1.032 ## age          Contin.    0.119 Not Balanced, >0.05       0.458 ## educ         Contin.   -0.028     Balanced, <0.05       0.664 ## race_black    Binary   -0.002     Balanced, <0.05           . ## race_hispan   Binary    0.000     Balanced, <0.05           . ## race_white    Binary    0.002     Balanced, <0.05           . ## married       Binary    0.019     Balanced, <0.05           . ## nodegree      Binary    0.018     Balanced, <0.05           . ## re74         Contin.   -0.002     Balanced, <0.05       1.321 ## re75         Contin.    0.011     Balanced, <0.05       1.394 ##  ## Balance tally for mean differences ##                     count ## Balanced, <0.05         9 ## Not Balanced, >0.05     1 ##  ## Variable with the greatest mean difference ##  Variable Diff.Adj         M.Threshold ##       age    0.119 Not Balanced, >0.05 ##  ## Effective sample sizes ##            Control Treated ## Unadjusted  429.       185 ## Adjusted     99.82     185 W.out <- weightit(treat ~ age + educ + race + married + nodegree + re74 + re75,                   data = lalonde, estimand = \"ATT\", method = \"ebal\") summary(W.out) ##                   Summary of weights ##  ## - Weight ranges: ##  ##            Min                                Max ## treated 1.0000    ||                         1.00 ## control 0.0188 |---------------------------| 9.42 ##  ## - Units with the 5 most extreme weights by group: ##                                             ##               5      4      3      2      1 ##  treated      1      1      1      1      1 ##             608    381    597    303    411 ##  control 7.1271 7.5013 7.9999 9.0357 9.4204 ##  ## - Weight statistics: ##  ##         Coef of Var   MAD Entropy # Zeros ## treated       0.000 0.000   0.000       0 ## control       1.834 1.287   1.101       0 ##  ## - Effective Sample Sizes: ##  ##            Control Treated ## Unweighted  429.       185 ## Weighted     98.46     185 bal.tab(W.out, stats = c(\"m\", \"v\"), thresholds = c(m = .05)) ## Balance Measures ##                Type Diff.Adj     M.Threshold V.Ratio.Adj ## age         Contin.        0 Balanced, <0.05       0.410 ## educ        Contin.        0 Balanced, <0.05       0.664 ## race_black   Binary        0 Balanced, <0.05           . ## race_hispan  Binary       -0 Balanced, <0.05           . ## race_white   Binary        0 Balanced, <0.05           . ## married      Binary        0 Balanced, <0.05           . ## nodegree     Binary       -0 Balanced, <0.05           . ## re74        Contin.       -0 Balanced, <0.05       1.326 ## re75        Contin.       -0 Balanced, <0.05       1.335 ##  ## Balance tally for mean differences ##                     count ## Balanced, <0.05         9 ## Not Balanced, >0.05     0 ##  ## Variable with the greatest mean difference ##  Variable Diff.Adj     M.Threshold ##      re74       -0 Balanced, <0.05 ##  ## Effective sample sizes ##            Control Treated ## Unadjusted  429.       185 ## Adjusted     98.46     185 # Fit outcome model fit <- lm_weightit(re78 ~ treat * (age + educ + race + married +                                      nodegree + re74 + re75),                    data = lalonde, weightit = W.out) # G-computation for the treatment effect library(\"marginaleffects\") avg_comparisons(fit, variables = \"treat\",                 newdata = subset(treat == 1))"},{"path":"https://ngreifer.github.io/WeightIt/articles/WeightIt.html","id":"balancing-weights-for-a-longitudinal-treatment","dir":"Articles","previous_headings":"","what":"Balancing Weights for a Longitudinal Treatment","title":"Using WeightIt to Estimate Balancing Weights","text":"WeightIt can estimate weights marginal structural models longitudinal treatment well. time, ’ll use sample data set msmdata estimate weights. Data must “wide” format, one row per unit. binary outcome variable (Y_B), pre-treatment time-varying variables (X1_0 X2_0, measured first treatment, X1_1 X2_1 measured first second treatments, X1_2 X2_2 measured second third treatments), three time-varying binary treatment variables (A_1, A_2, A_3). interested joint, unique, causal effects treatment period outcome. treatment time point, need achieve balance variables measured prior treatment, including previous treatments. Using cobalt, can examine initial imbalance time point overall: bal.tab() indicates significant imbalance covariates time points, need work eliminate imbalance weighted data set. ’ll use weightitMSM() function specify weight models. syntax similar weightit() point treatments bal.tab() longitudinal treatments. ’ll use method = \"glm\" stabilize = TRUE stabilized propensity score weights estimated using logistic regression. weightitMSM() estimates separate weights time period takes product weights individual arrive final estimated weights. Printing output weightitMSM() provides details function call output. can take look quality weights summary(), just point treatments. Displayed summaries weights perform time point respect variability. Next, ’ll examine well perform respect covariate balance. setting .time = .none bal.tab(), can focus overall balance assessment, displays greatest imbalance covariate across time points. can see estimated weights balance covariates time points respect means KS statistics. Now can estimate treatment effects. First, fit marginal structural model outcome using glm_weightit() weightit object supplied: , compute average expected potential outcomes treatment regime using marginaleffects::avg_predictions(): can compare expected potential outcomes regime using marginaleffects::hypotheses(). get pairwise comparisons, supply avg_predictions() output hypotheses(., ~ pairwise). compare individual regimes, can use hypotheses(), identifying rows avg_predictions() output. example, compare regimes treatment three time points vs. regime treatment three time points, run results indicate receiving treatment time points reduces risk outcome relative receiving treatment .","code":"data(\"msmdata\") head(msmdata) ##   X1_0 X2_0 A_1 X1_1 X2_1 A_2 X1_2 X2_2 A_3 Y_B ## 1    2    0   1    5    1   0    4    1   0   0 ## 2    4    0   1    9    0   1   10    0   1   1 ## 3    4    1   0    5    0   1    4    0   0   1 ## 4    4    1   0    4    0   0    6    1   0   1 ## 5    6    1   1    5    0   1    6    0   0   1 ## 6    5    1   0    4    0   1    4    0   1   0 library(\"cobalt\") #if not already attached bal.tab(list(A_1 ~ X1_0 + X2_0,              A_2 ~ X1_1 + X2_1 +                A_1 + X1_0 + X2_0,              A_3 ~ X1_2 + X2_2 +                A_2 + X1_1 + X2_1 +                A_1 + X1_0 + X2_0),         data = msmdata, stats = c(\"m\", \"ks\"),         which.time = .all) ## Balance by Time Point ##  ##  - - - Time: 1 - - -  ## Balance Measures ##         Type Diff.Un KS.Un ## X1_0 Contin.   0.690 0.276 ## X2_0  Binary  -0.325 0.325 ##  ## Sample sizes ##     Control Treated ## All    3306    4194 ##  ##  - - - Time: 2 - - -  ## Balance Measures ##         Type Diff.Un KS.Un ## X1_1 Contin.   0.874 0.340 ## X2_1  Binary  -0.299 0.299 ## A_1   Binary   0.127 0.127 ## X1_0 Contin.   0.528 0.201 ## X2_0  Binary  -0.060 0.060 ##  ## Sample sizes ##     Control Treated ## All    3701    3799 ##  ##  - - - Time: 3 - - -  ## Balance Measures ##         Type Diff.Un KS.Un ## X1_2 Contin.   0.475 0.212 ## X2_2  Binary  -0.594 0.594 ## A_2   Binary   0.162 0.162 ## X1_1 Contin.   0.573 0.237 ## X2_1  Binary  -0.040 0.040 ## A_1   Binary   0.100 0.100 ## X1_0 Contin.   0.361 0.148 ## X2_0  Binary  -0.040 0.040 ##  ## Sample sizes ##     Control Treated ## All    4886    2614 ##  - - - - - - - - - - - Wmsm.out <- weightitMSM(list(A_1 ~ X1_0 + X2_0,                              A_2 ~ X1_1 + X2_1 +                                A_1 + X1_0 + X2_0,                              A_3 ~ X1_2 + X2_2 +                                A_2 + X1_1 + X2_1 +                                A_1 + X1_0 + X2_0),                         data = msmdata, method = \"glm\",                         stabilize = TRUE) Wmsm.out ## A weightitMSM object ##  - method: \"glm\" (propensity score weighting with GLM) ##  - number of obs.: 7500 ##  - sampling weights: none ##  - number of time points: 3 (A_1, A_2, A_3) ##  - treatment: ##     + time 1: 2-category ##     + time 2: 2-category ##     + time 3: 2-category ##  - covariates: ##     + baseline: X1_0, X2_0 ##     + after time 1: X1_1, X2_1, A_1, X1_0, X2_0 ##     + after time 2: X1_2, X2_2, A_2, X1_1, X2_1, A_1, X1_0, X2_0 ##  - stabilized; stabilization factors: ##     + baseline: (none) ##     + after time 1: A_1 ##     + after time 2: A_1, A_2, A_1:A_2 summary(Wmsm.out) ##                         Time 1                         ##                   Summary of weights ##  ## - Weight ranges: ##  ##            Min                                 Max ## treated 0.1527 |---------------------------| 57.08 ## control 0.1089 |--------|                    20.46 ##  ## - Units with the 5 most extreme weights by group: ##                                                 ##             4390    3440    3774   3593    5685 ##  treated 22.1008 24.1278 25.6999 27.786 57.0794 ##             6659    6284    1875   6163    2533 ##  control 12.8943   13.09 14.5234 14.705  20.465 ##  ## - Weight statistics: ##  ##         Coef of Var   MAD Entropy # Zeros ## treated       1.779 0.775   0.573       0 ## control       1.331 0.752   0.486       0 ##  ## - Mean of Weights = 0.99 ##  ## - Effective Sample Sizes: ##  ##            Control Treated ## Unweighted    3306    4194 ## Weighted      1193    1007 ##  ##                         Time 2                         ##                   Summary of weights ##  ## - Weight ranges: ##  ##            Min                                 Max ## treated 0.1089 |---------------------------| 57.08 ## control 0.1501 |--------|                    20.49 ##  ## - Units with the 5 most extreme weights by group: ##                                                  ##             4390    3440    3774    3593    5685 ##  treated 22.1008 24.1278 25.6999  27.786 57.0794 ##             1875    6163    6862    1286    6158 ##  control 14.5234  14.705 14.8079 16.2311 20.4862 ##  ## - Weight statistics: ##  ##         Coef of Var   MAD Entropy # Zeros ## treated       1.797 0.779   0.580       0 ## control       1.359 0.750   0.488       0 ##  ## - Mean of Weights = 0.99 ##  ## - Effective Sample Sizes: ##  ##            Control Treated ## Unweighted    3701  3799.  ## Weighted      1300   898.2 ##  ##                         Time 3                         ##                   Summary of weights ##  ## - Weight ranges: ##  ##            Min                                 Max ## treated 0.1089 |---------------------------| 57.08 ## control 0.2085 |-----------|                 25.70 ##  ## - Units with the 5 most extreme weights by group: ##                                                  ##             3576    4390    3440    3593    5685 ##  treated 20.5828 22.1008 24.1278  27.786 57.0794 ##             6163    6862     168    6158    3774 ##  control  14.705 14.8079 16.9698 20.4862 25.6999 ##  ## - Weight statistics: ##  ##         Coef of Var   MAD Entropy # Zeros ## treated       2.008 0.931   0.753       0 ## control       1.269 0.672   0.407       0 ##  ## - Mean of Weights = 0.99 ##  ## - Effective Sample Sizes: ##  ##            Control Treated ## Unweighted    4886  2614.  ## Weighted      1871   519.8 bal.tab(Wmsm.out, stats = c(\"m\", \"ks\"),         which.time = .none) ## Balance summary across all time points ##        Times    Type Max.Diff.Adj Max.KS.Adj ## X1_0 1, 2, 3 Contin.        0.033      0.018 ## X2_0 1, 2, 3  Binary        0.018      0.018 ## X1_1    2, 3 Contin.        0.087      0.039 ## X2_1    2, 3  Binary        0.031      0.031 ## A_1     2, 3  Binary        0.130      0.130 ## X1_2       3 Contin.        0.104      0.054 ## X2_2       3  Binary        0.007      0.007 ## A_2        3  Binary        0.154      0.154 ##  ## Effective sample sizes ##  - Time 1 ##            Control Treated ## Unadjusted    3306    4194 ## Adjusted      1193    1007 ##  - Time 2 ##            Control Treated ## Unadjusted    3701  3799.  ## Adjusted      1300   898.2 ##  - Time 3 ##            Control Treated ## Unadjusted    4886  2614.  ## Adjusted      1871   519.8 # Fit outcome model fit <- glm_weightit(Y_B ~ A_1 * A_2 * A_3 * (X1_0 + X2_0),                     data = msmdata,                     weightit = Wmsm.out,                     family = binomial) library(\"marginaleffects\") (p <- avg_predictions(fit,                       variables = c(\"A_1\", \"A_2\", \"A_3\"))) hypotheses(p, \"b8 - b1 = 0\")"},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/articles/estimating-effects.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Estimating Effects After Weighting","text":"assessing balance deciding weighting specification, comes time estimate effect treatment weighted sample. effect estimated interpreted depends desired estimand effect measure used. addition estimating effects, estimating uncertainty effects critical communicating assessing whether observed effect compatible effect population. guide explains estimate effects weighting point longitudinal treatments various outcome types. guide structured follows: first, information concepts related effect standard error (SE) estimation presented . , instructions estimate effects SEs described standard case (weighting ATE binary treatment continuous outcome) common circumstances. Finally, recommendations reporting results tips avoid making common mistakes presented.","code":""},{"path":"https://ngreifer.github.io/WeightIt/articles/estimating-effects.html","id":"identifying-the-estimand","dir":"Articles","previous_headings":"Introduction","what":"Identifying the estimand","title":"Estimating Effects After Weighting","text":"effect estimated, estimand must specified clarified. Although aspects estimand depend effect estimated weighting also weighting method , aspects must considered time effect estimation interpretation. , consider three aspects estimand: population effect meant generalize (target population), effect measure, whether effect marginal conditional. target population. Different weighting methods allow estimate effects can generalize different target populations. common estimand weighting average treatment effect population (ATE), average effect treatment population sample random sample. common estimands include average treatment effect treated (ATT), average treatment effect control (ATC), average treatment effect overlap (ATO). defined explained Greifer Stuart (2021). estimand weighting controlled estimand argument call weightit(). allowable estimands weighting methods include average treatment effect matched sample (ATM) average treatment effect optimal subset (ATOS). treated just like ATO differentiated . Marginal conditional effects. marginal effect comparison expected potential outcome treatment expected potential outcome control. quantity estimated randomized trials without blocking covariate adjustment particularly useful quantifying overall effect policy population-wide intervention. conditional effect comparison expected potential outcomes treatment groups within strata. useful identifying effect treatment individual patient subset population. Effect measures. main outcome types consider continuous, effect measured mean difference; binary, effect measured risk difference (RD), risk ratio (RR), odds ratio (); time--event (.e., survival), effect measured hazard ratio (HR). RR, , HR noncollapsible effect measures, means marginal effect scale (possibly) weighted average conditional effects within strata, even stratum-specific effects magnitude. effect measures, critical distinguish marginal conditional effects different statistical methods target different types effects. mean difference RD collapsible effect measures, methods can used estimate marginal conditional effects. primary focus marginal effects, appropriate effect measures, easily interpretable, require modeling assumptions.","code":""},{"path":"https://ngreifer.github.io/WeightIt/articles/estimating-effects.html","id":"g-computation","dir":"Articles","previous_headings":"Introduction","what":"G-computation","title":"Estimating Effects After Weighting","text":"estimate marginal effects, use method known g-computation (Snowden, Rose, Mortimer 2011) regression estimation (Schafer Kang 2008). involves first specifying model outcome function treatment covariates. , unit, compute predicted values outcome setting treatment status treated, control, leaving us two predicted outcome values unit, estimates potential outcomes treatment level. compute mean estimated potential outcomes across entire sample, leaves us two average estimated potential outcomes. Finally, contrast average estimated potential outcomes (e.g., difference ratio, depending effect measure desired) estimate treatment effect. g-computation weighting, additional considerations required. First, outcome model fit incorporating estimated weights (e.g., using weighted least squares weighted maximum likelihood estimation) (Vansteelandt Keiding 2011; Gabriel et al. 2024). Second, take average estimated potential outcomes treatment level, estimand ATT, ATC, ATE used sampling weights included, must weighted average incorporates weights1. Third, want target ATT ATC, estimate potential outcomes treated control group, respectively (though still generate predicted values treatment control) (Wang, Nianogo, Arah 2017). G-computation framework estimating effects weighting number advantages approaches. works regardless form outcome model type outcome (e.g., whether linear model used continuous outcome logistic model used binary outcome); difference might average expected potential outcomes contrasted final step. simple cases, estimated effect numerically identical effects estimated using methods; example, covariates included outcome model, g-computation estimate equal difference means t-test coefficient treatment linear model outcome. analytic bootstrap approximations SEs g-computation estimate. analytic approximation computed using delta method, technique computing SE quantity derived regression model parameters, g-computation estimate (Hansen Overgaard 2024). reasons , use weighted g-computation possible effect estimates, even simpler methods yield estimates. Using single workflow (slight modifications depending context; see ) facilitates implementing best practices regardless choices user makes. methods incorporate outcome model estimation treatment effect, best studied augmented inverse probability weighting (AIPW), also involves g-computation step. describe weighted g-computation conceptual simplicity, ease implementation, connection best practices estimating effects matching.","code":""},{"path":"https://ngreifer.github.io/WeightIt/articles/estimating-effects.html","id":"modeling-the-outcome","dir":"Articles","previous_headings":"Introduction","what":"Modeling the Outcome","title":"Estimating Effects After Weighting","text":"goal outcome model generate good predictions use g-computation procedure described . type form outcome model depend outcome type. continuous outcomes, one can use linear model regressing outcome treatment; binary outcomes, one can use generalized linear model , e.g., logistic link; time--event outcomes, one can use Cox proportional hazards model. Note purpose including outcome model arrive doubly robust estimator (.e., one consistent either outcome propensity score model correct); rather, simply increase precision weighted estimate essentially free. take advantage feature, important use canonical link (.e., default link given family), recommended Gabriel et al. (2024). additional decision make whether () include covariates outcome model. One may ask, use weighting going model outcome covariates anyway? Weighting reduces dependence effect estimate correct specification outcome model; central thesis Ho et al. (2007) (though applied matching case). Including covariates outcome model weighting several functions: can increase precision effect estimate, reduce bias due residual imbalance, make effect estimate “doubly robust”, means consistent either weighting reduces sufficient imbalance covariates outcome model correct. reasons, recommend covariate adjustment weighting possible. evidence covariate adjustment helpful covariates standardized mean differences greater .1 (Nguyen et al. 2017), covariates covariates thought highly predictive outcome prioritized treatment effect models can included due sample size constraints. Although many possible ways include covariates (e.g., just main effects interactions, smoothing terms like splines, nonlinear transformations), important engage specification search (.e., trying many outcomes models search “best” one). can invalidate results yield conclusion fails replicate. reason, recommend including terms included weighting model unless strong priori justifiable reason model outcome differently. important interpret coefficients tests covariates outcome model. causal effects estimates may severely confounded. treatment effect estimate can interpreted causal assuming relevant assumptions unconfoundedness met. Inappropriately interpreting coefficients covariates outcome model known Table 2 fallacy (Westreich Greenland 2013). avoid , display results g-computation procedure examine interpret outcome models .","code":""},{"path":"https://ngreifer.github.io/WeightIt/articles/estimating-effects.html","id":"estimating-standard-errors-and-confidence-intervals","dir":"Articles","previous_headings":"Introduction","what":"Estimating Standard Errors and Confidence Intervals","title":"Estimating Effects After Weighting","text":"Uncertainty estimation (.e., SEs, confidence intervals, p-values) may consider variety sources uncertainty present analysis, including (limited !) estimation propensity score (used) estimation treatment effect (.e., sampling error). methods, methods analytically computing correct asymptotic SE described implement WeightIt available. methods rely M-estimation (Stefanski Boos 2002; Ross et al. 2024), method combining estimation multiple models adjust standard errors joint estimation. methods, one must rely approximation treats weights fixed known, can either yield conservative anti-conservative standard errors depending method estimand (Austin 2022; Reifeis Hudgens 2020). Alternatively, one can use bootstrap, tends good performance regardless situation can computationally intensive large datasets slow weight estimation methods (Austin 2022). describe correct asymptotic standard errors, robust standard errors treating weights fixed, bootstrap .","code":""},{"path":"https://ngreifer.github.io/WeightIt/articles/estimating-effects.html","id":"asymptotically-correct-standard-errors-using-m-estimation","dir":"Articles","previous_headings":"Introduction > Estimating Standard Errors and Confidence Intervals","what":"Asymptotically Correct Standard Errors Using M-estimation","title":"Estimating Effects After Weighting","text":"M-estimation involves specifying stack estimating equations, one parameter estimated, function parameters. final parameter estimates yield vector 0s solutions estimating equations. estimating equations include parameters weighting model (e.g., coefficients logistic regression model propensity score) parameters outcome model (.e., coefficients treatment covariates). possible compute joint covariance matrix estimated parameters using functions estimating equations estimated parameters. refer curious readers Stefanski Boos (2002) introduction. theoretical developments estimation asymptotically correct standard errors propensity score-weighted treatment effect estimates rely M-estimation theory (Lunceford Davidian 2004; Reifeis Hudgens 2020; Gabriel et al. 2024). method can used model used estimate weights involves estimating equations. Currently, weights estimated using generalized linear model propensity score, entropy balancing, inverse probability tilting, just-identified covariate balancing propensity score can accommodated method. glm_weightit() can used fit generalized linear models outcome account estimation weights; ordinal_weightit(), multinom_weightit(), coxph_weightit() can used fit ordinal, multinomial, Cox proportional hazards models. estimating equations available given method, weights treated fixed known, M-estimation standard errors equal robust standard errors described .","code":""},{"path":"https://ngreifer.github.io/WeightIt/articles/estimating-effects.html","id":"robust-standard-errors","dir":"Articles","previous_headings":"Introduction > Estimating Standard Errors and Confidence Intervals","what":"Robust Standard Errors","title":"Estimating Effects After Weighting","text":"Also known sandwich SEs (due form formula computing ), heteroscedasticity-consistent SEs, Huber-White SEs, robust SEs adjustment usual maximum likelihood ordinary least squares SEs robust violations assumptions required usual SEs valid (MacKinnon White 1985). can also adjusted accommodate arbitrary levels clustering data (e.g., units sampled schools). Robust SEs shown conservative (.e., yielding overly large SEs wide confidence intervals) estimating ATE forms weighting (Robins, Hernán, Brumback 2000), though can either conservative weighting methods estimands, ATT (Reifeis Hudgens 2020) entropy balancing (Chan, Yam, Zhang 2016). Robust SEs treat estimated weights fixed known, ignoring uncertainty estimation otherwise accounted asymptotically correct standard errors described . Although quick simple estimate using functionality sandwich survey packages using glm_weightit(), used caution, bootstrap (described ) asymptotically correct standard errors preferred cases.","code":""},{"path":"https://ngreifer.github.io/WeightIt/articles/estimating-effects.html","id":"bootstrapping","dir":"Articles","previous_headings":"Introduction > Estimating Standard Errors and Confidence Intervals","what":"Bootstrapping","title":"Estimating Effects After Weighting","text":"problems robust SEs described include fail take account estimation weights approximation used compute derived quantities nonlinear models, often true using g-computation estimate effects. One solution problems bootstrapping, technique used simulate sampling distribution estimator repeatedly drawing samples replacement estimating effect bootstrap sample (Efron Tibshirani 1986). bootstrap distribution, SEs confidence intervals can computed several ways, including using standard deviation bootstrap estimates SE estimate using 2.5 97.5 percentiles 95% confidence interval bounds. Bootstrapping tends useful analytic estimator SE possible derived yet. Bootstrapping found effective estimating SEs confidence intervals weighting, often performing better even asymptotically correct method available, specially smaller samples (Austin 2022). Typically, bootstrapping involves performing entire estimation process bootstrap sample, including estimation weights outcome model parameters. bootstrap replications always better can take time increase chances least one error occur within bootstrap analysis (e.g., bootstrap sample zero treated units zero units event). general, numbers replications upwards 1000 recommended. traditional bootstrap resamples units dataset, fractional weighted bootstrap (Xu et al. 2020) draws weight unit specified distribution treats weights sampling weights replication. equally valid cases, fractional weighted bootstrap can outperform traditional bootstrap sparse categorical variables present data. Bootstrap standard errors can computed manually using boot (traditional bootstrap) fwb (fractional weighted bootstrap) automatically using functionality glm_weightit(), , bootstrap sample, re-estimates weights uses weights fit outcome model. covariance bootstrap estimates used parameter variance matrix. methods computing bootstrap confidence intervals use bootstrap replications , e.g., using quantiles bootstrap distribution. available WeightIt must programmed manually. bootstrap standard errors involve random component, imperative set seed using set.seed() ensure reproducibility.","code":""},{"path":"https://ngreifer.github.io/WeightIt/articles/estimating-effects.html","id":"estimating-treatment-effects-and-standard-errors-after-weighting","dir":"Articles","previous_headings":"","what":"Estimating Treatment Effects and Standard Errors After Weighting","title":"Estimating Effects After Weighting","text":", describe effect estimation weighting. focus evaluating methods simply demonstrating . cases, correct propensity score model used. present asymptotically correct method uses M-estimation, method uses robust SEs treat weights fixed, bootstrapping. ’ll using simulated toy dataset d several outcome treatment types. Code generate dataset end document. display first six rows d: binary treatment variable, X1 X9 covariates, Y_C continuous outcome, Y_B binary outcome, Y_S survival outcome. Ac multi-category continuous treatment variables, respectively. addition WeightIt whatever package may required estimate weights, need following packages perform analyses: marginaleffects provides avg_comparisons() function performing g-computation estimating SEs confidence intervals average estimate potential outcomes treatment effects survival provides coxph() estimate coefficients Cox-proportional hazards model marginal hazard ratio, called internally coxph_weightit() survival outcomes Effect estimates computed using marginaleffects::avg_comparisons(), even use may superfluous (e.g., comparing weighted difference means). previously mentioned, useful single workflow works matter situation, perhaps slight modifications accommodate different contexts. Using avg_comparisons() several advantages, even alternatives simple: provides effect estimate coefficients, always produces average marginal effects correct population requested. packages may use used . alternatives marginaleffects package computing average marginal effects, including margins stdReg. survey package can used estimate robust SEs incorporating weights provides functions survey-weighted generalized linear models Cox-proportional hazards models. Much code can adapted used survey, demonstrate well.","code":"head(d) ##   A Am      Ac      X1      X2      X3       X4 X5      X6      X7      X8       X9    Y_C Y_B    Y_S ## 1 0 C1 -2.2185  0.1725 -1.4283 -0.4103 -2.36059  1 -1.1199  0.6398 -0.4840 -0.59385 -3.591   0  857.7 ## 2 0 C2 -2.2837 -1.0959  0.8463  0.2456 -0.12333  1 -2.2687 -1.4491 -0.5514 -0.31439 -1.548   0  311.6 ## 3 0 C1 -1.1362  0.1768  0.7905 -0.8436  0.82366  1 -0.2221  0.2971 -0.6966 -0.69516  6.071   0  241.2 ## 4 0 C1 -0.8865 -0.4595  0.1726  1.9542 -0.62661  1 -0.4019 -0.8294 -0.5384  0.20729  2.491   1  142.4 ## 5 1  T  0.8613  0.3563 -1.8121  0.8135 -0.67189  1 -0.8297  1.7297 -0.6439 -0.02648 -1.100   0  206.8 ## 6 0 C2 -2.1697 -2.4313 -1.7984 -1.2940  0.04609  1 -1.2419 -1.1252 -1.8659 -0.56513 -9.850   0 1962.9 library(\"WeightIt\") ## library(\"marginaleffects\")"},{"path":"https://ngreifer.github.io/WeightIt/articles/estimating-effects.html","id":"the-standard-case-binary-treatment-with-asymptotically-correct-ses","dir":"Articles","previous_headings":"Estimating Treatment Effects and Standard Errors After Weighting","what":"The Standard Case: Binary Treatment with Asymptotically Correct SEs","title":"Estimating Effects After Weighting","text":"many weighting methods, estimating effect weighting straightforward involves fitting model outcome incorporates estimated weights using lm_weightit() glm_weightit(), estimating treatment effect using g-computation (.e., using marginaleffects::avg_comparisons()). procedure continuous binary outcomes without covariates. method uses asymptotically correct M-estimation-based SEs available robust SEs otherwise bootstrap standard errors requested. adjustments need made certain scenarios, describe section “Adjustments Standard Case”. adjustments include following cases: weighting ATT ATC, estimating effects binary outcomes, estimating effects survival outcomes. Estimation estimands ATT ATC proceeds ATE. must read Standard Case understand basic procedure reading special scenarios. also demonstrate estimate effects multi-category, continuous, sequential treatments. , demonstrate faster analytic approach estimating confidence intervals; bootstrap approach, see section “Using Bootstrapping Estimate Confidence Intervals” . First, perform propensity score weighting ATE. Remember, weighting methods use exact procedure slight variation, section critical even using different weighting method. Typically one assess balance ensure weighting specification works, skip step focus effect estimation. See vignette(\"WeightIt\") vignette(\"cobalt\", package = \"cobalt\") information necessary step. First, fit model outcome given treatment (optionally) covariates. ’s usually good idea include treatment-covariate interactions, , always necessary, especially excellent balance achieved. Next, use marginaleffects::avg_comparisons() estimate ATE. , addition effect estimate, want average estimated potential outcomes, can use marginaleffects::avg_predictions(), demonstrate . Note interpretation resulting estimates expected potential outcomes valid covariates present outcome model () interacted treatment. can see difference potential outcome means equal average treatment effect computed previously2. arguments avg_predictions() avg_comparisons().","code":"#PS weighting for the ATE with a logistic regression PS W <- weightit(A ~ X1 + X2 + X3 + X4 + X5 +                  X6 + X7 + X8 + X9, data = d,               method = \"glm\", estimand = \"ATE\") W ## A weightit object ##  - method: \"glm\" (propensity score weighting with GLM) ##  - number of obs.: 2000 ##  - sampling weights: none ##  - treatment: 2-category ##  - estimand: ATE ##  - covariates: X1, X2, X3, X4, X5, X6, X7, X8, X9 #Linear model with covariates fit <- lm_weightit(Y_C ~ A * (X1 + X2 + X3 + X4 + X5 +                          X6 + X7 + X8 + X9),                     data = d, weightit = W) avg_comparisons(fit, variables = \"A\") avg_predictions(fit, variables = \"A\")"},{"path":"https://ngreifer.github.io/WeightIt/articles/estimating-effects.html","id":"adjustments-to-the-standard-case","dir":"Articles","previous_headings":"Estimating Treatment Effects and Standard Errors After Weighting","what":"Adjustments to the Standard Case","title":"Estimating Effects After Weighting","text":"section explains procedure might differ following special circumstances occur.","code":""},{"path":"https://ngreifer.github.io/WeightIt/articles/estimating-effects.html","id":"weighting-for-the-att-or-atc","dir":"Articles","previous_headings":"Estimating Treatment Effects and Standard Errors After Weighting > Adjustments to the Standard Case","what":"Weighting for the ATT or ATC","title":"Estimating Effects After Weighting","text":"weighting ATT, everything identical Standard Case except calls avg_comparisons() avg_predictions(), newdata argument must additionally supplied avg_comparisons() avg_predictions() requests g-computation done treated units. ATC, replace 1 0.","code":"newdata = subset(A == 1)"},{"path":"https://ngreifer.github.io/WeightIt/articles/estimating-effects.html","id":"weighting-for-estimands-other-than-the-att-atc-or-ate","dir":"Articles","previous_headings":"Estimating Treatment Effects and Standard Errors After Weighting > Adjustments to the Standard Case","what":"Weighting for estimands other than the ATT, ATC, or ATE","title":"Estimating Effects After Weighting","text":"weighting estimand changes target population away one naturally defined data, e.g., ATO, ATM, ATOS, trimming weights way trimmed units dropped sample (.e., receive weight 0), adjustment needs made call avg_comparisons() avg_predictions(). estimands listed, need supply estimated weights wts argument. , weighting ATO example, need specify following:","code":"wts = W$weights"},{"path":"https://ngreifer.github.io/WeightIt/articles/estimating-effects.html","id":"binary-and-count-outcomes","dir":"Articles","previous_headings":"Estimating Treatment Effects and Standard Errors After Weighting > Adjustments to the Standard Case","what":"Binary and count outcomes","title":"Estimating Effects After Weighting","text":"Estimating effects binary count outcomes essentially continuous outcomes. main difference several measures effect one can consider, include odds ratio (), risk ratio/relative risk (RR), risk difference (RD) binary outcomes count/incidence ratio count outcomes, syntax avg_comparisons() depends one desired. outcome model one appropriate outcomes (e.g., logistic regression binary outcomes Poisson regression count outcomes) unrelated desired effect measure can compute effect measures using avg_comparisons() fitting model. fit generalized linear model, change lm_weightit() glm_weightit() set family = binomial binary outcomes family = poisson count outcomes. compute marginal RD mean difference, can use exactly syntax Standard Case; nothing needs change3. compute marginal RR count/incidence ratio, first need add comparison = \"lnratioavg\" avg_comparisons(); computes marginal log ratio. get marginal ratio , need add transform = \"exp\" avg_comparisons(), exponentiates marginal log ratio confidence interval. code computes effects displays statistics interest binary outcome Y_B: output displays marginal RR, Z-value, p-value Z-test log RR 0, confidence interval. (Note even though Comparison label still suggests log RR, RR actually displayed.) view log RR standard error, omit transform argument. marginal , thing needs change comparison set \"lnoravg\". Multi-category outcomes can modeled using ordinal_weightit() multinom_weightit(), fit ordinal multinomial regression models, respectively.","code":"#Logistic regression model with covariates fit <- glm_weightit(Y_B ~ A * (X1 + X2 + X3 + X4 + X5 +                          X6 + X7 + X8 + X9),                     data = d, weightit = W,                     family = binomial)  #Compute effects; RR and confidence interval avg_comparisons(fit,                 variables = \"A\",                 comparison = \"lnratioavg\",                 transform = \"exp\")"},{"path":"https://ngreifer.github.io/WeightIt/articles/estimating-effects.html","id":"survival-outcomes","dir":"Articles","previous_headings":"Estimating Treatment Effects and Standard Errors After Weighting > Adjustments to the Standard Case","what":"Survival outcomes","title":"Estimating Effects After Weighting","text":"several measures effect size survival outcomes, described Mao et al. (2018). using Cox proportional hazards model, quantity interest hazard ratio (HR) treated control groups. , HR non-collapsible, means estimated HR valid estimate marginal HR covariates included model. effect measures, difference mean survival times probability survival given time, can treated just like continuous binary outcomes previously described. HR, compute average marginal effects must use coefficient treatment Cox model fit without covariates. means use procedures Standard Case. describe estimating marginal HR using coxph_weightit(), adapts coxph() survival package allows estimation SEs account estimation weights. Robust SEs HRs studied Austin (2016) found conservative. formulas developed estimating standard errors accurately (Mao et al. 2018; Hajage et al. 2018), though Austin (2016) also found bootstrap adequate. HC0 robust standard error requested default, bootstrap standard errors can requested setting vcov \"BS\" \"FWB\". Note output differs survival::coxph() similar glm(). Estimate column contains log HR. can request HR setting transform = \"exp\" call summary(). adjustedCurves package provides integration WeightIt estimate adjusted survival estimands. strongly recommend using package estimate effects weighting.","code":"#Cox Regression for marginal HR fit <- coxph_weightit(survival::Surv(Y_S) ~ A, data = d,                       weightit = W)  #Log HR estimates summary(fit) ##  ## Call: ## coxph_weightit(formula = survival::Surv(Y_S) ~ A, data = d, weightit = W) ##  ## Coefficients: ##   Estimate Std. Error z value Pr(>|z|)     ## A   0.3873     0.0965    4.02  5.9e-05 *** ## Standard error: HC0 robust ## #HR and CIs; requested by exponentiating log HRs summary(fit, ci = TRUE, transform = \"exp\") ##  ## Call: ## coxph_weightit(formula = survival::Surv(Y_S) ~ A, data = d, weightit = W) ##  ## Coefficients (transformed): ##   Estimate z value Pr(>|z|) 2.5 % 97.5 %     ## A     1.47    4.02  5.9e-05  1.22   1.78 *** ## Standard error: HC0 robust ##"},{"path":"https://ngreifer.github.io/WeightIt/articles/estimating-effects.html","id":"using-sampling-weights-andor-clustered-data","dir":"Articles","previous_headings":"Estimating Treatment Effects and Standard Errors After Weighting > Adjustments to the Standard Case","what":"Using sampling weights and/or clustered data","title":"Estimating Effects After Weighting","text":"sampling weights required generalize correct population, must included call weightit() estimate balancing weights, outcome model, call avg_comparisons(), etc. cases, can handled without modifying standard case except s.weights must supplied weightit(), wts = W$s.weights must supplied avg_comparisons(), etc. lm_weightit() glm_weightit() automatically incorporate sampling weights estimation outcome model. clustering sampling design, clustering can accounted using cluster argument glm_weightit(), accepts one-sided formula clustering variable(s) right side. can used traditional fractional weighted bootstrap well. complicated survey designs might require use survey package handle ; important note using survey::svyglm() instead glm_weightit() produce standard errors account estimation balancing weights. See example using survey estimate effects weighting.","code":"#Estimate the balancing weights, with sampling weights called \"sw\" W <- weightit(A ~ X1 + X2 + X3 + X4 + X5 +                  X6 + X7 + X8 + X9, data = d,               method = \"glm\", estimand = \"ATE\",               s.weights = \"sw\")  #Fit the outcome model, with clustering variable \"clu\" fit <- glm_weightit(Y_C ~ A * (X1 + X2 + X3 + X4 + X5 +                             X6 + X7 + X8 + X9),                     data = d, weightit = W,                     cluster = ~clu)  #Compute the ATE, include sampling weights in the estimation avg_comparisons(fit,                 variables = \"A\",                 wts = W$s.weights) library(\"survey\")  #Multiply sampling weights and estimated weights d$weights <- W$weights * d$sw  #Declare a survey design using the combined weights with #appropriate clustering des <- svydesign(~clu, weights = ~weights, data = d)  #Fit the outcome model fit <- svyglm(Y_C ~ A * (X1 + X2 + X3 + X4 + X5 +                             X6 + X7 + X8 + X9),               design = des)  #G-computation for the difference in means, including sampling weights avg_comparisons(fit,                 variables = \"A\",                 wts = d$sw)"},{"path":"https://ngreifer.github.io/WeightIt/articles/estimating-effects.html","id":"multi-category-treatments","dir":"Articles","previous_headings":"Estimating Treatment Effects and Standard Errors After Weighting","what":"Multi-Category Treatments","title":"Estimating Effects After Weighting","text":"Multi-category treatments work essentially way binary treatments. main practical differences choosing estimand estimating weights. ATE ATO straightforward. ATT requires choosing one group treated “focal” group. Effects estimated members group. contrast focal group expected potential outcomes non-focal treatment expected potential outcomes focal (actual) treatment can interpreted similarly ATTs interpreted binary treatments. contrasts focal group expected potential outcomes pair non-focal treatments can interpreted contrast ATTs non-focal treatments. Weights may estimated differently multi-category treatments binary treatments; see individual methods pages differ. , ’ll estimate ATTs multi-category treatment focal level. focal treatment group, \"T\", two control groups \"C1\" \"C2\". expect ATTs two control groups since assigned randomly within original control group. First, estimate weights using entropy balancing (Hainmueller 2012), identifying focal group using focal: Typically one assess performance weights (balance effective sample size) skip now. Next, fit outcome model perform weighted g-computation. use avg_predictions() first compute expected potential outcome treatment focal group, use hypotheses() test pairwise comparisons. find significant ATTs focal treatment control levels (T - C1 T - C2), difference control levels (C2 - C1), can interpreted difference ATTs, nonsignificant, expected.","code":"table(d$Am) ##  ##  C1  C2   T  ## 751 801 448 W <- weightit(Am ~ X1 + X2 + X3 + X4 + X5 +                  X6 + X7 + X8 + X9, data = d,               method = \"ebal\", estimand = \"ATT\",               focal = \"T\") W ## A weightit object ##  - method: \"ebal\" (entropy balancing) ##  - number of obs.: 2000 ##  - sampling weights: none ##  - treatment: 3-category (C1, C2, T) ##  - estimand: ATT (focal: T) ##  - covariates: X1, X2, X3, X4, X5, X6, X7, X8, X9 #Fit the outcome model fit <- lm_weightit(Y_C ~ Am * (X1 + X2 + X3 + X4 + X5 +                                   X6 + X7 + X8 + X9),                    data = d, weightit = W)  #G-computation p <- avg_predictions(fit,                      variables = \"Am\",                      newdata = subset(Am == \"T\")) p  hypotheses(p, ~pairwise)"},{"path":"https://ngreifer.github.io/WeightIt/articles/estimating-effects.html","id":"continuous-treatments","dir":"Articles","previous_headings":"Estimating Treatment Effects and Standard Errors After Weighting","what":"Continuous Treatments","title":"Estimating Effects After Weighting","text":"continuous treatments, one estimand interest average dose-response function (ADRF), links value treatment expected potential outcome treatment value across full sample. provide detailed account ADRF demonstrate estimate weights balance covariates respect continuous treatment estimate plot ADRF weighted sample. ’ll use continuous treatment Ac, ranges -10.37 6.26, estimate effect continuous outcome Y_C. First, ’ll estimate entropy balancing weights (Vegetabile et al. 2021) using weightit(). Typically one assess performance weights (balance effective sample size) skip now. Next, fit outcome model perform weighted g-computation. outcome model, use natural cubic spline Ac 4 df. can use transformation treatment, e.g., polynomial transformation using poly(), long flexible enough capture possible ADRF; purely nonparametric methods like kernel regression can used well, inference challenging. Covariates can included model, many covariates, many basis functions treatment, full set treatment-covariate interactions, resulting estimates may imprecise. Next use avg_predictions() first compute expected potential outcome representative set treatment values. ’ll examine 31 treatment values 10th 90th percentiles Ac estimates outside ranges tend imprecise. Although one can examine expected potential outcomes, often useful see plotted. can generate plot ADRF pointwise confidence band using ggplot24: can see ADRF elbow-shape, zone evidence treatment effect followed zone increasing values outcome treatment increases. Another way characterize effect continuous treatments examine average marginal effect function (AMEF), function relates value treatment derivative ADRF. derivative different zero, evidence treatment effect corresponding value treatment. , use avg_slopes() compute pointwise derivatives ADRF across levels Ac plot it5. can see values -1.8 .5, evidence positive effect Ac Y_C (.e., confidence intervals slope ADRF values Ac exclude 0). line observation treatment appears effect higher values Ac.","code":"W <- weightit(Ac ~ X1 + X2 + X3 + X4 + X5 +                  X6 + X7 + X8 + X9, data = d,               moments = 2, int = TRUE,               method = \"ebal\") W ## A weightit object ##  - method: \"ebal\" (entropy balancing) ##  - number of obs.: 2000 ##  - sampling weights: none ##  - treatment: continuous ##  - covariates: X1, X2, X3, X4, X5, X6, X7, X8, X9 #Fit the outcome model fit <- lm_weightit(Y_C ~ splines::ns(Ac, df = 4) *                      (X1 + X2 + X3 + X4 + X5 +                          X6 + X7 + X8 + X9),                    data = d, weightit = W) #Represenative values of Ac: values <- with(d, seq(quantile(Ac, .1),                       quantile(Ac, .9),                       length.out = 31))  #G-computation p <- avg_predictions(fit,                      variables = list(Ac = values)) library(\"ggplot2\") ggplot(p, aes(x = Ac)) +   geom_line(aes(y = estimate)) +   geom_ribbon(aes(ymin = conf.low, ymax = conf.high),               alpha = .3) +   labs(x = \"Ac\", y = \"E[Y|Ac]\") +   theme_bw() # Estimate the pointwise derivatives at representative # values of Ac s <- avg_slopes(fit,                 variables = \"Ac\",                 newdata = datagrid(Ac = values,                                    grid_type = \"counterfactual\"),                 by = \"Ac\")  # Plot the AMEF ggplot(s, aes(x = Ac)) +   geom_line(aes(y = estimate)) +   geom_ribbon(aes(ymin = conf.low, ymax = conf.high),               alpha = .3) +   geom_hline(yintercept = 0, linetype = \"dashed\") +   labs(x = \"Ac\", y = \"dE[Y|Ac]/dAc\") +   theme_bw()"},{"path":"https://ngreifer.github.io/WeightIt/articles/estimating-effects.html","id":"longitudinal-treatments","dir":"Articles","previous_headings":"Estimating Treatment Effects and Standard Errors After Weighting","what":"Longitudinal Treatments","title":"Estimating Effects After Weighting","text":"longitudinal treatments, estimation proceeds usual, except estimate weights using weightitMSM() fit weighted outcome model includes treatment time periods (, optionally, covariates measured prior first treatment). important include covariates possibly caused treatments avoid bias; whole point using weighting estimate marginal structural model first place. glm_weightit() makes easy incorporate weights account estimation computing parameter variance matrix, desired. methods, components required M-estimation included weightitMSM() output, used estimate standard errors adjust estimation weights. Otherwise, weights can treated fixed whole estimation can done bootstrapping. , demonstrate using usual inverse probability weights marginal structural model. weights computed propensity scores estimated logistic regression, can use M-estimation adjust estimation computing parameter covariance matrix. also includes estimation standardization factor, . ’ll use toy dataset msmdata comes WeightIt. Next ’ll fit outcome model using glm_weightit(), includes baseline covariates (.e., measured prior first treatment). , compute average expected potential outcomes treatment regime using marginaleffects::avg_predictions(): can compare individual predictions using marginaleffects::hypotheses(). example, compare treatment histories just first treatment history (.e., units untreated time periods), can run following:","code":"data(\"msmdata\")  Wmsm <- weightitMSM(list(A_1 ~ X1_0 + X2_0,                              A_2 ~ X1_1 + X2_1 +                                A_1 + X1_0 + X2_0,                              A_3 ~ X1_2 + X2_2 +                                A_2 + X1_1 + X2_1 +                                A_1 + X1_0 + X2_0),                         data = msmdata, method = \"glm\",                         stabilize = TRUE) fit <- glm_weightit(Y_B ~ A_1 * A_2 * A_3 * (X1_0 + X2_0),                     data = msmdata, weightit = Wmsm,                     family = binomial) p <- avg_predictions(fit,                      variables = c(\"A_1\", \"A_2\", \"A_3\"))  p hypotheses(p, ~reference)"},{"path":"https://ngreifer.github.io/WeightIt/articles/estimating-effects.html","id":"moderation-analysis","dir":"Articles","previous_headings":"Estimating Treatment Effects and Standard Errors After Weighting","what":"Moderation Analysis","title":"Estimating Effects After Weighting","text":"Moderation analysis involves determining whether treatment effect differs across levels another variable. use weighting moderation analysis described Griffin et al. (2023). goal achieve balance within subgroup potential moderating variable, several ways . Broadly, one can either perform weighting full dataset interact moderator covariates weighting model, one can perform completely separate analyses subgroup. cases, e.g., using unregularized parametric methods like GLM propensity score weighting, CBPS, entropy balancing, inverse probability tilting, methods yield identical results. fit weighting model within subgroup, one can use argument weightit(), just this6. also possibility using subgroup balancing propensity score (Dong et al. 2020) implemented using sbps(), determines whether single weighting model fit whole subgroup-specific weighting model best subgroup overall. chosen approach achieves best balance, though don’t demonstrate assessing balance maintain focus effect estimation. ’ll consider binary variable X5 potential moderator effect Y_C. , ’ll estimate weights using CBPS ATE within level X5 supplying argument. using CBPS, equivalent weights estimated interacting X5 covariates, e.g., ~ X5 * (X1 + X2 + ...). straightforward check subgroup balance using cobalt::bal.tab(), cluster argument can used assess balance within subgroups, e.g., cobalt::bal.tab(Wm, cluster = \"X5\"). See vignette(\"segmented-data\", package = \"cobalt\") details. satisfied balance, can model outcome interaction treatment, moderator, , optionally, covariates. equivalent fitting model outcome within combination treatment moderator, can good idea simplify model include just important covariates outcome order avoid estimating many parameters. minimum, must include interaction treatment moderator. , ’ll fit linear regression model including just first three covariates addition treatment moderator (allowing covariates interact ). estimate subgroup ATEs, can use avg_comparisons(), time specifying argument signify want treatment effects stratified moderator. can see subgroup mean differences differ , can formally test moderation using hypotheses(): Though subgroup effects differ sample, difference statistically significant .05 level, evidence moderation X5. moderator two levels, possible run omnibus test moderation changing hypothesis ~reference supplying output hypotheses() joint = TRUE, e.g., produces single p-value test pairwise differences subgroups equal zero. important remember presence moderation imply moderator causes differences treatment effect; may simply associated variable interacts treatment (VanderWeele 2009). addition, depending variables adjusted , moderation may necessarily represent disparity, even one group experiences less benefit another group (Jackson 2021).","code":"Wm <- weightit(A ~ X1 + X2 + X3 + X4 + X5 +                   X6 + X7 + X8 + X9, data = d,                method = \"cbps\", estimand = \"ATE\",                by = ~X5)  Wm ## A weightit object ##  - method: \"cbps\" (covariate balancing propensity score weighting) ##  - number of obs.: 2000 ##  - sampling weights: none ##  - treatment: 2-category ##  - estimand: ATE ##  - covariates: X1, X2, X3, X4, X5, X6, X7, X8, X9 ##  - by: X5 fit <- lm_weightit(Y_C ~ A * X5 * (X1 + X2 + X3),                     data = d, weightit = Wm) a <- avg_comparisons(fit, variables = \"A\",                      by = \"X5\") a hypotheses(a, ~pairwise) avg_comparisons(fit, variables = \"A\",                 by = \"X5\",                 hypothesis = ~reference) |>   hypotheses(joint = TRUE)"},{"path":"https://ngreifer.github.io/WeightIt/articles/estimating-effects.html","id":"reporting-results","dir":"Articles","previous_headings":"","what":"Reporting Results","title":"Estimating Effects After Weighting","text":"important thorough complete possible describing methods estimating treatment effect results analysis. improves transparency replicability analysis. Results least include following: description outcome model used (e.g., logistic regression, linear model treatment-covariate interactions covariates, Cox proportional hazards model propensity score weights applied) way effect estimated (e.g., using g-computation coefficient outcome model) way SEs estimated (e.g., using M-estimation, using robust SEs treat weights fixed, using fractional weighted bootstrap 1000 bootstrap replications entire process weighting effect estimation included replication) R packages functions used estimating effect SE (e.g., glm_weightit() WeightIt, avg_comparisons() marginaleffects) effect SE confidence interval addition information weighting method, estimand, propensity score estimation procedure (used), balance assessment, etc.","code":""},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/articles/estimating-effects.html","id":"code-to-generate-data-used-in-examples","dir":"Articles","previous_headings":"","what":"Code to Generate Data used in Examples","title":"Estimating Effects After Weighting","text":"","code":"#Generating data similar to Austin (2009) for demonstrating treatment effect estimation gen_X <- function(n) {   X <- matrix(rnorm(9 * n), nrow = n, ncol = 9)   X[,5] <- as.numeric(X[,5] < .5)   X }  gen_Ac <- function(X) {   LP_A <- -1.2 + log(2)*X[,1] - log(1.5)*X[,2] + log(2)*X[,4] - log(2.4)*X[,5] + log(2)*X[,7] - log(1.5)*X[,8]   LP_A + rlogis(nrow(X)) }  #~20% treated gen_A <- function(Ac) {   1 * (Ac > 0) }  gen_Am <- function(A) {   factor(ifelse(A == 1, \"T\", sample(c(\"C1\", \"C2\"), length(A), TRUE))) }  # Continuous outcome gen_Y_C <- function(A, X) {   2*A + 2*X[,1] + 2*X[,2] + 2*X[,3] + 1*X[,4] + 2*X[,5] + 1*X[,6] + rnorm(length(A), 0, 5) } #Conditional: #  MD: 2 #Marginal: #  MD: 2  # Binary outcome gen_Y_B <- function(A, X) {   LP_B <- -2 + log(2.4)*A + log(2)*X[,1] + log(2)*X[,2] + log(2)*X[,3] + log(1.5)*X[,4] + log(2.4)*X[,5] + log(1.5)*X[,6]   P_B <- plogis(LP_B)   rbinom(length(A), 1, P_B) } #Conditional: #  OR:   2.4 #  logOR: .875 #Marginal: #  RD:    .144 #  RR:   1.54 #  logRR: .433 #  OR:   1.92 #  logOR  .655  # Survival outcome gen_Y_S <- function(A, X) {   LP_S <- -2 + log(2.4)*A + log(2)*X[,1] + log(2)*X[,2] + log(2)*X[,3] + log(1.5)*X[,4] + log(2.4)*X[,5] + log(1.5)*X[,6]   sqrt(-log(runif(length(A)))*2e4*exp(-LP_S)) } #Conditional: #  HR:   2.4 #  logHR: .875 #Marginal: #  HR:   1.57 #  logHR: .452  set.seed(19599)  n <- 2000 X <- gen_X(n) Ac <- gen_Ac(X) A <- gen_A(Ac) Am <- gen_Am(A)  Y_C <- gen_Y_C(A, X) Y_B <- gen_Y_B(A, X) Y_S <- gen_Y_S(A, X)  d <- data.frame(A, Am, Ac, X, Y_C, Y_B, Y_S)"},{"path":"https://ngreifer.github.io/WeightIt/articles/installing-packages.html","id":"propensity-score-weighting-using-glms-method-glm","dir":"Articles","previous_headings":"","what":"Propensity score weighting using GLMs (method = \"glm\")","title":"Installing Supporting Packages","text":"Several options available estimating propensity score weights using GLMs depending treatment type features desired model. binary treatments, weightit() uses stats::glm() default, continuous treatments, weightit() uses stats::lm() default, additional packages required. multi-category treatments multi.method = \"weightit\", default, weightit() uses internal code.","code":""},{"path":"https://ngreifer.github.io/WeightIt/articles/installing-packages.html","id":"missing-saem","dir":"Articles","previous_headings":"Propensity score weighting using GLMs (method = \"glm\")","what":"missing = \"saem\"","title":"Installing Supporting Packages","text":"missing data present missing = \"saem\" supplied, misaem package required. install misaem CRAN, run misaem CRAN, want install development version source, can developer’s GitHub repo using following code:","code":"pak::pkg_install(\"misaem\") pak::pkg_install(\"julierennes/misaem\")"},{"path":"https://ngreifer.github.io/WeightIt/articles/installing-packages.html","id":"binary-and-multi-category-treatments-with-link-br-logit","dir":"Articles","previous_headings":"Propensity score weighting using GLMs (method = \"glm\")","what":"Binary and multi-category treatments with link = \"br.logit\"","title":"Installing Supporting Packages","text":"binary multi-category treatments, link supplied \"br.logit\" another link beginning \"br.\", brglm2 package required. install brglm2 CRAN, run brglm2 CRAN, want install development version source, can developer, Ioannis Kosmidis’s, GitHub repo using following code: brglm2 requires compilation, means may need additional software installed computer install source.","code":"pak::pkg_install(\"brglm2\") pak::pkg_install(\"ikosmidis/brglm2\")"},{"path":"https://ngreifer.github.io/WeightIt/articles/installing-packages.html","id":"multi-category-treatments-with-multi-method-mclogit","dir":"Articles","previous_headings":"Propensity score weighting using GLMs (method = \"glm\")","what":"Multi-category treatments with multi.method = \"mclogit\"","title":"Installing Supporting Packages","text":"multi-category treatments, multi.method = \"mclogit\", mclogit package required multinomial logistic regression. install mclogit CRAN, run mclogit CRAN, want install development version source, can developer, Martin Elff’s, GitHub repo using following code:","code":"pak::pkg_install(\"mclogit\") pak::pkg_install(\"melff/mclogit\")"},{"path":"https://ngreifer.github.io/WeightIt/articles/installing-packages.html","id":"multi-category-treatments-with-multi-method-mnp","dir":"Articles","previous_headings":"Propensity score weighting using GLMs (method = \"glm\")","what":"Multi-category treatments with multi.method = \"mnp\"","title":"Installing Supporting Packages","text":"multi-category treatments, multi.method = \"mnp\", MNP package required Bayesian multinomial probit regression. install MNP CRAN, run MNP CRAN, want install development version source, can developer, Kosuke Imai’s, GitHub repo using following code: MNP requires compilation, means may need additional software installed computer install source.","code":"pak::pkg_install(\"MNP\") pak::pkg_install(\"kosukeimai/MNP\")"},{"path":"https://ngreifer.github.io/WeightIt/articles/installing-packages.html","id":"propensity-score-weighting-using-gbm-method-gbm","dir":"Articles","previous_headings":"","what":"Propensity Score weighting using GBM (method = \"gbm\")","title":"Installing Supporting Packages","text":"WeightIt uses R package gbm estimate propensity score weights using GBM. rely twang package . install gbm CRAN, run gbm CRAN, want install development version source, can developer’s GitHub repo using following code: gbm requires compilation, means may need additional software installed computer install source.","code":"pak::pkg_install(\"gbm\") pak::pkg_install(\"gbm-developers/gbm\")"},{"path":"https://ngreifer.github.io/WeightIt/articles/installing-packages.html","id":"covariate-balancing-propensity-score-weighting-method-cbps","dir":"Articles","previous_headings":"","what":"Covariate Balancing Propensity Score weighting (method = \"cbps\")","title":"Installing Supporting Packages","text":"method = \"cbps\", WeightIt uses code written WeightIt, additional packages need installed use CBPS. Installing rootSolve can improve estimation, necessary.","code":""},{"path":"https://ngreifer.github.io/WeightIt/articles/installing-packages.html","id":"nonparametric-covariate-balancing-propensity-score-weighting-method-npcbps","dir":"Articles","previous_headings":"","what":"Nonparametric Covariate Balancing Propensity Score weighting (method = \"npcbps\")","title":"Installing Supporting Packages","text":"method = \"npcbps\", WeightIt uses R package CBPS perform nonparametric covariate balancing propensity score weighting. install CBPS CRAN, run CBPS CRAN, want install development version source, can developer, Kosuke Imai’s, GitHub repo using following code:","code":"pak::pkg_install(\"CBPS\") pak::pkg_install(\"kosukeimai/CBPS\")"},{"path":"https://ngreifer.github.io/WeightIt/articles/installing-packages.html","id":"entropy-balancing-method-ebal","dir":"Articles","previous_headings":"","what":"Entropy balancing (method = \"ebal\")","title":"Installing Supporting Packages","text":"WeightIt uses code written WeightIt, additional packages need installed use entropy balancing. Installing rootSolve can improve estimation, necessary.","code":""},{"path":"https://ngreifer.github.io/WeightIt/articles/installing-packages.html","id":"inverse-probability-tilting-method-ipt","dir":"Articles","previous_headings":"","what":"Inverse probability tilting (method = \"ipt\")","title":"Installing Supporting Packages","text":"WeightIt uses R package rootSolve perform root finding required inverse probability tilting. install rootSolve CRAN, run","code":"pak::pkg_install(\"rootSolve\")"},{"path":"https://ngreifer.github.io/WeightIt/articles/installing-packages.html","id":"optimization-based-weighting-method-optweight","dir":"Articles","previous_headings":"","what":"Optimization-based weighting (method = \"optweight\")","title":"Installing Supporting Packages","text":"WeightIt uses R package optweight perform optimization-based weighting. install optweight CRAN, run optweight CRAN, want install development version source, can developer, Noah Greifer’s (), GitHub repo using following code: optweight depends osqp package, requires compilation, means may need additional software installed computer install source.","code":"pak::pkg_install(\"optweight\") pak::pkg_install(\"ngreifer/optweight\")"},{"path":"https://ngreifer.github.io/WeightIt/articles/installing-packages.html","id":"propensity-score-weighting-using-superlearner-method-super","dir":"Articles","previous_headings":"","what":"Propensity score weighting using SuperLearner (method = \"super\")","title":"Installing Supporting Packages","text":"WeightIt uses R package SuperLearner estimate propensity score weights using SuperLearner. install SuperLearner CRAN, run SuperLearner CRAN, want install development version source, can developer, Eric Polley’s, GitHub repo using following code: SuperLearner wrapper many packages. whole point using SuperLearner include many different machine learning algorithms combine well-fitting stacked model. algorithms exist many different R packages, need installed use . See Suggested packages SuperLearner CRAN page see packages might used SuperLearner. additional functions use SuperLearner SuperLearnerExtra repository. read R session used method = \"super\", use source() raw text file URL. example, read code SL.dbarts, run","code":"pak::pkg_install(\"SuperLearner\") pak::pkg_install(\"ecpolley/SuperLearner\") source(\"https://raw.githubusercontent.com/ecpolley/SuperLearnerExtra/master/SL/SL.dbarts.R\")"},{"path":"https://ngreifer.github.io/WeightIt/articles/installing-packages.html","id":"propensity-score-weighting-using-bart-method-bart","dir":"Articles","previous_headings":"","what":"Propensity score weighting using BART (method = \"bart\")","title":"Installing Supporting Packages","text":"WeightIt uses R package dbarts estimate propensity score weights using BART. install dbarts CRAN, run dbarts CRAN, want install development version source, can developer, Vincent Dorie’s, GitHub repo using following code: dbarts requires compilation, means may need additional software installed computer install source.","code":"pak::pkg_install(\"dbarts\") pak::pkg_install(\"vdorie/dbarts\")"},{"path":"https://ngreifer.github.io/WeightIt/articles/installing-packages.html","id":"energy-balancing-method-energy","dir":"Articles","previous_headings":"","what":"Energy Balancing (method = \"energy\")","title":"Installing Supporting Packages","text":"WeightIt uses R package osqp perform optimization required energy balancing. install osqp CRAN, run osqp CRAN, want install development version source, can developer’s site using instructions given , though bit involved installations source.","code":"pak::pkg_install(\"osqp\")"},{"path":"https://ngreifer.github.io/WeightIt/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Noah Greifer. Author, maintainer.","code":""},{"path":"https://ngreifer.github.io/WeightIt/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Greifer N (2025). WeightIt: Weighting Covariate Balance Observational Studies. R package version 1.4.0.9001, https://github.com/ngreifer/WeightIt, https://ngreifer.github.io/WeightIt/.","code":"@Manual{,   title = {WeightIt: Weighting for Covariate Balance in Observational Studies},   author = {Noah Greifer},   year = {2025},   note = {R package version 1.4.0.9001,     https://github.com/ngreifer/WeightIt},   url = {https://ngreifer.github.io/WeightIt/}, }"},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/index.html","id":"overview","dir":"","previous_headings":"","what":"Overview","title":"Weighting for Covariate Balance in Observational Studies","text":"WeightIt one-stop package generate balancing weights point longitudinal treatments observational studies. Support included binary, multi-category, continuous treatments, variety estimands including ATE, ATT, ATC, ATO, others, support wide variety weighting methods, including rely parametric modeling, machine learning, optimization. WeightIt also provides functionality fitting regression models weighted samples account estimation weights quantifying uncertainty. WeightIt uses familiar formula interface meant complement MatchIt package provides unified interface basic advanced weighting methods. complete vignette, see website WeightIt vignette(\"WeightIt\"). install load WeightIt, use code : workhorse function WeightIt weightit(), generates weights given formula data input according methods parameters specified user. example use weightit() generate propensity score weights estimating ATT: Evaluating weights two components: evaluating covariate balance produced weights, evaluating whether weights allow sufficient precision eventual effect estimate. first goal, functions cobalt package, fully compatible WeightIt, can used, demonstrated : second goal, qualities distributions weights can assessed using summary(), demonstrated . Large effective sample sizes imply low variability weights, therefore increased precision estimating treatment effect. Finally, can estimate effect treatment using weighted outcome model, accounting estimation weights standard error effect estimate: tables contain available methods WeightIt estimating weights binary, multi-category, continuous treatments. methods require installing packages use; see vignette(\"installing-packages\") information install .","code":"#CRAN version pak::pkg_install(\"WeightIt\")  #Development version pak::pkg_install(\"ngreifer/WeightIt\")  library(\"WeightIt\") data(\"lalonde\", package = \"cobalt\")  W <- weightit(treat ~ age + educ + nodegree +                  married + race + re74 + re75,                data = lalonde, method = \"glm\",                estimand = \"ATT\") W #> A weightit object #>  - method: \"glm\" (propensity score weighting with GLM) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 2-category #>  - estimand: ATT (focal: 1) #>  - covariates: age, educ, nodegree, married, race, re74, re75 library(\"cobalt\")  bal.tab(W, un = TRUE) #> Balance Measures #>                 Type Diff.Un Diff.Adj #> prop.score  Distance  1.7941  -0.0205 #> age          Contin. -0.3094   0.1188 #> educ         Contin.  0.0550  -0.0284 #> nodegree      Binary  0.1114   0.0184 #> married       Binary -0.3236   0.0186 #> race_black    Binary  0.6404  -0.0022 #> race_hispan   Binary -0.0827   0.0002 #> race_white    Binary -0.5577   0.0021 #> re74         Contin. -0.7211  -0.0021 #> re75         Contin. -0.2903   0.0110 #>  #> Effective sample sizes #>            Control Treated #> Unadjusted  429.       185 #> Adjusted     99.82     185 summary(W) #>                   Summary of weights #>  #> - Weight ranges: #>  #>            Min                                  Max #> treated 1.0000         ||                    1.0000 #> control 0.0092 |---------------------------| 3.7432 #>  #> - Units with the 5 most extreme weights by group: #>                                             #>               5      4      3      2      1 #>  treated      1      1      1      1      1 #>             597    573    381    411    303 #>  control 3.0301 3.0592 3.2397 3.5231 3.7432 #>  #> - Weight statistics: #>  #>         Coef of Var   MAD Entropy # Zeros #> treated       0.000 0.000   0.000       0 #> control       1.818 1.289   1.098       0 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted  429.       185 #> Weighted     99.82     185 fit <- lm_weightit(re78 ~ treat, data = lalonde,                    weightit = W)  summary(fit, ci = TRUE) #>  #> Call: #> lm_weightit(formula = re78 ~ treat, data = lalonde, weightit = W) #>  #> Coefficients: #>             Estimate Std. Error z value Pr(>|z|)  2.5 % 97.5 %     #> (Intercept)   5135.1      583.8   8.797   <1e-06 3990.9 6279.2 *** #> treat         1214.1      798.2   1.521    0.128 -350.3 2778.4     #> Standard error: HC0 robust (adjusted for estimation of weights)"},{"path":[]},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/index.html","id":"continuous-treatments","dir":"","previous_headings":"Overview","what":"Continuous Treatments","title":"Weighting for Covariate Balance in Observational Studies","text":"addition, WeightIt implements subgroup balancing propensity score using function sbps(). Several tools utilities available, including trim() trim truncate weights, calibrate() calibrate propensity scores, get_w_from_ps() compute weights propensity scores. WeightIt provides functions fit weighted models account uncertainty estimating weights. include glm_weightit() fitting generalized linear models, ordinal_weightit() ordinal regression models, multinom_weightit() multinomial regression models, coxph_weightit() Cox proportional hazards models. Several methods available computing parameter variances, including asymptotically correct M-estimation-based variances, robust variances treat weights fixed, traditional fractional weighted bootstrap variances. Clustered variances supported. See vignette(\"estimating-effects\") information use weighting estimate treatment effects. Please submit bug reports, questions, comments, issues https://github.com/ngreifer/WeightIt/issues. like see package method integrated WeightIt, please contact author. Fan mail greatly appreciated.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/ESS.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute effective sample size of weighted sample — ESS","title":"Compute effective sample size of weighted sample — ESS","text":"Computes effective sample size (ESS) weighted sample, represents size unweighted sample approximately amount precision weighted sample consideration. ESS calculated \\((\\sum w)^2/\\sum w^2\\).","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/ESS.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute effective sample size of weighted sample — ESS","text":"","code":"ESS(w)"},{"path":"https://ngreifer.github.io/WeightIt/reference/ESS.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute effective sample size of weighted sample — ESS","text":"w vector weights","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/ESS.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Compute effective sample size of weighted sample — ESS","text":"McCaffrey, D. F., Ridgeway, G., & Morral, . R. (2004). Propensity Score Estimation Boosted Regression Evaluating Causal Effects Observational Studies. Psychological Methods, 9(4), 403–425. doi:10.1037/1082-989X.9.4.403 Shook‐Sa, B. E., & Hudgens, M. G. (2020). Power sample size observational studies point exposure effects. Biometrics, biom.13405. doi:10.1111/biom.13405","code":""},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/reference/ESS.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute effective sample size of weighted sample — ESS","text":"","code":"library(\"cobalt\") #>  cobalt (Version 4.5.5, Build Date: 2024-04-02) data(\"lalonde\", package = \"cobalt\")  #Balancing covariates between treatment groups (binary) (W1 <- weightit(treat ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"glm\", estimand = \"ATE\")) #> A weightit object #>  - method: \"glm\" (propensity score weighting with GLM) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 2-category #>  - estimand: ATE #>  - covariates: age, educ, married, nodegree, re74 summary(W1) #>                   Summary of weights #>  #> - Weight ranges: #>  #>            Min                                   Max #> treated 1.5560  |--------------------------| 73.3315 #> control 1.0222 ||                             3.0438 #>  #> - Units with the 5 most extreme weights by group: #>                                                  #>              124     184     172     181     182 #>  treated 11.2281 11.3437 12.0848 26.1775 73.3315 #>              411     595     269     409     296 #>  control  2.3303  2.4365  2.5005  2.6369  3.0438 #>  #> - Weight statistics: #>  #>         Coef of Var   MAD Entropy # Zeros #> treated       1.609 0.555   0.403       0 #> control       0.247 0.211   0.029       0 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted  429.    185.   #> Weighted    404.35   51.73 ESS(W1$weights[W1$treat == 0]) #> [1] 404.3484 ESS(W1$weights[W1$treat == 1]) #> [1] 51.73462"},{"path":"https://ngreifer.github.io/WeightIt/reference/WeightIt-package.html","id":null,"dir":"Reference","previous_headings":"","what":"WeightIt: Weighting for Covariate Balance in Observational Studies — WeightIt-package","title":"WeightIt: Weighting for Covariate Balance in Observational Studies — WeightIt-package","text":"Generates balancing weights causal effect estimation observational studies binary, multi-category, continuous point longitudinal treatments easing extending functionality several R packages providing -house estimation methods. Available methods include rely parametric modeling, optimization, machine learning. Also allows assessment weights checking covariate balance interfacing directly 'cobalt' package. Methods estimating weighted regression models take account uncertainty estimation weights via M-estimation bootstrapping available. See vignette \"Installing Supporting Packages\" instructions install package 'WeightIt' uses, including may CRAN.","code":""},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/reference/WeightIt-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"WeightIt: Weighting for Covariate Balance in Observational Studies — WeightIt-package","text":"Maintainer: Noah Greifer noah.greifer@gmail.com (ORCID)","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/anova.glm_weightit.html","id":null,"dir":"Reference","previous_headings":"","what":"Methods for glm_weightit() objects — anova.glm_weightit","title":"Methods for glm_weightit() objects — anova.glm_weightit","text":"anova() used compare nested models fit glm_weightit(), mutinom_weightit(), ordinal_weightit(), coxph_weightit() using Wald test incorporates uncertainty estimating weights ().","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/anova.glm_weightit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Methods for glm_weightit() objects — anova.glm_weightit","text":"","code":"# S3 method for class 'glm_weightit' anova(   object,   object2,   test = \"Chisq\",   method = \"Wald\",   tolerance = 1e-07,   vcov = NULL,   ... )"},{"path":"https://ngreifer.github.io/WeightIt/reference/anova.glm_weightit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Methods for glm_weightit() objects — anova.glm_weightit","text":"object, object2 output one modeling functions. object2 required. test type test statistic used compare models. Currently \"Chisq\" (chi-square statistic) allowed. method kind test used compare models. Currently \"Wald\" allowed. tolerance Wald test, tolerance used determine models symbolically nested. vcov either string indicating method used compute variance estimated parameters object, function used extract variance, variance matrix . Default use variance matrix already present object. string function, arguments passed ... supplied method function. (Note: vcov(), can also supplied type.) ... arguments passed function used computing parameter variance matrix, supplied string function, e.g., cluster, R, fwb.args.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/anova.glm_weightit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Methods for glm_weightit() objects — anova.glm_weightit","text":"object class \"anova\" inheriting class \"data.frame\".","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/anova.glm_weightit.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Methods for glm_weightit() objects — anova.glm_weightit","text":"anova() performs Wald test compare two fitted models. models must nested, nested symbolically (.e., names coefficients smaller model subset names coefficients larger model). larger model must supplied object smaller object2. models must contain units, weights (), outcomes. variance-covariance matrix coefficients smaller model used.","code":""},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/reference/anova.glm_weightit.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Methods for glm_weightit() objects — anova.glm_weightit","text":"","code":"data(\"lalonde\", package = \"cobalt\")  # Model comparison for any relationship between `treat` # and `re78` (not the same as testing for the ATE) fit1 <- glm_weightit(   re78 ~ treat * (age + educ + race + married + nodegree +                     re74 + re75), data = lalonde )  fit2 <- glm_weightit(   re78 ~ age + educ + race + married + nodegree +     re74 + re75, data = lalonde )  anova(fit1, fit2) #>  #> Wald test #> Variance: HC0 robust #>  #> Model 1: re78 ~ treat * (age + educ + race + married + nodegree + re74 + re75) #> Model 2: re78 ~ age + educ + race + married + nodegree + re74 + re75 #>  #>   Res.Df Df  Chisq Pr(>Chisq)   #> 1    596                        #> 2    605  9 17.563    0.04059 * #> --- #> Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1  # Using the usual maximum likelihood variance matrix anova(fit1, fit2, vcov = \"const\") #>  #> Wald test #> Variance: maximum likelihood #>  #> Model 1: re78 ~ treat * (age + educ + race + married + nodegree + re74 + re75) #> Model 2: re78 ~ age + educ + race + married + nodegree + re74 + re75 #>  #>   Res.Df Df  Chisq Pr(>Chisq)   #> 1    596                        #> 2    605  9 19.761    0.01944 * #> --- #> Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1  # Using a bootstrapped variance matrix anova(fit1, fit2, vcov = \"BS\", R = 100) #>  #> Wald test #> Variance: traditional bootstrap #>  #> Model 1: re78 ~ treat * (age + educ + race + married + nodegree + re74 + re75) #> Model 2: re78 ~ age + educ + race + married + nodegree + re74 + re75 #>  #>   Res.Df Df  Chisq Pr(>Chisq)   #> 1    596                        #> 2    605  9 18.043    0.03468 * #> --- #> Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1  # Model comparison between spline model and linear # model; note they are nested but not symbolically # nested fit_s <- glm_weightit(re78 ~ splines::ns(age, df =4),                       data = lalonde )  fit_l <- glm_weightit( re78 ~ age, data = lalonde )  anova(fit_s, fit_l) #>  #> Wald test #> Variance: HC0 robust #>  #> Model 1: re78 ~ splines::ns(age, df = 4) #> Model 2: re78 ~ age #>  #>   Res.Df Df  Chisq Pr(>Chisq)    #> 1    609                         #> 2    612  3 14.166   0.002688 ** #> --- #> Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1"},{"path":"https://ngreifer.github.io/WeightIt/reference/as.weightit.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a weightit object manually — as.weightit","title":"Create a weightit object manually — as.weightit","text":"function allows users get benefits weightit object using weights estimated weightit() weightitMSM(). benefits include diagnostics, plots, direct compatibility cobalt assessing balance.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/as.weightit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a weightit object manually — as.weightit","text":"","code":"as.weightit(x, ...)  # S3 method for class 'weightit.fit' as.weightit(x, covs = NULL, ...)  # Default S3 method as.weightit(   x,   treat,   covs = NULL,   estimand = NULL,   s.weights = NULL,   ps = NULL,   ... )  as.weightitMSM(x, ...)  # Default S3 method as.weightitMSM(   x,   treat.list,   covs.list = NULL,   estimand = NULL,   s.weights = NULL,   ps.list = NULL,   ... )"},{"path":"https://ngreifer.github.io/WeightIt/reference/as.weightit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a weightit object manually — as.weightit","text":"x required; numeric vector weights, one unit, weightit.fit object weightit.fit(). ... additional arguments. must named. included output object. covs optional data.frame covariates. using WeightIt functions, necessary, use cobalt . Note using weightit.fit object, matrix supplied covs argument weightit.fit() unless factor/character variables . Ideally original, unprocessed covariate data frame factor variables included. treat vector treatment statuses, one unit. Required x vector weights. estimand optional character length 1 giving estimand. text checked. s.weights optional numeric vector sampling weights, one unit. ps optional numeric vector propensity scores, one unit. treat.list list treatment statuses time point. covs.list optional list data.frames covariates covariates time point. using WeightIt functions, necessary, use cobalt . ps.list optional list numeric vectors propensity scores time point.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/as.weightit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a weightit object manually — as.weightit","text":"object class weightit (.weightit()) weightitMSM (.weightitMSM()).","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/as.weightit.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a weightit object manually — as.weightit","text":"","code":"treat <- rbinom(500, 1, .3) weights <- rchisq(500, df = 2)  W <- as.weightit(weights, treat = treat, estimand = \"ATE\") summary(W) #>                   Summary of weights #>  #> - Weight ranges: #>  #>            Min                                   Max #> treated 0.0192 |-------------------|          8.8431 #> control 0.0038 |---------------------------| 12.0278 #>  #> - Units with the 5 most extreme weights by group: #>                                               #>              88    271     60     403      99 #>  treated 6.2296 7.3136 7.6141    7.65  8.8431 #>             169    187    430      50     415 #>  control 8.8257 8.8302 9.9044 11.1675 12.0278 #>  #> - Weight statistics: #>  #>         Coef of Var   MAD Entropy # Zeros #> treated       0.947 0.766   0.411       0 #> control       1.002 0.734   0.430       0 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted  346.    154.   #> Weighted    172.99   81.44  # See ?weightit.fit for using as.weightit() with a # weightit.fit object."},{"path":"https://ngreifer.github.io/WeightIt/reference/calibrate.html","id":null,"dir":"Reference","previous_headings":"","what":"Calibrate Propensity Score Weights — calibrate","title":"Calibrate Propensity Score Weights — calibrate","text":"calibrate() calibrates propensity scores used weights. involves fitting new propensity score model using logistic isotonic regression previously estimated propensity score sole predictor. Weights computed using new propensity score.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/calibrate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calibrate Propensity Score Weights — calibrate","text":"","code":"calibrate(x, ...)  # Default S3 method calibrate(x, treat, s.weights = NULL, data = NULL, method = \"platt\", ...)  # S3 method for class 'weightit' calibrate(x, ...)"},{"path":"https://ngreifer.github.io/WeightIt/reference/calibrate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calibrate Propensity Score Weights — calibrate","text":"x weightit object vector propensity scores. binary treatments supported. ... used. treat vector treatment status unit. binary treatments supported. s.weights vector sampling weights name variable data contains sampling weights. data optional data frame containing variable named s.weights supplied string. method character; method calibration used. Allowable options include \"platt\" (default) Platt scaling described Gutman et al. (2024) \"isoreg\" isotonic regression described van der Laan et al. (2024) implemented isoreg().","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/calibrate.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calibrate Propensity Score Weights — calibrate","text":"input weightit object, output weightit object propensity scores replaced calibrated propensity scores weights replaced weights computed calibrated propensity scores. input numeric vector weights, output numeric vector calibrated propensity scores.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/calibrate.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Calibrate Propensity Score Weights — calibrate","text":"Gutman, R., Karavani, E., & Shimoni, Y. (2024). Improving Inverse Probability Weighting Post-calibrating Propensity Scores. Epidemiology, 35(4). doi:10.1097/EDE.0000000000001733 van der Laan, L., Lin, Z., Carone, M., & Luedtke, . (2024). Stabilized Inverse Probability Weighting via Isotonic Calibration (arXiv:2411.06342). arXiv. http://arxiv.org/abs/2411.06342","code":""},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/reference/calibrate.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calibrate Propensity Score Weights — calibrate","text":"","code":"library(\"cobalt\") data(\"lalonde\", package = \"cobalt\")  #Using GBM to estimate weights (W <- weightit(treat ~ age + educ + married +                  nodegree + re74, data = lalonde,                method = \"gbm\", estimand = \"ATT\",                criterion = \"smd.max\")) #> A weightit object #>  - method: \"gbm\" (propensity score weighting with GBM) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 2-category #>  - estimand: ATT (focal: 1) #>  - covariates: age, educ, married, nodegree, re74 summary(W) #>                   Summary of weights #>  #> - Weight ranges: #>  #>            Min                                   Max #> treated 1.0000   ||                           1.0000 #> control 0.0028 |---------------------------| 17.3885 #>  #> - Units with the 5 most extreme weights by group: #>                                             #>               5      4     3      2       1 #>  treated      1      1     1      1       1 #>             585    557   592    374     608 #>  control 3.1627 3.5102 6.074 6.3303 17.3885 #>  #> - Weight statistics: #>  #>         Coef of Var   MAD Entropy # Zeros #> treated       0.000 0.000   0.000       0 #> control       3.165 1.101   1.202       0 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted  429.       185 #> Weighted     39.02     185  #Calibrating the GBM propensity scores Wc <- calibrate(W)  #Calibrating propensity scores directly PSc <- calibrate(W$ps, treat = lalonde$treat)"},{"path":"https://ngreifer.github.io/WeightIt/reference/dot-weightit_methods.html","id":null,"dir":"Reference","previous_headings":"","what":"Weighting methods — .weightit_methods","title":"Weighting methods — .weightit_methods","text":".weightit_methods list containing allowable weighting methods can supplied name method argument weightit(), weightitMSM(), weightit.fit(). entry corresponds allowed method contains information options allowed method. list primarily internal use checking functions WeightIt, might use package authors want support different weighting methods.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/dot-weightit_methods.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Weighting methods — .weightit_methods","text":"","code":".weightit_methods"},{"path":"https://ngreifer.github.io/WeightIt/reference/dot-weightit_methods.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Weighting methods — .weightit_methods","text":"object class list length 10.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/dot-weightit_methods.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Weighting methods — .weightit_methods","text":"component list containing following components: treat_type least one \"binary\", \"multi-category\", \"continuous\" indicating treatment types available method. estimand estimands available method. methods support binary multi-category treatments accept \"ATE\", \"ATT\", \"ATC\", well estimands depending method. See get_w_from_ps() details estimand means. alias character vector aliases method. alias supplied, corresponding method still dispatched. example, canonical method request entropy balancing \"ebal\", \"ebalance\" \"entropy\" also work. first value canonical name. description string containing description name English. ps logical whether propensity scores returned method binary treatments. Propensity scores never returned multi-category continuous treatments. msm_valid logical whether method can validly used longitudinal treatments. msm_method_available logical whether version method can used estimates weights using single model rather multiplying weights across time points. related .MSM.method argument weightitMSM(). subclass_ok logical whether subclass can supplied compute subclassification weights propensity scores. packages_needed character vector minimal packages required use method. methods may require additional packages certain options. s.weights_ok logical whether sampling weights can used method. missing character vector allowed options can supplied missing missing data present. methods accept \"ind\" missingness indicator approach; methods accept additional values. moments_int_ok logical whether moments, int, quantile can used method. moments_default moments_int_ok TRUE, default value moments used method. methods, 1. density_ok logical whether arguments control density can used method used continuous treatment. stabilize_ok logical whether stabilize argument (num.formula longitudinal treatments) can used method. plot.weightit_ok logical whether plot() can used weightit output method.","code":""},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/reference/dot-weightit_methods.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Weighting methods — .weightit_methods","text":"","code":"# Get all acceptable names names(.weightit_methods) #>  [1] \"glm\"       \"bart\"      \"cbps\"      \"ebal\"      \"energy\"    \"gbm\"       #>  [7] \"ipt\"       \"npcbps\"    \"optweight\" \"super\"      # Get all acceptable names and aliases lapply(.weightit_methods, `[[`, \"alias\") #> $glm #> [1] \"glm\" \"ps\"  #>  #> $bart #> [1] \"bart\" #>  #> $cbps #> [1] \"cbps\"  \"cbgps\" #>  #> $ebal #> [1] \"ebal\"     \"ebalance\" \"entropy\"  #>  #> $energy #> [1] \"energy\" \"dcows\"  #>  #> $gbm #> [1] \"gbm\" \"gbr\" #>  #> $ipt #> [1] \"ipt\" #>  #> $npcbps #> [1] \"npcbps\"  \"npcbgps\" #>  #> $optweight #> [1] \"optweight\" \"sbw\"       #>  #> $super #> [1] \"super\"        \"superlearner\" #>   # Which estimands are allowed with `method = \"bart\"` .weightit_methods[[\"bart\"]]$estimand #> [1] \"ATE\"  \"ATT\"  \"ATC\"  \"ATO\"  \"ATM\"  \"ATOS\"  # All methods that support continuous treatments supp <- sapply(.weightit_methods, function(x) {   \"continuous\" %in% x$treat_type }) names(.weightit_methods)[supp] #> [1] \"glm\"       \"bart\"      \"cbps\"      \"ebal\"      \"energy\"    \"gbm\"       #> [7] \"npcbps\"    \"optweight\" \"super\"      # All methods that return propensity scores (for # binary treatments only) supp <- sapply(.weightit_methods, `[[`, \"ps\") names(.weightit_methods)[supp] #> [1] \"glm\"   \"bart\"  \"cbps\"  \"gbm\"   \"ipt\"   \"super\""},{"path":"https://ngreifer.github.io/WeightIt/reference/get_w_from_ps.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute weights from propensity scores — get_w_from_ps","title":"Compute weights from propensity scores — get_w_from_ps","text":"Given vector matrix propensity scores, outputs vector weights target provided estimand.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/get_w_from_ps.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute weights from propensity scores — get_w_from_ps","text":"","code":"get_w_from_ps(   ps,   treat,   estimand = \"ATE\",   focal = NULL,   treated = NULL,   subclass = NULL,   stabilize = FALSE )"},{"path":"https://ngreifer.github.io/WeightIt/reference/get_w_from_ps.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute weights from propensity scores — get_w_from_ps","text":"ps vector, matrix, data frame propensity scores. See Details. treat vector treatment status individual. See Details. estimand desired estimand weights target. Current options include \"ATE\" (average treatment effect), \"ATT\" (average treatment effect treated), \"ATC\" (average treatment effect control), \"ATO\" (average treatment effect overlap), \"ATM\" (average treatment effect matched sample), \"ATOS\" (average treatment effect optimal subset). See Details. focal estimand \"ATT\" \"ATC\", group consider (focal) \"treated\" \"control\" group, respectively. NULL estimand \"ATT\" \"ATC\", estimand automatically set \"ATT\". treated treatment binary, value treat considered \"treated\" group (.e., group propensity scores probability ). NULL, get_w_from_ps() attempt figure using heuristics. really matters treat values 0 1 ps given vector unnamed single-column matrix data frame. subclass numeric; number subclasses use computing weights using marginal mean weighting stratification (also known fine stratification). NULL, standard inverse probability weights (extensions) computed; number greater 1, subclasses formed weights computed based subclass membership. estimand must \"ATE\", \"ATT\", \"ATC\" subclass non-NULL. See Details. stabilize logical; whether compute stabilized weights . simply involves multiplying unit's weight proportion units treatment group. saturated outcome models balance checking, make difference; otherwise, can improve performance.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/get_w_from_ps.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute weights from propensity scores — get_w_from_ps","text":"vector weights. subclass NULL, subclasses returned \"subclass\" attribute. estimand = \"ATOS\", chosen value alpha (smallest propensity score allowed remain sample) returned \"alpha\" attribute.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/get_w_from_ps.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Compute weights from propensity scores — get_w_from_ps","text":"get_w_from_ps() applies formula computing weights propensity scores desired estimand. formula estimand , \\(A_i\\) treatment value unit \\(\\) taking values \\(\\mathcal{} = (1, \\ldots, g)\\), \\(p_{, }\\) probability receiving treatment level \\(\\) unit \\(\\), \\(f\\) focal group (treated group ATT control group ATC): $$ \\begin{aligned} w^{ATE}_i &= 1 / p_{A_i, } \\\\ w^{ATT}_i &= w^{ATE}_i \\times p_{f, } \\\\ w^{ATO}_i &= w^{ATE}_i / \\sum_{\\\\mathcal{}}{1/p_{, }} \\\\ w^{ATM}_i &= w^{ATE}_i \\times \\min(p_{1, }, \\ldots, p_{g, }) \\\\ w^{ATOS}_i &= w^{ATE}_i \\times \\mathbb{1}\\left(\\alpha < p_{2, } < 1 - \\alpha\\right) \\end{aligned} $$ get_w_from_ps() can used binary multi-category treatments.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/get_w_from_ps.html","id":"supplying-the-ps-argument","dir":"Reference","previous_headings":"","what":"Supplying the ps argument","title":"Compute weights from propensity scores — get_w_from_ps","text":"ps argument can entered two ways: numeric matrix row unit (named) column treatment level, cell corresponding probability receiving corresponding treatment level numeric vector value unit corresponding probability \"treated\" (allowed binary treatments) supplied vector, get_w_from_ps() know value treat corresponds \"treated\" group. 0/1 variables, 1 considered treated. types variables, get_w_from_ps() try figure using heuristics, safer supply argument treated. estimand \"ATT\" \"ATC\", supplying value focal sufficient (ATT, focal treated group, ATC, focal control group). supplied matrix, columns must named levels treatment, assumed column corresponds probability treatment group. safest way supply ps unless treat 0/1 variable. estimand \"ATT\" \"ATC\", value focal must specified.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/get_w_from_ps.html","id":"marginal-mean-weighting-through-stratification-mmws-","dir":"Reference","previous_headings":"","what":"Marginal mean weighting through stratification (MMWS)","title":"Compute weights from propensity scores — get_w_from_ps","text":"subclass NULL, MMWS weights computed. implementation differs slightly described Hong (2010, 2012). First, subclasses formed finding quantiles propensity scores target group (ATE, units; ATT ATC, just units focal group). subclasses lacking members treatment group filled neighboring subclasses subclass always least one member treatment group. new subclass-propensity score matrix formed, unit's subclass-propensity score treatment value computed proportion units treatment value unit's subclass. example, subclass 10 treated units 90 control units , subclass-propensity score treated .1 subclass-propensity score control .9 units subclass. multi-category treatments, propensity scores treatment stratified separately described Hong (2012); binary treatments, one set propensity scores stratified subclass-propensity scores treatment computed complement propensity scores stratified treatment. subclass-propensity scores computed, standard propensity score weighting formulas used compute unstabilized MMWS weights. estimate MMWS weights equivalent described Hong (2010, 2012), stabilize must set TRUE, , standard propensity score weights, optional. Note MMWS weights also known fine stratification weights described Desai et al. (2017).","code":""},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/reference/get_w_from_ps.html","id":"binary-treatments","dir":"Reference","previous_headings":"","what":"Binary treatments","title":"Compute weights from propensity scores — get_w_from_ps","text":"estimand = \"ATO\" Li, F., Morgan, K. L., & Zaslavsky, . M. (2018). Balancing covariates via propensity score weighting. Journal American Statistical Association, 113(521), 390–400. doi:10.1080/01621459.2016.1260466 estimand = \"ATM\" Li, L., & Greene, T. (2013). Weighting Analogue Pair Matching Propensity Score Analysis. International Journal Biostatistics, 9(2). doi:10.1515/ijb-2012-0030 estimand = \"ATOS\" Crump, R. K., Hotz, V. J., Imbens, G. W., & Mitnik, O. . (2009). Dealing limited overlap estimation average treatment effects. Biometrika, 96(1), 187–199. doi:10.1093/biomet/asn055 estimands Austin, P. C. (2011). Introduction Propensity Score Methods Reducing Effects Confounding Observational Studies. Multivariate Behavioral Research, 46(3), 399–424. doi:10.1080/00273171.2011.568786 Marginal mean weighting stratification (MMWS) Hong, G. (2010). Marginal mean weighting stratification: Adjustment selection bias multilevel data. Journal Educational Behavioral Statistics, 35(5), 499–531. doi:10.3102/1076998609359785 Desai, R. J., Rothman, K. J., Bateman, B. . T., Hernandez-Diaz, S., & Huybrechts, K. F. (2017). Propensity-score-based Fine Stratification Approach Confounding Adjustment Exposure Infrequent: Epidemiology, 28(2), 249–257. doi:10.1097/EDE.0000000000000595","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/get_w_from_ps.html","id":"multi-category-treatments","dir":"Reference","previous_headings":"","what":"Multi-Category Treatments","title":"Compute weights from propensity scores — get_w_from_ps","text":"estimand = \"ATO\" Li, F., & Li, F. (2019). Propensity score weighting causal inference multiple treatments. Annals Applied Statistics, 13(4), 2389–2415. doi:10.1214/19-AOAS1282 estimand = \"ATM\" Yoshida, K., Hernández-Díaz, S., Solomon, D. H., Jackson, J. W., Gagne, J. J., Glynn, R. J., & Franklin, J. M. (2017). Matching weights simultaneously compare three treatment groups: Comparison three-way matching. Epidemiology (Cambridge, Mass.), 28(3), 387–395. doi:10.1097/EDE.0000000000000627 estimands McCaffrey, D. F., Griffin, B. ., Almirall, D., Slaughter, M. E., Ramchand, R., & Burgette, L. F. (2013). Tutorial Propensity Score Estimation Multiple Treatments Using Generalized Boosted Models. Statistics Medicine, 32(19), 3388–3414. doi:10.1002/sim.5753 Marginal mean weighting stratification Hong, G. (2012). Marginal mean weighting stratification: generalized method evaluating multivalued multiple treatments nonexperimental data. Psychological Methods, 17(1), 44–60. doi:10.1037/a0024918","code":""},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/reference/get_w_from_ps.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute weights from propensity scores — get_w_from_ps","text":"","code":"library(\"cobalt\") data(\"lalonde\", package = \"cobalt\")  ps.fit <- glm(treat ~ age + educ + race + married +                 nodegree + re74 + re75, data = lalonde,               family = binomial) ps <- ps.fit$fitted.values  w1 <- get_w_from_ps(ps, treat = lalonde$treat,                     estimand = \"ATT\")  treatAB <- factor(ifelse(lalonde$treat == 1, \"A\", \"B\")) w2 <- get_w_from_ps(ps, treat = treatAB,                     estimand = \"ATT\", focal = \"A\") all.equal(w1, w2) #> [1] TRUE w3 <- get_w_from_ps(ps, treat = treatAB,                     estimand = \"ATT\", treated = \"A\") all.equal(w1, w3) #> [1] TRUE  # Using MMWS w4 <- get_w_from_ps(ps, treat = lalonde$treat,                     estimand = \"ATE\", subclass = 20,                     stabilize = TRUE)  # A multi-category example using predicted probabilities # from multinomial logistic regression T3 <- factor(sample(c(\"A\", \"B\", \"C\"), nrow(lalonde),                     replace = TRUE))  multi.fit <- multinom_weightit(   T3 ~ age + educ + race + married +     nodegree + re74 + re75, data = lalonde,   vcov = \"none\" )  ps.multi <- fitted(multi.fit) head(ps.multi) #>           A         B         C #> 1 0.3275345 0.2714686 0.4009970 #> 2 0.3110696 0.3349112 0.3540192 #> 3 0.3261981 0.3602605 0.3135413 #> 4 0.3176130 0.3061721 0.3762149 #> 5 0.3214072 0.3244588 0.3541339 #> 6 0.3294088 0.3469696 0.3236216  w5 <- get_w_from_ps(ps.multi, treat = T3,                     estimand = \"ATE\")"},{"path":"https://ngreifer.github.io/WeightIt/reference/glm_weightit-methods.html","id":null,"dir":"Reference","previous_headings":"","what":"Methods for glm_weightit() objects — glm_weightit-methods","title":"Methods for glm_weightit() objects — glm_weightit-methods","text":"page documents methods objects returned glm_weightit(), lm_weightit(), ordinal_weightit(), multinom_weightit(), coxph_weightit(). predict() methods described predict.glm_weightit() anova() methods described anova.glm_weightit().","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/glm_weightit-methods.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Methods for glm_weightit() objects — glm_weightit-methods","text":"","code":"# S3 method for class 'glm_weightit' summary(object, ci = FALSE, level = 0.95, transform = NULL, vcov = NULL, ...)  # S3 method for class 'multinom_weightit' summary(object, ci = FALSE, level = 0.95, transform = NULL, vcov = NULL, ...)  # S3 method for class 'ordinal_weightit' summary(   object,   ci = FALSE,   level = 0.95,   transform = NULL,   thresholds = TRUE,   vcov = NULL,   ... )  # S3 method for class 'coxph_weightit' summary(object, ci = FALSE, level = 0.95, transform = NULL, vcov = NULL, ...)  # S3 method for class 'glm_weightit' print(x, digits = max(3L, getOption(\"digits\") - 3L), ...)  # S3 method for class 'glm_weightit' vcov(object, complete = TRUE, vcov = NULL, ...)  # S3 method for class 'glm_weightit' estfun(x, asympt = TRUE, ...)  # S3 method for class 'glm_weightit' update(object, formula. = NULL, ..., evaluate = TRUE)"},{"path":"https://ngreifer.github.io/WeightIt/reference/glm_weightit-methods.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Methods for glm_weightit() objects — glm_weightit-methods","text":"object, x output one modeling functions. ci logical; whether display Wald confidence intervals estimated coefficients. Default FALSE. (Note: argument can also supplied conf.int.) level ci = TRUE, desired confidence level. transform function used transform coefficients, e.g., exp (can also supplied string, e.g., \"exp\"); passed match.fun() used coefficients. ci = TRUE, also applied confidence interval bounds. specified, standard error omitted output. Default transformation. vcov either string indicating method used compute variance estimated parameters object, function used extract variance, variance matrix . Default use variance matrix already present object. string function, arguments passed ... supplied method function. (Note: vcov(), can also supplied type.) ... vcov() summary() confint() vcov supplied, arguments used compute variance matrix depending method supplied vcov, e.g., cluster, R, fwb.args. update(), additional arguments call arguments changed values. See glm_weightit() details. thresholds logical; whether include thresholds summary() output ordinal_weightit objects. Default TRUE. digits number significant digits     passed format(coef(x), .)     print()ing. complete logical; whether full variance-covariance matrix returned also case -determined system coefficients undefined coef(.) contains NAs correspondingly. complete = TRUE, vcov() compatible coef() also singular case. asympt logical; estfun(), whether use asymptotic empirical estimating functions account estimation weights (Mparts available). Default TRUE. Set FALSE ignore estimation weights. Ignored Mparts available argument supplied weightit fitting function. formula. changes model formula, passed new argument update.formula(). evaluate whether evaluate call (TRUE, default) just return .","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/glm_weightit-methods.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Methods for glm_weightit() objects — glm_weightit-methods","text":"summary() returns summary.glm_weightit() object, print() method. coxph_weightit() objects, print() summary() methods like glm objects coxph objects. Otherwise, methods return type object generics.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/glm_weightit-methods.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Methods for glm_weightit() objects — glm_weightit-methods","text":"vcov() default extracts parameter covariance matrix already computed fitting function, summary() confint() uses covariance matrix compute standard errors Wald confidence intervals (internally calling confint.lm()), respectively. Supplying arguments vcov ... compute new covariance matrix. cluster supplied original fitting function, incorporated newly computed covariance matrix unless cluster = NULL specified vcov(), summary(), confint(). arguments (e.g., R fwb.args), defaults used glm_weightit(). Note vcov = \"BS\" vcov = \"FWB\" (vcov = \"const\" multinom_weightit ordinal_weightit objects), environment fitting function used, changes environment may affect calculation. always safer simply recompute fitted object new covariance matrix modify vcov argument, can quicker just request new covariance matrix refitting model slow. update() updates fitted model object new arguments, e.g., new model formula, dataset, variance matrix. arguments control computation variance supplied, variance recalculated (.e., parameters re-estimated). data supplied, weightit supplied, weightit object originally passed model fitting function, weightit object re-fit new dataset model refit using new weights new data. , calling update(obj, data = d) equivalent calling update(obj, data = d, weightit = update(obj$weightit, data = d)) weightit object supplied model fitting function. Similarly, supplying s.weights weights passes argument weightit() refit. s.weights weights supplied weightit object present, fake one containing just supplied weights created. estfun() extracts empirical estimating functions fitted model, optionally accounting estimation weights (available). , along bread(), used sandwich::sandwich() compute robust covariance matrix estimated coefficients. See glm_weightit() vcov() details.","code":""},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/reference/glm_weightit-methods.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Methods for glm_weightit() objects — glm_weightit-methods","text":"","code":"## See more examples at ?glm_weightit"},{"path":"https://ngreifer.github.io/WeightIt/reference/glm_weightit.html","id":null,"dir":"Reference","previous_headings":"","what":"Fitting Weighted Generalized Linear Models — glm_weightit","title":"Fitting Weighted Generalized Linear Models — glm_weightit","text":"glm_weightit() used fit generalized linear models covariance matrix accounts estimation weights, supplied. lm_weightit() wrapper glm_weightit() Gaussian family identity link (.e., linear model). ordinal_weightit() fits proportional odds ordinal regression models. multinom_weightit() fits multinomial logistic regression models. coxph_weightit() fits Cox proportional hazards model wrapper survival::coxph(). default, functions use M-estimation construct robust covariance matrix using estimating equations weighting model outcome model available.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/glm_weightit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fitting Weighted Generalized Linear Models — glm_weightit","text":"","code":"glm_weightit(   formula,   data,   family = gaussian,   weightit = NULL,   vcov = NULL,   cluster,   R = 500L,   offset,   start = NULL,   etastart,   mustart,   control = list(...),   x = FALSE,   y = TRUE,   contrasts = NULL,   fwb.args = list(),   br = FALSE,   ... )  lm_weightit(   formula,   data,   weightit = NULL,   vcov = NULL,   cluster,   R = 500L,   offset,   x = FALSE,   y = TRUE,   contrasts = NULL,   fwb.args = list(),   ... )  ordinal_weightit(   formula,   data,   link = \"logit\",   weightit = NULL,   vcov = NULL,   cluster,   R = 500L,   offset,   start = NULL,   control = list(...),   x = FALSE,   y = TRUE,   contrasts = NULL,   fwb.args = list(),   ... )  multinom_weightit(   formula,   data,   link = \"logit\",   weightit = NULL,   vcov = NULL,   cluster,   R = 500L,   offset,   start = NULL,   control = list(...),   x = FALSE,   y = TRUE,   contrasts = NULL,   fwb.args = list(),   ... )  coxph_weightit(   formula,   data,   weightit = NULL,   vcov = NULL,   cluster,   R = 500L,   x = FALSE,   y = TRUE,   fwb.args = list(),   ... )"},{"path":"https://ngreifer.github.io/WeightIt/reference/glm_weightit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fitting Weighted Generalized Linear Models — glm_weightit","text":"formula object class \"formula\" (one can coerced class): symbolic description model fitted. coxph_weightit(), see survival::coxph() specified. data data frame containing variables model. found data, variables taken environment(formula), typically environment function called. family description error distribution link function used model. can character string naming family function, family function result call family function. See family details family functions. weightit weightit weightitMSM object; output call weightit() weightitMSM(). supplied, unweighted model fit. vcov string; method used compute variance estimated parameters. Allowable options include \"asympt\", uses asymptotically correct M-estimation-based method accounts estimation weights available; \"const\", uses usual maximum likelihood estimates (available weightit supplied); \"HC0\", computes robust sandwich variance treating weights (supplied) fixed; \"BS\", uses traditional bootstrap (including re-estimation weights, supplied); \"FWB\", uses fractional weighted bootstrap implemented fwbfwb (including re-estimation weights, supplied); \"none\" omit calculation variance matrix. NULL (default), use \"asympt\" weightit supplied M-estimation available \"HC0\" otherwise. See vcov_type component outcome object see used. cluster optional; computing cluster-robust variance matrix, variable indicating clustering observations, list (data frame) thereof, one-sided formula specifying variable(s) fitted model used. Note cluster-robust variance matrix uses correction small samples, done sandwich::vcovCL() default. Cluster-robust variance calculations available vcov \"asympt\", \"HC0\", \"BS\", \"FWB\". R number bootstrap replications vcov \"BS\" \"FWB\". Default 500. Ignored otherwise. offset optional; numeric vector containing model offset. See offset(). offset can also preset model formula. start optional starting values coefficients. etastart, mustart optional starting values linear predictor vector means. Passed glm(). control list parameters controlling fitting process. x, y logical values indicating whether response vector model matrix used fitting process returned components returned value. contrasts optional list defining contrasts factor variables. See model.matrix(). fwb.args optional list arguments supply fwbfwb vcov = \"FWB\". br logical; whether use bias-reduced regression implemented brglm2brglmFit. TRUE, arguments passed control ... passed brglm2brglmControl. ... arguments used form default control argument supplied directly. link plor_weightit() multinom_weightit(), string corresponding desired link function. ordinal_weightit(), allowed binomial() accepted; multinom_weightit(), \"logit\" allowed. Default \"logit\" ordinal multinomial logistic regression, respectively.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/glm_weightit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fitting Weighted Generalized Linear Models — glm_weightit","text":"lm_weightit() glm_weightit(), glm_weightit object, inherits glm. ordinal_weightit() multinom_weightit(), ordinal_weightit multinom_weightit, respectively. coxph_weightit(), coxph_weightit object, inherits coxph. See survival::coxph() details. Unless vcov = \"none\", vcov component contains covariance matrix adjusted estimation weights requested compatible weightit object supplied. vcov_type component contains type variance matrix requested. cluster supplied, stored \"cluster\" attribute output object, even used. model component output object (also model.frame() output) include two extra columns weightit supplied: (weights) containing weights used model (product estimated weights sampling weights, ) (s.weights) containing sampling weights, 1 s.weights supplied original weightit() call.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/glm_weightit.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fitting Weighted Generalized Linear Models — glm_weightit","text":"glm_weightit() essentially wrapper glm() optionally computes coefficient variance matrix can adjusted account estimation weights weightit weightitMSM object supplied weightit argument. argument supplied weightit \"Mparts\" attribute supplied object, default variance matrix returned \"HC0\" sandwich variance matrix, robust misspecification outcome family (including heteroscedasticity). Otherwise, default variance matrix uses M-estimation additionally adjust estimation weights. possible, often yields smaller (accurate) standard errors. See individual methods pages see whether \"Mparts\" attribute included supplied object. request variance matrix computed account estimation weights even compatible weightit object supplied, set vcov = \"HC0\", treats weights fixed. Bootstrapping can also used compute coefficient variance matrix; vcov = \"BS\" vcov = \"FWB\", implement traditional resampling-based fractional weighted bootstrap, respectively, entire process estimating weights fitting outcome model repeated bootstrap samples (weightit object supplied). accounts estimation weights can used weighting method. important set seed using set.seed() ensure replicability results. fractional weighted bootstrap reliable requires weighting method accept sampling weights (, get error ). Setting vcov = \"FWB\" supplying fwb.args = list(wtype = \"multinom\") also performs resampling-based bootstrap additional features fwb provides (e.g., progress bar parallelization) expense needing fwb installed. multinom_weightit() implements multinomial logistic regression using custom function WeightIt. implementation less robust failures multinomial logistic regression solvers used caution. Estimation coefficients align mlogit::mlogit() mclogit::mblogit(). ordinal_weightit() implements proportional odds ordinal regression using custom function WeightIt. Estimation coefficients align MASS::polr(). coxph_weightit() wrapper survival::coxph() fit weighted survival models. differs coxph() cluster argument (used) specified one-sided formula (can include multiple clustering variables) uses small sample correction cluster variance estimates specified. Currently, M-estimation supported, bootstrapping (.e., vcov = \"BS\" \"FWB\") way correctly adjust estimation weights. default, robust variance estimated treating weights fixed, variance returned robust = TRUE coxph(). Functions sandwich package can compute standard errors fitting, regardless vcov specified, though ignore estimation weights, . adjustment done estimation weights (.e., weightit argument supplied \"Mparts\" component supplied object), default variance matrix produced glm_weightit() align sandwich::vcovHC(. type = \"HC0\") sandwich::vcovCL(., type = \"HC0\", cluster = cluster) cluster supplied. types available models.","code":""},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/reference/glm_weightit.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fitting Weighted Generalized Linear Models — glm_weightit","text":"","code":"data(\"lalonde\", package = \"cobalt\")  # Logistic regression ATT weights w.out <- weightit(treat ~ age + educ + married + re74,                   data = lalonde, method = \"glm\",                   estimand = \"ATT\")  # Linear regression outcome model that adjusts # for estimation of weights fit1 <- lm_weightit(re78 ~ treat, data = lalonde,                     weightit = w.out)  summary(fit1) #>  #> Call: #> lm_weightit(formula = re78 ~ treat, data = lalonde, weightit = w.out) #>  #> Coefficients: #>             Estimate Std. Error z value Pr(>|z|)     #> (Intercept)   5515.9      376.5  14.649   <1e-06 *** #> treat          833.2      669.1   1.245    0.213     #> Standard error: HC0 robust (adjusted for estimation of weights) #>   # Linear regression outcome model that treats weights # as fixed fit2 <- lm_weightit(re78 ~ treat, data = lalonde,                     weightit = w.out, vcov = \"HC0\")  summary(fit2) #>  #> Call: #> lm_weightit(formula = re78 ~ treat, data = lalonde, weightit = w.out,  #>     vcov = \"HC0\") #>  #> Coefficients: #>             Estimate Std. Error z value Pr(>|z|)     #> (Intercept)   5515.9      353.5  15.604   <1e-06 *** #> treat          833.2      676.6   1.232    0.218     #> Standard error: HC0 robust #>  # example code  # Linear regression outcome model that bootstraps # estimation of weights and outcome model fitting # using fractional weighted bootstrap with \"Mammen\" # weights set.seed(123) fit3 <- lm_weightit(re78 ~ treat, data = lalonde,                     weightit = w.out,                     vcov = \"FWB\",                     R = 50, #should use way more                     fwb.args = list(wtype = \"mammen\"))  summary(fit3) #>  #> Call: #> lm_weightit(formula = re78 ~ treat, data = lalonde, weightit = w.out,  #>     vcov = \"FWB\", R = 50, fwb.args = list(wtype = \"mammen\")) #>  #> Coefficients: #>             Estimate Std. Error z value Pr(>|z|)     #> (Intercept)   5515.9      361.5  15.257   <1e-06 *** #> treat          833.2      644.4   1.293    0.196     #> Standard error: fractional weighted bootstrap #>  # Multinomial logistic regression outcome model # that adjusts for estimation of weights lalonde$re78_3 <- factor(findInterval(lalonde$re78,                                       c(0, 5e3, 1e4)))  fit4 <- multinom_weightit(re78_3 ~ treat,                           data = lalonde,                           weightit = w.out)  summary(fit4) #>  #> Call: #> multinom_weightit(formula = re78_3 ~ treat, data = lalonde, weightit = w.out) #>  #> Coefficients: #>               Estimate Std. Error z value Pr(>|z|)     #> 2~(Intercept) -0.90398    0.14906  -6.064   <1e-06 *** #> 2~treat        0.05006    0.23633   0.212    0.832     #> 3~(Intercept) -1.02170    0.15699  -6.508   <1e-06 *** #> 3~treat        0.12015    0.23835   0.504    0.614     #> Standard error: HC0 robust (adjusted for estimation of weights) #>   # Ordinal probit regression that adjusts for estimation # of weights fit5 <- ordinal_weightit(ordered(re78_3) ~ treat,                          data = lalonde,                          link = \"probit\",                          weightit = w.out)  summary(fit5) #>  #> Call: #> ordinal_weightit(formula = ordered(re78_3) ~ treat, data = lalonde,  #>     link = \"probit\", weightit = w.out) #>  #> Coefficients: #>       Estimate Std. Error z value Pr(>|z|) #> treat   0.0554     0.1114   0.498    0.619 #> Standard error: HC0 robust (adjusted for estimation of weights) #>  #> Thresholds: #>     Estimate Std. Error z value Pr(>|z|)     #> 1|2  0.16926    0.07479   2.263   0.0236 *   #> 2|3  0.82476    0.08193  10.066   <1e-06 ***"},{"path":"https://ngreifer.github.io/WeightIt/reference/make_full_rank.html","id":null,"dir":"Reference","previous_headings":"","what":"Make a design matrix full rank — make_full_rank","title":"Make a design matrix full rank — make_full_rank","text":"writing user-defined methods use weightit(), may necessary take potentially non-full rank covs data frame make full rank use downstream function. function performs operation.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/make_full_rank.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Make a design matrix full rank — make_full_rank","text":"","code":"make_full_rank(mat, with.intercept = TRUE)"},{"path":"https://ngreifer.github.io/WeightIt/reference/make_full_rank.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Make a design matrix full rank — make_full_rank","text":"mat numeric matrix data frame transformed. Typically contains covariates. NAs allowed. .intercept whether intercept (.e., vector 1s) added mat making full rank. TRUE, intercept used determining whether column linearly dependent others. Regardless, intercept included output.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/make_full_rank.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Make a design matrix full rank — make_full_rank","text":"object type mat containing linearly independent columns.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/make_full_rank.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Make a design matrix full rank — make_full_rank","text":"make_full_rank() calls qr() find rank linearly independent columns mat, retained others dropped. .intercept set TRUE, intercept column added matrix calling qr(). Note dependent columns appear later mat dropped first. See example method_user.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/make_full_rank.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Make a design matrix full rank — make_full_rank","text":"Older versions drop columns one value. .intercept = FALSE, one column one value, removed, function though intercept present; column one value, first one remain.","code":""},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/reference/make_full_rank.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Make a design matrix full rank — make_full_rank","text":"","code":"set.seed(1000) c1 <- rbinom(10, 1, .4) c2 <- 1-c1 c3 <- rnorm(10) c4 <- 10*c3 mat <- data.frame(c1, c2, c3, c4)  make_full_rank(mat) #leaves c2 and c4 #>    c1          c3 #> 1   0 -0.38548930 #> 2   1 -0.47586788 #> 3   0  0.71975069 #> 4   1 -0.01850562 #> 5   0 -1.37311776 #> 6   0 -0.98242783 #> 7   1 -0.55448870 #> 8   0  0.12138119 #> 9   0 -0.12087232 #> 10  0 -1.33604105  make_full_rank(mat, with.intercept = FALSE) #leaves c1, c2, and c4 #>    c1 c2          c3 #> 1   0  1 -0.38548930 #> 2   1  0 -0.47586788 #> 3   0  1  0.71975069 #> 4   1  0 -0.01850562 #> 5   0  1 -1.37311776 #> 6   0  1 -0.98242783 #> 7   1  0 -0.55448870 #> 8   0  1  0.12138119 #> 9   0  1 -0.12087232 #> 10  0  1 -1.33604105"},{"path":"https://ngreifer.github.io/WeightIt/reference/method_bart.html","id":null,"dir":"Reference","previous_headings":"","what":"Propensity Score Weighting Using BART — method_bart","title":"Propensity Score Weighting Using BART — method_bart","text":"page explains details estimating weights Bayesian additive regression trees (BART)-based propensity scores setting method = \"bart\" call weightit() weightitMSM(). method can used binary, multi-category, continuous treatments. general, method relies estimating propensity scores using BART converting propensity scores weights using formula depends desired estimand. method relies dbartsbart2 dbarts package.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_bart.html","id":"binary-treatments","dir":"Reference","previous_headings":"","what":"Binary Treatments","title":"Propensity Score Weighting Using BART — method_bart","text":"binary treatments, method estimates propensity scores using dbartsbart2. following estimands allowed: ATE, ATT, ATC, ATO, ATM, ATOS. Weights can also computed using marginal mean weighting stratification ATE, ATT, ATC. See get_w_from_ps() details.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_bart.html","id":"multi-category-treatments","dir":"Reference","previous_headings":"","what":"Multi-Category Treatments","title":"Propensity Score Weighting Using BART — method_bart","text":"multi-category treatments, propensity scores estimated using several calls dbartsbart2, one treatment group; treatment probabilities normalized sum 1. following estimands allowed: ATE, ATT, ATC, ATO, ATM. weights estimand computed using standard formulas mentioned . Weights can also computed using marginal mean weighting stratification ATE, ATT, ATC. See get_w_from_ps() details.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_bart.html","id":"continuous-treatments-for-continuous-treatments-weights-are-estimated-as","dir":"Reference","previous_headings":"","what":"Continuous Treatments For continuous treatments, weights are estimated as","title":"Propensity Score Weighting Using BART — method_bart","text":"\\(w_i = f_A(a_i) / f_{|X}(a_i)\\), \\(f_A(a_i)\\) (known stabilization factor) unconditional density treatment evaluated observed treatment value \\(f_{|X}(a_i)\\) (known generalized propensity score) conditional density treatment given covariates evaluated observed value treatment. shape \\(f_A(.)\\) \\(f_{|X}(.)\\) controlled density argument described (normal distributions default), predicted values used mean conditional density estimated using BART implemented dbartsbart2. Kernel density estimation can used instead assuming specific density numerator denominator setting density = \"kernel\". arguments density() can specified refine density estimation parameters.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_bart.html","id":"longitudinal-treatments","dir":"Reference","previous_headings":"","what":"Longitudinal Treatments","title":"Propensity Score Weighting Using BART — method_bart","text":"longitudinal treatments, weights product weights estimated time point.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_bart.html","id":"sampling-weights","dir":"Reference","previous_headings":"","what":"Sampling Weights","title":"Propensity Score Weighting Using BART — method_bart","text":"Sampling weights supported.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_bart.html","id":"missing-data","dir":"Reference","previous_headings":"","what":"Missing Data","title":"Propensity Score Weighting Using BART — method_bart","text":"presence missing data, following value(s) missing allowed: \"ind\" (default) First, variable missingness, new missingness indicator variable created takes value 1 original covariate NA 0 otherwise. missingness indicators added model formula main effects. missing values covariates replaced covariate medians. weight estimation proceeds new formula set covariates. covariates output resulting weightit object original covariates NAs.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_bart.html","id":"m-estimation","dir":"Reference","previous_headings":"","what":"M-estimation","title":"Propensity Score Weighting Using BART — method_bart","text":"M-estimation supported.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_bart.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Propensity Score Weighting Using BART — method_bart","text":"BART works fitting sum--trees model treatment probability treatment. number trees determined n.trees argument. Bayesian priors used hyperparameters, result posterior distribution predicted values unit. mean unit taken use computing (generalized) propensity score. Although hyperparameters governing priors can modified supplying arguments weightit() passed BART fitting function, default values tend work well require little modification (though defaults differ continuous categorical treatments; see dbartsbart2 documentation details). Unlike many machine learning methods, loss function optimized hyperparameters need tuned (e.g., using cross-validation), though performance can benefit tuning. BART tends balance sparseness flexibility using weak learners trees, makes suitable capturing complex functions without specifying particular functional form without overfitting.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_bart.html","id":"reproducibility","dir":"Reference","previous_headings":"","what":"Reproducibility","title":"Propensity Score Weighting Using BART — method_bart","text":"BART random component, work must done ensure reproducibility across runs. See Reproducibility section dbartsbart2 details. ensure reproducibility, one can one two things: 1) supply argument seed, passed dbarts::bart2() sets seed single- multi-threaded uses, 2) call set.seed(), though ensures reproducibility using single-threading, can requested setting n.threads = 1. Note ensure reproducibility machine, regardless number cores available, one use single-threading either supply seed call set.seed().","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_bart.html","id":"additional-arguments","dir":"Reference","previous_headings":"","what":"Additional Arguments","title":"Propensity Score Weighting Using BART — method_bart","text":"arguments dbartsbart2 can passed weightit() weightitMSM(), following exceptions: test, weights,subset, offset.test ignored combine.chains always set TRUE sampleronly always set FALSE continuous treatments , following arguments may supplied: density function corresponding conditional density treatment. standardized residuals treatment model fed function produce numerator denominator generalized propensity score weights. blank, dnorm() used recommended Robins et al. (2000). can also supplied string containing name function called. string contains underscores, call split underscores latter splits supplied arguments second argument beyond. example, density = \"dt_2\" specified, density used t-distribution 2 degrees freedom. Using t-distribution can useful extreme outcome values observed (Naimi et al., 2014). Can also \"kernel\" use kernel density estimation, calls density() estimate numerator denominator densities weights. (used requested setting use.kernel = TRUE, now deprecated.) bw, adjust, kernel, n density = \"kernel\", arguments density(). defaults density() except n 10 times number units sample. plot density = \"kernel\", whether plot estimated densities.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_bart.html","id":"additional-outputs","dir":"Reference","previous_headings":"","what":"Additional Outputs","title":"Propensity Score Weighting Using BART — method_bart","text":"obj include.obj = TRUE, bart2 fit(s) used generate predicted values. multi-category treatments, list fits; otherwise, single fit. predicted probabilities used compute propensity scores can extracted using 2dbartsbartfitted.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_bart.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Propensity Score Weighting Using BART — method_bart","text":"Hill, J., Weiss, C., & Zhai, F. (2011). Challenges Propensity Score Strategies High-Dimensional Setting Potential Alternative. Multivariate Behavioral Research, 46(3), 477–513. doi:10.1080/00273171.2011.570161 Chipman, H. ., George, E. ., & McCulloch, R. E. (2010). BART: Bayesian additive regression trees. Annals Applied Statistics, 4(1), 266–298. doi:10.1214/09-AOAS285 Note many references deal BART causal inference focus estimating potential outcomes BART, propensity scores, directly relevant using BART estimate propensity scores weights. See method_glm additional references propensity score weighting generally.","code":""},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/reference/method_bart.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Propensity Score Weighting Using BART — method_bart","text":"","code":"library(\"cobalt\") data(\"lalonde\", package = \"cobalt\")  #Balancing covariates between treatment groups (binary) (W1 <- weightit(treat ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"bart\", estimand = \"ATT\")) #> A weightit object #>  - method: \"bart\" (propensity score weighting with BART) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 2-category #>  - estimand: ATT (focal: 1) #>  - covariates: age, educ, married, nodegree, re74 summary(W1) #>                   Summary of weights #>  #> - Weight ranges: #>  #>            Min                                  Max #> treated 1.0000    ||                          1.000 #> control 0.0031 |---------------------------| 10.041 #>  #> - Units with the 5 most extreme weights by group: #>                                            #>               5      4     3      2      1 #>  treated      1      1     1      1      1 #>             585    569   592    374    608 #>  control 2.2708 2.8849 3.058 3.5182 10.041 #>  #> - Weight statistics: #>  #>         Coef of Var  MAD Entropy # Zeros #> treated       0.000 0.00    0.00       0 #> control       1.871 0.94    0.75       0 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted  429.       185 #> Weighted     95.49     185 bal.tab(W1) #> Balance Measures #>                Type Diff.Adj #> prop.score Distance   0.4840 #> age         Contin.   0.0620 #> educ        Contin.  -0.0188 #> married      Binary  -0.0320 #> nodegree     Binary   0.0326 #> re74        Contin.  -0.0498 #>  #> Effective sample sizes #>            Control Treated #> Unadjusted  429.       185 #> Adjusted     95.49     185  #Balancing covariates with respect to race (multi-category) (W2 <- weightit(race ~ age + educ + married +                 nodegree + re74, data = lalonde,                 method = \"bart\", estimand = \"ATE\")) #> A weightit object #>  - method: \"bart\" (propensity score weighting with BART) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 3-category (black, hispan, white) #>  - estimand: ATE #>  - covariates: age, educ, married, nodegree, re74 summary(W2) #>                   Summary of weights #>  #> - Weight ranges: #>  #>           Min                                   Max #> black  1.2425 |----------------|             8.7805 #> hispan 2.4077    |------------------------| 12.9012 #> white  1.0530 |----------------|             8.5826 #>  #> - Units with the 5 most extreme weights by group: #>                                                 #>             283     181     244     423     231 #>   black  6.9319  7.4651  7.8077  7.9795  8.7805 #>             512     346     426     570     564 #>  hispan 11.8961 12.0153 12.4313 12.8106 12.9012 #>              68      23      60      76     140 #>   white  4.4834  5.1905  5.7556  8.2101  8.5826 #>  #> - Weight statistics: #>  #>        Coef of Var   MAD Entropy # Zeros #> black        0.571 0.370   0.124       0 #> hispan       0.391 0.325   0.077       0 #> white        0.476 0.326   0.088       0 #>  #> - Effective Sample Sizes: #>  #>            black hispan  white #> Unweighted 243.   72.   299.   #> Weighted   183.5  62.59 243.83 bal.tab(W2) #> Balance summary across all treatment pairs #>             Type Max.Diff.Adj #> age      Contin.       0.1933 #> educ     Contin.       0.1508 #> married   Binary       0.0536 #> nodegree  Binary       0.0307 #> re74     Contin.       0.1084 #>  #> Effective sample sizes #>            black hispan  white #> Unadjusted 243.   72.   299.   #> Adjusted   183.5  62.59 243.83  #Balancing covariates with respect to re75 (continuous) #assuming t(3) conditional density for treatment (W3 <- weightit(re75 ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"bart\", density = \"dt_3\")) #> A weightit object #>  - method: \"bart\" (propensity score weighting with BART) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: continuous #>  - covariates: age, educ, married, nodegree, re74  summary(W3) #>                   Summary of weights #>  #> - Weight ranges: #>  #>        Min                                   Max #> all 0.0783 |---------------------------| 20.7345 #>  #> - Units with the 5 most extreme weights: #>                                           #>         486    310    469     487     484 #>  all 7.1404 7.2411 9.2577 19.2208 20.7345 #>  #> - Weight statistics: #>  #>     Coef of Var   MAD Entropy # Zeros #> all       1.149 0.476   0.279       0 #>  #> - Effective Sample Sizes: #>  #>             Total #> Unweighted 614.   #> Weighted   264.99  bal.tab(W3) #> Balance Measures #>             Type Corr.Adj #> age      Contin.   0.0295 #> educ     Contin.   0.0517 #> married   Binary   0.0798 #> nodegree  Binary  -0.0805 #> re74     Contin.   0.1098 #>  #> Effective sample sizes #>             Total #> Unadjusted 614.   #> Adjusted   264.99"},{"path":"https://ngreifer.github.io/WeightIt/reference/method_cbps.html","id":null,"dir":"Reference","previous_headings":"","what":"Covariate Balancing Propensity Score Weighting — method_cbps","title":"Covariate Balancing Propensity Score Weighting — method_cbps","text":"page explains details estimating weights covariate balancing propensity scores setting method = \"cbps\" call weightit() weightitMSM(). method can used binary, multi-category, continuous treatments. general, method relies estimating propensity scores using generalized method moments converting propensity scores weights using formula depends desired estimand. method relies code written WeightIt using optim().","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_cbps.html","id":"binary-treatments","dir":"Reference","previous_headings":"","what":"Binary Treatments","title":"Covariate Balancing Propensity Score Weighting — method_cbps","text":"binary treatments, method estimates propensity scores weights using optim() using formulas described Imai Ratkovic (2014). following estimands allowed: ATE, ATT, ATC, ATO.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_cbps.html","id":"multi-category-treatments","dir":"Reference","previous_headings":"","what":"Multi-Category Treatments","title":"Covariate Balancing Propensity Score Weighting — method_cbps","text":"multi-category treatments, method estimates generalized propensity scores weights using optim() using formulas described Imai Ratkovic (2014). following estimands allowed: ATE ATT.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_cbps.html","id":"continuous-treatments","dir":"Reference","previous_headings":"","what":"Continuous Treatments","title":"Covariate Balancing Propensity Score Weighting — method_cbps","text":"continuous treatments, method estimates generalized propensity scores weights using optim() using modification formulas described Fong, Hazlett, Imai (2018). See Details.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_cbps.html","id":"longitudinal-treatments","dir":"Reference","previous_headings":"","what":"Longitudinal Treatments","title":"Covariate Balancing Propensity Score Weighting — method_cbps","text":"longitudinal treatments, weights computed using methods similar described Huffman van Gameren (2018). involves specifying moment conditions models time point single-time point treatments using product time-specific weights weights used balance moment conditions. yields weights balance covariate time point. implementation implemented CBPS::CBMSM(), results expected align two methods. combination treatment types supported. -identified version (.e., setting = TRUE), empirical variance used objective function, whereas expected variance averaging treatment used binary multi-category point treatments.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_cbps.html","id":"sampling-weights","dir":"Reference","previous_headings":"","what":"Sampling Weights","title":"Covariate Balancing Propensity Score Weighting — method_cbps","text":"Sampling weights supported s.weights scenarios.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_cbps.html","id":"missing-data","dir":"Reference","previous_headings":"","what":"Missing Data","title":"Covariate Balancing Propensity Score Weighting — method_cbps","text":"presence missing data, following value(s) missing allowed: \"ind\" (default) First, variable missingness, new missingness indicator variable created takes value 1 original covariate NA 0 otherwise. missingness indicators added model formula main effects. missing values covariates replaced covariate medians (value arbitrary affect estimation). weight estimation proceeds new formula set covariates. covariates output resulting weightit object original covariates NAs.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_cbps.html","id":"m-estimation","dir":"Reference","previous_headings":"","what":"M-estimation","title":"Covariate Balancing Propensity Score Weighting — method_cbps","text":"M-estimation supported just-identified CBPS (default, setting = FALSE) binary multi-category treatments. Otherwise (.e., continuous longitudinal treatments = TRUE), M-estimation supported. See glm_weightit() vignette(\"estimating-effects\") details.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_cbps.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Covariate Balancing Propensity Score Weighting — method_cbps","text":"CBPS estimates coefficients generalized linear model (binary treatments), multinomial logistic regression model (multi-category treatments), linear regression model (continuous treatments) used compute (generalized) propensity scores, weights computed. involves replacing (augmenting, case -identified version) standard regression score equations balance constraints generalized method moments estimation. idea nudge estimation coefficients toward produce balance weighted sample. just-identified version (exact = FALSE) away score equations coefficients balance constraints used. just-identified version therefore produce superior balance means (.e., corresponding balance constraints) binary multi-category treatments linear terms continuous treatments -identified version. Just-identified CBPS similar entropy balancing inverse probability tilting. ATT, three methods yield identical estimates. estimands, results differ. Note WeightIt provides different functionality CBPS package terms versions CBPS available; extensions CBPS (e.g., optimal CBPS CBPS instrumental variables), CBPS package may preferred. Note longitudinal treatments, CBPS::CBMSM() uses different methods produces different results weightitMSM() called method = \"cbps\".","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_cbps.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Covariate Balancing Propensity Score Weighting — method_cbps","text":"method used rely functionality CBPS package, longer . Slight differences may found two packages cases due numerical imprecision (, continuous longitudinal treatments, due difference estimator). WeightIt supports arbitrary numbers groups multi-category CBPS estimand, whereas CBPS supports four groups ATE. implementation just-identified CBPS continuous treatments also differs CBPS, departs slightly described Fong et al. (2018). treatment mean treatment variance treated random parameters estimated included balance moment conditions. Fong et al. (2018), treatment mean variance fixed empirical counterparts. continuous treatments -identified CBPS, WeightIt CBPS use different methods specifying GMM variance matrix, may lead differing results. Note default method differs two implementations; default WeightIt uses just-identified CBPS, faster fit, yields better balance, compatible M-estimation estimating standard error treatment effect, whereas CBPS uses -identified CBPS default. However, just-identified -identified versions available packages. rootSolve package installed, optimization process slightly faster accurate starting values provided initial call rootSolvemultiroot. However, package required.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_cbps.html","id":"additional-arguments","dir":"Reference","previous_headings":"","what":"Additional Arguments","title":"Covariate Balancing Propensity Score Weighting — method_cbps","text":"moments int accepted. See weightit() details. following additional arguments can specified: logical; whether request -identified CBPS, combines generalized linear model regression score equations (binary treatments), multinomial logistic regression score equations (multi-category treatments), linear regression score equations (continuous treatments) balance moment conditions. Default FALSE use just-identified CBPS. twostep logical; = TRUE, whether use two-step approximation generalized method moments variance. Default TRUE. Setting FALSE increases computation time may improve estimation. Ignored warning = FALSE. link link used generalized linear model propensity scores treatment binary. Default \"logit\" logistic regression, used original description method Imai Ratkovic (2014), others allowed, including \"probit\", \"cauchit\", \"cloglog\", \"loglog\", \"log\", \"clog\", \"identity\". Note negative weights possible last three used caution. object class \"link-glm\" can also supplied. argument passed quasibinomial(). Ignored multi-category, continuous, longitudinal treatments. reltol relative tolerance convergence optimization. Passed control argument optim(). Default 1e-10. maxit maximum number iterations convergence optimization. Passed control argument optim(). Default 1000 binary multi-category treatments 10000 continuous longitudinal treatments. solver solver use estimate parameters just-identified CBPS. Allowable options include \"multiroot\" use rootSolvemultiroot \"optim\" use stats::optim(). \"multiroot\" default rootSolve installed, tends much faster accurate; otherwise, \"optim\" default requires dependencies. Regardless solver, output optim() returned include.obj = TRUE (see ). = TRUE, parameter estimates just-identified CBPS used starting values -identified CBPS. quantile named list quantiles (values 0 1) continuous covariate, used create additional variables balanced ensure balance corresponding quantile variable. example, setting quantile = list(x1 = c(.25, .5. , .75)) ensures 25th, 50th, 75th percentiles x1 treatment group balanced weighted sample. Can also single number (e.g., .5) unnamed list length 1 (e.g., list(c(.25, .5, .75))) request quantile(s) continuous covariates, named vector (e.g., c(x1 = .5, x2 = .75) request one quantile covariate. allowed binary multi-category treatments.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_cbps.html","id":"additional-outputs","dir":"Reference","previous_headings":"","what":"Additional Outputs","title":"Covariate Balancing Propensity Score Weighting — method_cbps","text":"obj include.obj = TRUE, output final call optim() used produce model parameters. Note variable transformations, resulting parameter estimates may interpretable.","code":""},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/reference/method_cbps.html","id":"binary-treatments-1","dir":"Reference","previous_headings":"","what":"Binary treatments","title":"Covariate Balancing Propensity Score Weighting — method_cbps","text":"Imai, K., & Ratkovic, M. (2014). Covariate balancing propensity score. Journal Royal Statistical Society: Series B (Statistical Methodology), 76(1), 243–263.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_cbps.html","id":"multi-category-treatments-1","dir":"Reference","previous_headings":"","what":"Multi-Category treatments","title":"Covariate Balancing Propensity Score Weighting — method_cbps","text":"Imai, K., & Ratkovic, M. (2014). Covariate balancing propensity score. Journal Royal Statistical Society: Series B (Statistical Methodology), 76(1), 243–263.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_cbps.html","id":"continuous-treatments-1","dir":"Reference","previous_headings":"","what":"Continuous treatments","title":"Covariate Balancing Propensity Score Weighting — method_cbps","text":"Fong, C., Hazlett, C., & Imai, K. (2018). Covariate balancing propensity score continuous treatment: Application efficacy political advertisements. Annals Applied Statistics, 12(1), 156–177. doi:10.1214/17-AOAS1101","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_cbps.html","id":"longitudinal-treatments-1","dir":"Reference","previous_headings":"","what":"Longitudinal treatments","title":"Covariate Balancing Propensity Score Weighting — method_cbps","text":"Huffman, C., & van Gameren, E. (2018). Covariate Balancing Inverse Probability Weights Time-Varying Continuous Interventions. Journal Causal Inference, 6(2). doi:10.1515/jci-2017-0002 Note: one cite Imai & Ratkovic (2015) using CBPS longitudinal treatments. code inspired source code CBPS package.","code":""},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/reference/method_cbps.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Covariate Balancing Propensity Score Weighting — method_cbps","text":"","code":"data(\"lalonde\", package = \"cobalt\")  #Balancing covariates between treatment groups (binary) (W1a <- weightit(treat ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"cbps\", estimand = \"ATT\")) #> A weightit object #>  - method: \"cbps\" (covariate balancing propensity score weighting) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 2-category #>  - estimand: ATT (focal: 1) #>  - covariates: age, educ, married, nodegree, re74 summary(W1a) #>                   Summary of weights #>  #> - Weight ranges: #>  #>            Min                                  Max #> treated 1.0000              ||               1.0000 #> control 0.0172 |---------------------------| 2.2625 #>  #> - Units with the 5 most extreme weights by group: #>                                             #>               5      4      3      2      1 #>  treated      1      1      1      1      1 #>             589    595    269    409    296 #>  control 1.4644 1.4848 1.5763 1.7434 2.2625 #>  #> - Weight statistics: #>  #>         Coef of Var   MAD Entropy # Zeros #> treated       0.000 0.000   0.000       0 #> control       0.839 0.707   0.341       0 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted  429.       185 #> Weighted    252.12     185 cobalt::bal.tab(W1a) #> Balance Measures #>                Type Diff.Adj #> prop.score Distance   0.0164 #> age         Contin.  -0.0000 #> educ        Contin.   0.0000 #> married      Binary  -0.0000 #> nodegree     Binary   0.0000 #> re74        Contin.  -0.0000 #>  #> Effective sample sizes #>            Control Treated #> Unadjusted  429.       185 #> Adjusted    252.12     185  #Balancing covariates between treatment groups (binary) #using over-identified CBPS with probit link (W1b <- weightit(treat ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"cbps\", estimand = \"ATT\",                 over = TRUE, link = \"probit\")) #> A weightit object #>  - method: \"cbps\" (covariate balancing propensity score weighting) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 2-category #>  - estimand: ATT (focal: 1) #>  - covariates: age, educ, married, nodegree, re74 summary(W1b) #>                   Summary of weights #>  #> - Weight ranges: #>  #>            Min                                  Max #> treated 1.0000               ||              1.0000 #> control 0.0125 |---------------------------| 2.0531 #>  #> - Units with the 5 most extreme weights by group: #>                                            #>               5     4      3      2      1 #>  treated      1     1      1      1      1 #>             595   589    269    409    296 #>  control 1.3681 1.378 1.4721 1.6067 2.0531 #>  #> - Weight statistics: #>  #>         Coef of Var   MAD Entropy # Zeros #> treated        0.00 0.000   0.000       0 #> control        0.81 0.693   0.326       0 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted  429.       185 #> Weighted    259.24     185 cobalt::bal.tab(W1b) #> Balance Measures #>                Type Diff.Adj #> prop.score Distance   0.0334 #> age         Contin.  -0.0021 #> educ        Contin.   0.0028 #> married      Binary   0.0011 #> nodegree     Binary   0.0041 #> re74        Contin.  -0.0284 #>  #> Effective sample sizes #>            Control Treated #> Unadjusted  429.       185 #> Adjusted    259.24     185  #Balancing covariates with respect to race (multi-category) (W2 <- weightit(race ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"cbps\", estimand = \"ATE\")) #> A weightit object #>  - method: \"cbps\" (covariate balancing propensity score weighting) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 3-category (black, hispan, white) #>  - estimand: ATE #>  - covariates: age, educ, married, nodegree, re74 summary(W2) #>                   Summary of weights #>  #> - Weight ranges: #>  #>           Min                                   Max #> black  1.5007  |------------------|         17.9659 #> hispan 1.6315  |--------------------------| 24.5609 #> white  1.1311 |--|                           4.1340 #>  #> - Units with the 5 most extreme weights by group: #>                                                #>            226     231     485     181     182 #>   black 6.7985  6.8385  7.2674  9.8974 17.9659 #>            392     564     269     345     371 #>  hispan 16.762 19.8529 22.0193 23.8782 24.5609 #>            398     432     437     404     599 #>   white 3.6882  3.7815  3.8476  3.8952   4.134 #>  #> - Weight statistics: #>  #>        Coef of Var   MAD Entropy # Zeros #> black        0.635 0.387   0.133       0 #> hispan       0.582 0.447   0.155       0 #> white        0.389 0.327   0.071       0 #>  #> - Effective Sample Sizes: #>  #>             black hispan  white #> Unweighted 243.    72.   299.   #> Weighted   173.37  53.95 259.76 cobalt::bal.tab(W2) #> Balance summary across all treatment pairs #>             Type Max.Diff.Adj #> age      Contin.            0 #> educ     Contin.            0 #> married   Binary            0 #> nodegree  Binary            0 #> re74     Contin.            0 #>  #> Effective sample sizes #>             black hispan  white #> Unadjusted 243.    72.   299.   #> Adjusted   173.37  53.95 259.76  #Balancing covariates with respect to re75 (continuous) (W3 <- weightit(re75 ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"cbps\")) #> A weightit object #>  - method: \"cbps\" (covariate balancing propensity score weighting) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: continuous #>  - covariates: age, educ, married, nodegree, re74 summary(W3) #>                   Summary of weights #>  #> - Weight ranges: #>  #>        Min                                   Max #> all 0.0151 |---------------------------| 43.9963 #>  #> - Units with the 5 most extreme weights: #>                                              #>          482     180     481     483     185 #>  all 10.8239 11.0878 11.9703 13.1314 43.9963 #>  #> - Weight statistics: #>  #>     Coef of Var   MAD Entropy # Zeros #> all       1.942 0.528   0.454       0 #>  #> - Effective Sample Sizes: #>  #>             Total #> Unweighted 614.   #> Weighted   128.86 cobalt::bal.tab(W3) #> Balance Measures #>             Type Corr.Adj #> age      Contin.   0.1773 #> educ     Contin.   0.1071 #> married   Binary   0.4522 #> nodegree  Binary   0.3308 #> re74     Contin.   0.5514 #>  #> Effective sample sizes #>             Total #> Unadjusted 614.   #> Adjusted   128.86 # \\donttest{ #Longitudinal treatments data(\"msmdata\") (W4 <- weightitMSM(list(A_1 ~ X1_0 + X2_0,                         A_2 ~ X1_1 + X2_1 +                           A_1 + X1_0 + X2_0),                    data = msmdata,                    method = \"cbps\")) #> A weightitMSM object #>  - method: \"cbps\" (covariate balancing propensity score weighting) #>  - number of obs.: 7500 #>  - sampling weights: none #>  - number of time points: 2 (A_1, A_2) #>  - treatment: #>     + time 1: 2-category #>     + time 2: 2-category #>  - covariates: #>     + baseline: X1_0, X2_0 #>     + after time 1: X1_1, X2_1, A_1, X1_0, X2_0 summary(W4) #>                         Time 1                         #>                   Summary of weights #>  #> - Weight ranges: #>  #>            Min                                    Max #> treated 1.0503 |---------------------------| 110.8804 #> control 1.2386 |---------------------|        86.9515 #>  #> - Units with the 5 most extreme weights by group: #>                                                   #>             3880     168    2859    3774     3653 #>  treated 50.1139 50.1139 56.0786 96.4501 110.8804 #>             5695    6284    3500    1875     1362 #>  control  52.431 55.2118 55.2118 58.5934  86.9515 #>  #> - Weight statistics: #>  #>         Coef of Var   MAD Entropy # Zeros #> treated       1.107 0.552   0.291       0 #> control       1.033 0.600   0.315       0 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted 3306.   4194.   #> Weighted   1598.88 1884.92 #>  #>                         Time 2                         #>                   Summary of weights #>  #> - Weight ranges: #>  #>            Min                                    Max #> treated 1.0503 |-----------------------|      96.4501 #> control 1.2386 |---------------------------| 110.8804 #>  #> - Units with the 5 most extreme weights by group: #>                                                   #>             3729     871    3880     168     3774 #>  treated 42.3872 42.8221 50.1139 50.1139  96.4501 #>             3500    2859    1875    1362     3653 #>  control 55.2118 56.0786 58.5934 86.9515 110.8804 #>  #> - Weight statistics: #>  #>         Coef of Var   MAD Entropy # Zeros #> treated       0.986 0.588   0.292       0 #> control       1.165 0.583   0.329       0 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted 3701.      3799 #> Weighted   1570.46    1926 #>  cobalt::bal.tab(W4) #> Balance summary across all time points #>      Times    Type Max.Diff.Adj #> X1_0  1, 2 Contin.            0 #> X2_0  1, 2  Binary            0 #> X1_1     2 Contin.            0 #> X2_1     2  Binary            0 #> A_1      2  Binary            0 #>  #> Effective sample sizes #>  - Time 1 #>            Control Treated #> Unadjusted 3306.   4194.   #> Adjusted   1598.88 1884.92 #>  - Time 2 #>            Control Treated #> Unadjusted 3701.      3799 #> Adjusted   1570.46    1926 # }"},{"path":"https://ngreifer.github.io/WeightIt/reference/method_ebal.html","id":null,"dir":"Reference","previous_headings":"","what":"Entropy Balancing — method_ebal","title":"Entropy Balancing — method_ebal","text":"page explains details estimating weights using entropy balancing setting method = \"ebal\" call weightit() weightitMSM(). method can used binary, multi-category, continuous treatments. general, method relies estimating weights minimizing negative entropy weights subject exact moment balancing constraints. method relies code written WeightIt using optim().","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_ebal.html","id":"binary-treatments","dir":"Reference","previous_headings":"","what":"Binary Treatments","title":"Entropy Balancing — method_ebal","text":"binary treatments, method estimates weights using optim() using formulas described Hainmueller (2012). following estimands allowed: ATE, ATT, ATC. ATE requested, optimization run twice, treatment group.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_ebal.html","id":"multi-category-treatments","dir":"Reference","previous_headings":"","what":"Multi-Category Treatments","title":"Entropy Balancing — method_ebal","text":"multi-category treatments, method estimates weights using optim(). following estimands allowed: ATE ATT. ATE requested, optim() run treatment group. ATT requested, optim() run non-focal (.e., control) group.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_ebal.html","id":"continuous-treatments","dir":"Reference","previous_headings":"","what":"Continuous Treatments","title":"Entropy Balancing — method_ebal","text":"continuous treatments, method estimates weights using optim() using formulas described Tübbicke (2022) Vegetabile et al. (2021).","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_ebal.html","id":"longitudinal-treatments","dir":"Reference","previous_headings":"","what":"Longitudinal Treatments","title":"Entropy Balancing — method_ebal","text":"longitudinal treatments, weights product weights estimated time point. method guaranteed yield exact balance time point. NOTE: use entropy balancing longitudinal treatments validated!","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_ebal.html","id":"sampling-weights","dir":"Reference","previous_headings":"","what":"Sampling Weights","title":"Entropy Balancing — method_ebal","text":"Sampling weights supported s.weights scenarios.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_ebal.html","id":"missing-data","dir":"Reference","previous_headings":"","what":"Missing Data","title":"Entropy Balancing — method_ebal","text":"presence missing data, following value(s) missing allowed: \"ind\" (default) First, variable missingness, new missingness indicator variable created takes value 1 original covariate NA 0 otherwise. missingness indicators added model formula main effects. missing values covariates replaced covariate medians (value arbitrary affect estimation). weight estimation proceeds new formula set covariates. covariates output resulting weightit object original covariates NAs.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_ebal.html","id":"m-estimation","dir":"Reference","previous_headings":"","what":"M-estimation","title":"Entropy Balancing — method_ebal","text":"M-estimation supported scenarios. See glm_weightit() vignette(\"estimating-effects\") details.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_ebal.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Entropy Balancing — method_ebal","text":"Entropy balancing involves specification optimization problem, solution used compute weights. constraints primal optimization problem correspond covariate balance means (binary multi-category treatments) treatment-covariate covariances (continuous treatments), positivity weights, weights sum certain value. turns dual optimization problem much easier solve many variables balance constraints rather weights unit unconstrained. Zhao Percival (2017) found entropy balancing ATT binary treatment actually involves estimation coefficients logistic regression propensity score model using specialized loss function different optimized maximum likelihood. Entropy balancing doubly robust (ATT) sense consistent either true propensity score model logistic regression treatment covariates true outcome model control units linear regression outcome covariates, attains semi-parametric efficiency bound true. Entropy balancing always yield exact mean balance included terms.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_ebal.html","id":"additional-arguments","dir":"Reference","previous_headings":"","what":"Additional Arguments","title":"Entropy Balancing — method_ebal","text":"moments int accepted. See weightit() details. base.weights vector base weights, one unit. correspond base weights $q$ Hainmueller (2012). estimated weights minimize Kullback entropy divergence base weights, defined \\(\\sum w \\log(w/q)\\), subject exact balance constraints. can used supply previously estimated weights newly estimated weights retain properties original weights ensuring balance constraints met. Sampling weights passed base.weights can included weightit() call includes s.weights. reltol relative tolerance convergence optimization. Passed control argument optim(). Default 1e-10. maxit maximum number iterations convergence optimization. Passed control argument optim(). Default 1000 binary multi-category treatments 10000 continuous longitudinal treatments. solver solver use estimate parameters just-identified CBPS. Allowable options include \"multiroot\" use rootSolvemultiroot \"optim\" use stats::optim(). \"multiroot\" default rootSolve installed, tends much faster accurate; otherwise, \"optim\" default requires dependencies. Regardless solver, output optim() returned include.obj = TRUE (see ). = TRUE, parameter estimates just-identified CBPS used starting values -identified CBPS. quantile named list quantiles (values 0 1) continuous covariate, used create additional variables balanced ensure balance corresponding quantile variable. example, setting quantile = list(x1 = c(.25, .5. , .75)) ensures 25th, 50th, 75th percentiles x1 treatment group balanced weighted sample. Can also single number (e.g., .5) unnamed list length 1 (e.g., list(c(.25, .5, .75))) request quantile(s) continuous covariates, named vector (e.g., c(x1 = .5, x2 = .75) request one quantile covariate. allowed binary multi-category treatments. d.moments continuous treatments, number moments treatment covariate distributions constrained weighted sample original sample. example, setting d.moments = 3 ensures mean, variance, skew treatment covariates weighted sample unweighted sample. d.moments greater equal moments automatically set accordingly (specified). Vegetabile et al. (2021) recommend setting d.moments = 3, even moments less 3. argument corresponds tuning parameters $r$ $s$ Vegetabile et al. (2021) (set equal). Ignored binary multi-category treatments. stabilize argument ignored; past reduce variability weights iterative process. want minimize variance weights subject balance constraints, use method = \"optweight\".","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_ebal.html","id":"additional-outputs","dir":"Reference","previous_headings":"","what":"Additional Outputs","title":"Entropy Balancing — method_ebal","text":"obj include.obj = TRUE, output call optim(), contains dual variables convergence information. ATE fits multi-category treatments, list optim() outputs, one weighted group.","code":""},{"path":[]},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/reference/method_ebal.html","id":"estimand-att-hainmueller-j-entropy-balancing-for-causal","dir":"Reference","previous_headings":"","what":"estimand = \"ATT\" Hainmueller, J. (2012). Entropy Balancing for Causal","title":"Entropy Balancing — method_ebal","text":"Effects: Multivariate Reweighting Method Produce Balanced Samples Observational Studies. Political Analysis, 20(1), 25–46. doi:10.1093/pan/mpr025 Zhao, Q., & Percival, D. (2017). Entropy balancing doubly robust. Journal Causal Inference, 5(1). doi:10.1515/jci-2016-0010","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_ebal.html","id":"estimand-ate-","dir":"Reference","previous_headings":"","what":"estimand = \"ATE\"","title":"Entropy Balancing — method_ebal","text":"Källberg, D., & Waernbaum, . (2023). Large Sample Properties Entropy Balancing Estimators Average Causal Effects. Econometrics Statistics. doi:10.1016/j.ecosta.2023.11.004","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_ebal.html","id":"continuous-treatments-1","dir":"Reference","previous_headings":"","what":"Continuous Treatments","title":"Entropy Balancing — method_ebal","text":"Tübbicke, S. (2022). Entropy Balancing Continuous Treatments. Journal Econometric Methods, 11(1), 71–89. doi:10.1515/jem-2021-0002 Vegetabile, B. G., Griffin, B. ., Coffman, D. L., Cefalu, M., Robbins, M. W., & McCaffrey, D. F. (2021). Nonparametric estimation population average dose-response curves using entropy balancing weights continuous exposures. Health Services Outcomes Research Methodology, 21(1), 69–110. doi:10.1007/s10742-020-00236-2","code":""},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/reference/method_ebal.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Entropy Balancing — method_ebal","text":"","code":"data(\"lalonde\", package = \"cobalt\")  #Balancing covariates between treatment groups (binary) (W1 <- weightit(treat ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"ebal\", estimand = \"ATT\")) #> A weightit object #>  - method: \"ebal\" (entropy balancing) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 2-category #>  - estimand: ATT (focal: 1) #>  - covariates: age, educ, married, nodegree, re74 summary(W1) #>                   Summary of weights #>  #> - Weight ranges: #>  #>            Min                                  Max #> treated 1.0000       ||                      1.0000 #> control 0.0398 |---------------------------| 5.2466 #>  #> - Units with the 5 most extreme weights by group: #>                                             #>               5      4      3      2      1 #>  treated      1      1      1      1      1 #>             589    595    269    409    296 #>  control 3.3958 3.4432 3.6553 4.0428 5.2466 #>  #> - Weight statistics: #>  #>         Coef of Var   MAD Entropy # Zeros #> treated       0.000 0.000   0.000       0 #> control       0.839 0.707   0.341       0 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted  429.       185 #> Weighted    252.12     185 cobalt::bal.tab(W1) #> Balance Measures #>             Type Diff.Adj #> age      Contin.        0 #> educ     Contin.        0 #> married   Binary       -0 #> nodegree  Binary       -0 #> re74     Contin.       -0 #>  #> Effective sample sizes #>            Control Treated #> Unadjusted  429.       185 #> Adjusted    252.12     185  #Balancing covariates with respect to race (multi-category) (W2 <- weightit(race ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"ebal\", estimand = \"ATE\")) #> A weightit object #>  - method: \"ebal\" (entropy balancing) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 3-category (black, hispan, white) #>  - estimand: ATE #>  - covariates: age, educ, married, nodegree, re74 summary(W2) #>                   Summary of weights #>  #> - Weight ranges: #>  #>           Min                                  Max #> black  0.5530   |-------------------------| 5.3496 #> hispan 0.1408 |----------------|            3.3323 #> white  0.3978  |-------|                    1.9232 #>  #> - Units with the 5 most extreme weights by group: #>                                            #>            226    244    485    181    182 #>   black 2.5215 2.5492 2.8059 3.5551 5.3496 #>            392    564    269    345    371 #>  hispan 2.0464 2.5298 2.6323 2.7049 3.3323 #>             68    457    599    589    531 #>   white 1.7109 1.7226 1.7433 1.7741 1.9232 #>  #> - Weight statistics: #>  #>        Coef of Var   MAD Entropy # Zeros #> black        0.590 0.413   0.131       0 #> hispan       0.609 0.440   0.163       0 #> white        0.371 0.306   0.068       0 #>  #> - Effective Sample Sizes: #>  #>             black hispan  white #> Unweighted 243.    72.   299.   #> Weighted   180.47  52.71 262.93 cobalt::bal.tab(W2) #> Balance summary across all treatment pairs #>             Type Max.Diff.Adj #> age      Contin.            0 #> educ     Contin.            0 #> married   Binary            0 #> nodegree  Binary            0 #> re74     Contin.            0 #>  #> Effective sample sizes #>             black hispan  white #> Unadjusted 243.    72.   299.   #> Adjusted   180.47  52.71 262.93  #Balancing covariates and squares with respect to #re75 (continuous), maintaining 3 moments of the #covariate and treatment distributions (W3 <- weightit(re75 ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"ebal\", moments = 2,                 d.moments = 3)) #> A weightit object #>  - method: \"ebal\" (entropy balancing) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: continuous #>  - covariates: age, educ, married, nodegree, re74 summary(W3) #>                   Summary of weights #>  #> - Weight ranges: #>  #>        Min                                   Max #> all 0.0001 |---------------------------| 17.6502 #>  #> - Units with the 5 most extreme weights: #>                                           #>         484    200    166     171     180 #>  all 6.7447 7.6159 8.5233 10.2661 17.6502 #>  #> - Weight statistics: #>  #>     Coef of Var   MAD Entropy # Zeros #> all       1.252 0.614   0.423       0 #>  #> - Effective Sample Sizes: #>  #>             Total #> Unweighted 614.   #> Weighted   239.32 cobalt::bal.tab(W3) #> Balance Measures #>             Type Corr.Adj #> age      Contin.  -0.0000 #> educ     Contin.  -0.0001 #> married   Binary  -0.0001 #> nodegree  Binary   0.0005 #> re74     Contin.  -0.0001 #>  #> Effective sample sizes #>             Total #> Unadjusted 614.   #> Adjusted   239.32"},{"path":"https://ngreifer.github.io/WeightIt/reference/method_energy.html","id":null,"dir":"Reference","previous_headings":"","what":"Energy Balancing — method_energy","title":"Energy Balancing — method_energy","text":"page explains details estimating weights using energy balancing setting method = \"energy\" call weightit() weightitMSM(). method can used binary, multi-category, continuous treatments. general, method relies estimating weights minimizing energy statistic related covariate balance. binary multi-category treatments, energy distance, multivariate distance distributions, treatment groups. continuous treatments, sum distance covariance treatment variable covariates energy distances treatment covariates weighted sample distributions original sample. method relies code written WeightIt using osqposqp osqp package perform optimization. method may slow memory-intensive large datasets.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_energy.html","id":"binary-treatments","dir":"Reference","previous_headings":"","what":"Binary Treatments","title":"Energy Balancing — method_energy","text":"binary treatments, method estimates weights using osqp() using formulas described Huling Mak (2024). following estimands allowed: ATE, ATT, ATC.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_energy.html","id":"multi-category-treatments","dir":"Reference","previous_headings":"","what":"Multi-Category Treatments","title":"Energy Balancing — method_energy","text":"multi-category treatments, method estimates weights using osqp() using formulas described Huling Mak (2024). following estimands allowed: ATE ATT.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_energy.html","id":"continuous-treatments","dir":"Reference","previous_headings":"","what":"Continuous Treatments","title":"Energy Balancing — method_energy","text":"continuous treatments, method estimates weights using osqp() using formulas described Huling, Greifer, Chen (2023).","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_energy.html","id":"longitudinal-treatments","dir":"Reference","previous_headings":"","what":"Longitudinal Treatments","title":"Energy Balancing — method_energy","text":"longitudinal treatments, weights product weights estimated time point. method guaranteed yield optimal balance time point. NOTE: use energy balancing longitudinal treatments validated!","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_energy.html","id":"sampling-weights","dir":"Reference","previous_headings":"","what":"Sampling Weights","title":"Energy Balancing — method_energy","text":"Sampling weights supported s.weights scenarios. cases, sampling weights cause optimization fail due lack convexity infeasible constraints.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_energy.html","id":"missing-data","dir":"Reference","previous_headings":"","what":"Missing Data","title":"Energy Balancing — method_energy","text":"presence missing data, following value(s) missing allowed: \"ind\" (default) First, variable missingness, new missingness indicator variable created takes value 1 original covariate NA 0 otherwise. missingness indicators added model formula main effects. missing values covariates replaced covariate medians (value arbitrary affect estimation). weight estimation proceeds new formula set covariates. covariates output resulting weightit object original covariates NAs.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_energy.html","id":"m-estimation","dir":"Reference","previous_headings":"","what":"M-estimation","title":"Energy Balancing — method_energy","text":"M-estimation supported.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_energy.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Energy Balancing — method_energy","text":"Energy balancing method estimating weights using optimization without propensity score. weights solution constrain quadratic optimization problem objective function concerns covariate balance measured energy distance (continuous treatments) distance covariance. Energy balancing binary multi-category treatments involves minimizing energy distance treatment groups treatment group target group (e.g., full sample ATE). energy distance scalar measure difference two multivariate distributions equal 0 two distributions identical. Energy balancing continuous treatments involves minimizing distance covariance treatment covariates; distance covariance scalar measure association two (possibly multivariate) distributions equal 0 two distributions independent. addition, energy distances treatment covariate distributions weighted sample treatment covariate distributions original sample minimized. primary benefit energy balancing features covariate distribution balanced, just means, optimization-based methods like entropy balancing. Still, possible add additional balance constraints require balance individual terms using moments argument, just like entropy balancing. Energy balancing can sometimes yield weights high variability; lambda argument can supplied penalize highly variable weights increase effective sample size expense balance.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_energy.html","id":"reproducibility","dir":"Reference","previous_headings":"","what":"Reproducibility","title":"Energy Balancing — method_energy","text":"Although stochastic components optimization, feature turned default update optimization based long optimization running, vary across runs even seed set parameters changed. See discussion details. ensure reproducibility default, adaptive_rho_interval set 10. See osqposqpSettings details.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_energy.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Energy Balancing — method_energy","text":"Sometimes optimization can fail converge problem convex. warning displayed . cases, try simply re-fitting weights without changing anything (see Reproducibility section ). method repeatedly fails, try another method change supplied parameters (though uncommon). Increasing max_iter changing adaptive_rho_interval might help. seems like weights balancing covariates still get failure converge, usually indicates iterations needs find optimal solutions. can occur moments int specified. max_iter increased, setting verbose = TRUE allows monitor process examine optimization approaching convergence.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_energy.html","id":"additional-arguments","dir":"Reference","previous_headings":"","what":"Additional Arguments","title":"Energy Balancing — method_energy","text":"following following additional arguments can specified: dist.mat name method used compute distance matrix covariates numeric distance matrix . Allowable options include \"scaled_euclidean\" Euclidean (L2) distance scaled covariates (default), \"mahalanobis\" Mahalanobis distance, \"euclidean\" raw Euclidean distance. Abbreviations allowed. Note user-supplied distance matrices can cause R session abort due bug within osqp, argument used caution. distance matrix must square, symmetric, numeric matrix zeros along diagonal row column unit. Can also supplied output call dist(). lambda positive numeric scalar used penalize square weights. value divided square total sample size added diagonal quadratic part loss function. Higher values favor weights less variability. Note distinct lambda value described Huling Mak (2024), penalizes complexity individual treatment rules rather weights, correspond lambda Huling et al. (2023). Default .0001, essentially 0. binary multi-category treatments, following additional arguments can specified: improved logical; whether use improved energy balancing weights described Huling Mak (2024) estimand = \"ATE\". involves optimizing balance treatment group overall sample, also pair treatment groups. Huling Mak (2024) found improved energy balancing weights generally outperformed standard energy balancing. Default TRUE; set FALSE use standard energy balancing weights instead (recommended). quantile named list quantiles (values 0 1) continuous covariate, used create additional variables balanced ensure balance corresponding quantile variable. example, setting quantile = list(x1 = c(.25, .5. , .75)) ensures 25th, 50th, 75th percentiles x1 treatment group balanced weighted sample. Can also single number (e.g., .5) unnamed list length 1 (e.g., list(c(.25, .5, .75))) request quantile(s) continuous covariates, named vector (e.g., c(x1 = .5, x2 = .75) request one quantile covariate. continuous treatments, following additional arguments can specified: d.moments number moments treatment covariate distributions constrained weighted sample original sample. example, setting d.moments = 3 ensures mean, variance, skew treatment covariates weighted sample unweighted sample. d.moments greater equal moments automatically set accordingly (specified). dimension.adj logical; whether include dimensionality adjustment described Huling et al. (2023). TRUE, default, energy distance covariates weighted \\(\\sqrt{p}\\) times much energy distance treatment, \\(p\\) number covariates. FALSE, two energy distances given equal weights. Default TRUE. moments argument functions differently method = \"energy\" methods. unspecified set zero, energy balancing weights estimated described Huling Mak (2024) binary multi-category treatments Huling et al. (2023) continuous treatments. moments set integer larger 0, additional balance constraints requested moments covariates also included, guaranteeing exact moment balance covariates minimizing energy distance weighted sample. binary multi-category treatments, involves exact balance means entered covariates; continuous treatments, involves exact balance treatment-covariate correlations entered covariates. arguments passed osqposqpSettings. defaults differ osqpSettings(); see Reproducibility .","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_energy.html","id":"additional-outputs","dir":"Reference","previous_headings":"","what":"Additional Outputs","title":"Energy Balancing — method_energy","text":"obj include.obj = TRUE, output call osqpsolve_osqp, contains dual variables convergence information.","code":""},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/reference/method_energy.html","id":"binary-and-multi-category-treatments","dir":"Reference","previous_headings":"","what":"Binary and multi-category treatments","title":"Energy Balancing — method_energy","text":"Huling, J. D., & Mak, S. (2024). Energy balancing covariate distributions. Journal Causal Inference, 12(1). doi:10.1515/jci-2022-0029","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_energy.html","id":"continuous-treatments-1","dir":"Reference","previous_headings":"","what":"Continuous treatments","title":"Energy Balancing — method_energy","text":"Huling, J. D., Greifer, N., & Chen, G. (2023). Independence weights causal inference continuous treatments. Journal American Statistical Association, 0(ja), 1–25. doi:10.1080/01621459.2023.2213485","code":""},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/reference/method_energy.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Energy Balancing — method_energy","text":"Noah Greifer, using code Jared Huling's independenceWeights package continuous treatments.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_energy.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Energy Balancing — method_energy","text":"","code":"library(\"cobalt\") data(\"lalonde\", package = \"cobalt\")  #Balancing covariates between treatment groups (binary) (W1 <- weightit(treat ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"energy\", estimand = \"ATE\")) #> A weightit object #>  - method: \"energy\" (energy balancing) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 2-category #>  - estimand: ATE #>  - covariates: age, educ, married, nodegree, re74 summary(W1) #>                   Summary of weights #>  #> - Weight ranges: #>  #>         Min                                   Max #> treated   0 |---------------|              7.5273 #> control   0 |---------------------------| 12.9264 #>  #> - Units with the 5 most extreme weights by group: #>                                              #>             124    183    172    184     181 #>  treated 3.7973 3.9464 4.7248 5.7377  7.5273 #>             567    374    192    303     608 #>  control 5.0956 5.1599 5.1633 6.5388 12.9264 #>  #> - Weight statistics: #>  #>         Coef of Var   MAD Entropy # Zeros #> treated       1.148 0.852   0.551       6 #> control       1.162 0.764   0.466       4 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted  429.    185.   #> Weighted    182.83   80.09 bal.tab(W1) #> Balance Measures #>             Type Diff.Adj #> age      Contin.  -0.0235 #> educ     Contin.   0.0052 #> married   Binary  -0.0055 #> nodegree  Binary   0.0006 #> re74     Contin.  -0.0061 #>  #> Effective sample sizes #>            Control Treated #> Unadjusted  429.    185.   #> Adjusted    182.83   80.09  #Balancing covariates with respect to race (multi-category) (W2 <- weightit(race ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"energy\", estimand = \"ATT\",                 focal = \"black\")) #> A weightit object #>  - method: \"energy\" (energy balancing) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 3-category (black, hispan, white) #>  - estimand: ATT (focal: black) #>  - covariates: age, educ, married, nodegree, re74 summary(W2) #>                   Summary of weights #>  #> - Weight ranges: #>  #>        Min                                   Max #> black    1   ||                           1.0000 #> hispan   0 |---------|                    4.6858 #> white    0 |---------------------------| 12.6742 #>  #> - Units with the 5 most extreme weights by group: #>                                              #>              6      5      4       3       1 #>   black      1      1      1       1       1 #>             28    269    373     570     424 #>  hispan 3.6372 3.9319 4.1098  4.5448  4.6858 #>             76    422     96      42     140 #>   white 7.6424 7.6594 9.3907 11.1661 12.6742 #>  #> - Weight statistics: #>  #>        Coef of Var   MAD Entropy # Zeros #> black        0.000 0.000   0.000       0 #> hispan       1.165 0.879   0.618       2 #> white        1.733 1.073   0.966      79 #>  #> - Effective Sample Sizes: #>  #>            black hispan  white #> Unweighted   243   72.  299.   #> Weighted     243   30.8  74.87 bal.tab(W2) #> Balance summary across all treatment pairs #>             Type Max.Diff.Adj #> age      Contin.       0.0389 #> educ     Contin.       0.0455 #> married   Binary       0.0094 #> nodegree  Binary       0.0027 #> re74     Contin.       0.0259 #>  #> Effective sample sizes #>            hispan  white black #> Unadjusted   72.  299.     243 #> Adjusted     30.8  74.87   243 # \\donttest{   #Balancing covariates with respect to re75 (continuous)   (W3 <- weightit(re75 ~ age + educ + married +                     nodegree + re74, data = lalonde,                   method = \"energy\", moments = 1)) #> A weightit object #>  - method: \"energy\" (energy balancing) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: continuous #>  - covariates: age, educ, married, nodegree, re74   summary(W3) #>                   Summary of weights #>  #> - Weight ranges: #>  #>     Min                                  Max #> all   0 |---------------------------| 9.7874 #>  #> - Units with the 5 most extreme weights: #>                                         #>         501    486    180    166    171 #>  all 6.7811 6.8314 7.7046 9.3441 9.7874 #>  #> - Weight statistics: #>  #>     Coef of Var   MAD Entropy # Zeros #> all       1.209 0.817   0.564      98 #>  #> - Effective Sample Sizes: #>  #>             Total #> Unweighted 614.   #> Weighted   249.59   bal.tab(W3, poly = 2) #> Balance Measures #>             Type Corr.Adj #> age      Contin.   0.0000 #> educ     Contin.   0.0000 #> married   Binary   0.0000 #> nodegree  Binary   0.0000 #> re74     Contin.   0.0000 #> age²     Contin.  -0.0106 #> educ²    Contin.   0.0117 #> re74²    Contin.  -0.0440 #>  #> Effective sample sizes #>             Total #> Unadjusted 614.   #> Adjusted   249.59 # }"},{"path":"https://ngreifer.github.io/WeightIt/reference/method_gbm.html","id":null,"dir":"Reference","previous_headings":"","what":"Propensity Score Weighting Using Generalized Boosted Models — method_gbm","title":"Propensity Score Weighting Using Generalized Boosted Models — method_gbm","text":"page explains details estimating weights generalized boosted model-based propensity scores setting method = \"gbm\" call weightit() weightitMSM(). method can used binary, multi-category, continuous treatments. general, method relies estimating propensity scores using generalized boosted modeling converting propensity scores weights using formula depends desired estimand. algorithm involves using balance-based prediction-based criterion optimize choosing value tuning parameters (number trees possibly others). method relies gbm package. method mimics functionality functions twang package, improved performance flexible options. See Details section details.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_gbm.html","id":"binary-treatments","dir":"Reference","previous_headings":"","what":"Binary Treatments","title":"Propensity Score Weighting Using Generalized Boosted Models — method_gbm","text":"binary treatments, method estimates propensity scores using gbmgbm.fit selects optimal tuning parameter values using method specified criterion argument. following estimands allowed: ATE, ATT, ATC, ATO, ATM. weights computed estimated propensity scores using get_w_from_ps(), implements standard formulas. Weights can also computed using marginal mean weighting stratification ATE, ATT, ATC. See get_w_from_ps() details.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_gbm.html","id":"multi-category-treatments","dir":"Reference","previous_headings":"","what":"Multi-Category Treatments","title":"Propensity Score Weighting Using Generalized Boosted Models — method_gbm","text":"binary treatments, method estimates propensity scores using gbmgbm.fit selects optimal tuning parameter values using method specified criterion argument. following estimands allowed: ATE, ATT, ATC, ATO, ATM. weights computed estimated propensity scores using get_w_from_ps(), implements standard formulas. Weights can also computed using marginal mean weighting stratification ATE, ATT, ATC. See get_w_from_ps() details.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_gbm.html","id":"continuous-treatments","dir":"Reference","previous_headings":"","what":"Continuous Treatments","title":"Propensity Score Weighting Using Generalized Boosted Models — method_gbm","text":"continuous treatments, method estimates generalized propensity score using gbmgbm.fit selects optimal tuning parameter values using method specified criterion argument.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_gbm.html","id":"longitudinal-treatments","dir":"Reference","previous_headings":"","what":"Longitudinal Treatments","title":"Propensity Score Weighting Using Generalized Boosted Models — method_gbm","text":"longitudinal treatments, weights product weights estimated time point.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_gbm.html","id":"sampling-weights","dir":"Reference","previous_headings":"","what":"Sampling Weights","title":"Propensity Score Weighting Using Generalized Boosted Models — method_gbm","text":"Sampling weights supported s.weights scenarios.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_gbm.html","id":"missing-data","dir":"Reference","previous_headings":"","what":"Missing Data","title":"Propensity Score Weighting Using Generalized Boosted Models — method_gbm","text":"presence missing data, following value(s) missing allowed: \"ind\" (default) First, variable missingness, new missingness indicator variable created takes value 1 original covariate NA 0 otherwise. missingness indicators added model formula main effects. missing values covariates replaced covariate medians (value arbitrary affect estimation). weight estimation proceeds new formula set covariates. covariates output resulting weightit object original covariates NAs. \"surr\" Surrogate splitting used process NAs. missingness indicators created. Nodes split using non-missing values variable. generate predicted values unit, non-missing variable operates similarly variable missingness used surrogate. Missing values ignored calculating balance statistics choose optimal tree.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_gbm.html","id":"m-estimation","dir":"Reference","previous_headings":"","what":"M-estimation","title":"Propensity Score Weighting Using Generalized Boosted Models — method_gbm","text":"M-estimation supported.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_gbm.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Propensity Score Weighting Using Generalized Boosted Models — method_gbm","text":"Generalized boosted modeling (GBM, also known gradient boosting machines) machine learning method generates predicted values flexible regression treatment covariates, treated propensity scores used compute weights. building series regression trees, fit residuals last, minimizing loss function depends distribution chosen. optimal number trees tuning parameter must chosen; McCaffrey et al. (2004) innovative using covariate balance select value rather traditional machine learning performance metrics cross-validation accuracy. GBM particularly effective fitting nonlinear treatment models characterized curves interactions, performs worse simpler treatment models. unclear balance measure used select number trees, though research indicated balance measures tend perform better cross-validation accuracy estimating effective propensity score weights. WeightIt offers almost identical functionality twang, first package implement method. Compared current version twang, WeightIt offers options measure balance used select number trees, improved performance, tuning hyperparameters, estimands, support continuous treatments. WeightIt computes weights multi-category treatments differently twang ; rather fitting separate binary GBM pair treatments, WeightIt fits single multi-class GBM model uses balance measures appropriate multi-category treatments. plot() can used output weightit() method = \"gbm\" display results tuning process; see Examples plot.weightit() details.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_gbm.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Propensity Score Weighting Using Generalized Boosted Models — method_gbm","text":"criterion argument used called stop.method, name twang. stop.method still works backward compatibility. Additionally, criteria formerly named \"es.mean\", \"es.max\", \"es.rms\" renamed \"smd.mean\", \"smd.max\", \"smd.rms\". former used twang still work weightit() backward compatibility. Estimated propensity scores trimmed \\(10^{-8}\\) \\(1 - 10^{-8}\\) ensure balance statistics can computed.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_gbm.html","id":"additional-arguments","dir":"Reference","previous_headings":"","what":"Additional Arguments","title":"Propensity Score Weighting Using Generalized Boosted Models — method_gbm","text":"following additional arguments can specified: criterion string describing balance criterion used select best weights. See cobaltbal.compute allowable options treatment type. addition, optimize cross-validation error instead balance, criterion can set \"cv{#}\", {#} replaced number representing number cross-validation folds used (e.g., \"cv5\" 5-fold cross-validation). binary multi-category treatments, default \"smd.mean\", minimizes average absolute standard mean difference among covariates treatment groups. continuous treatments, default \"p.mean\", minimizes average absolute Pearson correlation treatment covariates. trim.number supplied trim() trims weights trees choosing best tree. can valuable weights extreme, occurs especially continuous treatments. default 0 (.e., trimming). distribution string distribution used loss function boosted model. supplied distribution argument gbmgbm.fit. binary treatments, \"bernoulli\" \"adaboost\" available, \"bernoulli\" default. multi-category treatments, \"multinomial\" allowed. continuous treatments \"gaussian\", \"laplace\", \"tdist\" available, \"gaussian\" default. argument tunable. n.trees maximum number trees used. passed onto n.trees argument gbm.fit(). default 10000 binary multi-category treatments 20000 continuous treatments. start.tree tree start balance checking. know best balance first 100 trees, example, can set start.tree = 101 balance statistics computed first 100 trees. can save time since balance checking takes bulk run time balance-based stopping methods, especially useful running model adding trees. default 1, .e., start first tree assessing balance. interaction.depth depth trees. passed onto interaction.depth argument gbm.fit(). Higher values indicate better ability capture nonlinear nonadditive relationships. default 3 binary multi-category treatments 4 continuous treatments. argument tunable. shrinkage shrinkage parameter applied trees. passed onto shrinkage argument gbm.fit(). default .01 binary multi-category treatments .0005 continuous treatments. lower value , trees one may include reach optimum. argument tunable. bag.fraction fraction units randomly selected propose next tree expansion. passed onto bag.fraction argument gbm.fit(). default 1, smaller values tried. values less 1, subsequent runs parameters yield different results due random sampling; sure seed seed using set.seed() ensure replicability results. use.offset logical; whether use linear predictor resulting generalized linear model offset GBM model. TRUE, fits logistic regression model (binary treatments) linear regression model (continuous treatments) supplies linear predict offset argument gbm.fit(). often improves performance generally especially true propensity score model well approximated GLM, yields uniformly superior performance method = \"glm\" respect criterion. Default FALSE omit offset. allowed binary continuous treatments. argument tunable. arguments take defaults gbmgbm.fit, used . binary multi-category treatments cross-validation used criterion, class.stratify.cv set TRUE default. w argument gbm.fit() ignored sampling weights passed using s.weights. continuous treatments , following arguments may supplied: density function corresponding conditional density treatment. standardized residuals treatment model fed function produce numerator denominator generalized propensity score weights. can also supplied string containing name function called. string contains underscores, call split underscores latter splits supplied arguments second argument beyond. example, density = \"dt_2\" specified, density used t-distribution 2 degrees freedom. Using t-distribution can useful extreme outcome values observed (Naimi et al., 2014). Can also \"kernel\" use kernel density estimation, calls density() estimate numerator denominator densities weights. (used requested setting use.kernel = TRUE, now deprecated.) unspecified, density corresponding argument passed distribution. \"gaussian\" (default), dnorm() used. \"tdist\", t-distribution 4 degrees freedom used. \"laplace\", laplace distribution used. bw, adjust, kernel, n density = \"kernel\", arguments density(). defaults density() except n 10 times number units sample. plot density = \"kernel\", whether plot estimated densities. tunable arguments, multiple entries may supplied, weightit() choose best value optimizing criterion specified criterion. See additional outputs included arguments supplied tuned. See Examples example tuning. seed used every run ensure variation performance across tuning parameters due specification using random seed. matters bag.fraction differs 1 (default) cross-validation used criterion; otherwise, random components model.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_gbm.html","id":"additional-outputs","dir":"Reference","previous_headings":"","what":"Additional Outputs","title":"Propensity Score Weighting Using Generalized Boosted Models — method_gbm","text":"info list following entries: best.tree number trees optimum. close n.trees, weightit() rerun larger value n.trees, start.tree can set just best.tree. parameters tuned, best tree value best combination tuned parameters. See example. tree.val data frame two columns: first number trees second value criterion corresponding tree. Running plot() object plot criterion number trees good way see patterns relationship determine trees needed. parameters tuned, number trees criterion values best combination tuned parameters. See example. arguments tuned (.e., supplied one value), following two additional components included info: tune data frame column argument tuned, best value balance criterion given combination parameters, number trees best value reached. best.tune one-row data frame containing values arguments tuned ultimately selected estimate returned weights. obj include.obj = TRUE, gbm fit used generate predicted values.","code":""},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/reference/method_gbm.html","id":"binary-treatments-1","dir":"Reference","previous_headings":"","what":"Binary treatments","title":"Propensity Score Weighting Using Generalized Boosted Models — method_gbm","text":"McCaffrey, D. F., Ridgeway, G., & Morral, . R. (2004). Propensity Score Estimation Boosted Regression Evaluating Causal Effects Observational Studies. Psychological Methods, 9(4), 403–425. doi:10.1037/1082-989X.9.4.403","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_gbm.html","id":"multi-category-treatments-1","dir":"Reference","previous_headings":"","what":"Multi-Category Treatments","title":"Propensity Score Weighting Using Generalized Boosted Models — method_gbm","text":"McCaffrey, D. F., Griffin, B. ., Almirall, D., Slaughter, M. E., Ramchand, R., & Burgette, L. F. (2013). Tutorial Propensity Score Estimation Multiple Treatments Using Generalized Boosted Models. Statistics Medicine, 32(19), 3388–3414. doi:10.1002/sim.5753","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_gbm.html","id":"continuous-treatments-1","dir":"Reference","previous_headings":"","what":"Continuous treatments","title":"Propensity Score Weighting Using Generalized Boosted Models — method_gbm","text":"Zhu, Y., Coffman, D. L., & Ghosh, D. (2015). Boosting Algorithm Estimating Generalized Propensity Scores Continuous Treatments. Journal Causal Inference, 3(1). doi:10.1515/jci-2014-0022","code":""},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/reference/method_gbm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Propensity Score Weighting Using Generalized Boosted Models — method_gbm","text":"","code":"library(\"cobalt\") data(\"lalonde\", package = \"cobalt\")  #Balancing covariates between treatment groups (binary) (W1 <- weightit(treat ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"gbm\", estimand = \"ATE\",                 criterion = \"smd.max\",                 use.offset = TRUE)) #> A weightit object #>  - method: \"gbm\" (propensity score weighting with GBM) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 2-category #>  - estimand: ATE #>  - covariates: age, educ, married, nodegree, re74 summary(W1) #>                   Summary of weights #>  #> - Weight ranges: #>  #>            Min                                   Max #> treated 1.0771 |---------------------------| 36.9577 #> control 1.0045 |-------|                     12.3008 #>  #> - Units with the 5 most extreme weights by group: #>                                                  #>              183     167     177     184     182 #>  treated 10.4264 10.6142 11.3679 13.1832 36.9577 #>              569     557     592     374     608 #>  control  3.9665  3.9845  5.6957  6.0823 12.3008 #>  #> - Weight statistics: #>  #>         Coef of Var   MAD Entropy # Zeros #> treated       1.271 0.604   0.370       0 #> control       0.555 0.242   0.081       0 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted  429.    185.   #> Weighted    328.23   70.95 bal.tab(W1) #> Balance Measures #>                Type Diff.Adj #> prop.score Distance   0.6568 #> age         Contin.  -0.2045 #> educ        Contin.   0.1478 #> married      Binary  -0.0492 #> nodegree     Binary  -0.0024 #> re74        Contin.   0.1973 #>  #> Effective sample sizes #>            Control Treated #> Unadjusted  429.    185.   #> Adjusted    328.23   70.95  # View information about the fitting process W1$info$best.tree #best tree #>   1  #> 634  plot(W1) #plot of criterion value against number of trees   # \\donttest{   #Balancing covariates with respect to race (multi-category)   (W2 <- weightit(race ~ age + educ + married +                     nodegree + re74, data = lalonde,                   method = \"gbm\", estimand = \"ATT\",                   focal = \"hispan\", criterion = \"ks.mean\")) #> A weightit object #>  - method: \"gbm\" (propensity score weighting with GBM) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 3-category (black, hispan, white) #>  - estimand: ATT (focal: hispan) #>  - covariates: age, educ, married, nodegree, re74   summary(W2) #>                   Summary of weights #>  #> - Weight ranges: #>  #>           Min                                  Max #> black  0.0501  |--------------------------| 2.3757 #> hispan 1.0000             ||                1.0000 #> white  0.0223 |---------------|             1.3588 #>  #> - Units with the 5 most extreme weights by group: #>                                            #>            485    212    463    191    162 #>   black 0.9202 0.9471  0.984 1.0218 2.3757 #>            100     87     44     28      2 #>  hispan      1      1      1      1      1 #>            434    227    385    523    380 #>   white 1.0022 1.0737 1.1325  1.255 1.3588 #>  #> - Weight statistics: #>  #>        Coef of Var   MAD Entropy # Zeros #> black        0.935 0.603   0.300       0 #> hispan       0.000 0.000   0.000       0 #> white        0.837 0.560   0.263       0 #>  #> - Effective Sample Sizes: #>  #>             black hispan white #> Unweighted 243.       72   299 #> Weighted   129.86     72   176   bal.tab(W2, stats = c(\"m\", \"ks\")) #> Balance summary across all treatment pairs #>             Type Max.Diff.Adj Max.KS.Adj #> age      Contin.       0.0565     0.0633 #> educ     Contin.       0.1956     0.0818 #> married   Binary       0.1154     0.1154 #> nodegree  Binary       0.0420     0.0420 #> re74     Contin.       0.0317     0.0791 #>  #> Effective sample sizes #>             black white hispan #> Unadjusted 243.     299     72 #> Adjusted   129.86   176     72    #Balancing covariates with respect to re75 (continuous)   (W3 <- weightit(re75 ~ age + educ + married +                     nodegree + re74, data = lalonde,                   method = \"gbm\", density = \"kernel\",                   criterion = \"p.rms\", trim.at = .97)) #> A weightit object #>  - method: \"gbm\" (propensity score weighting with GBM) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: continuous #>  - covariates: age, educ, married, nodegree, re74   summary(W3) #>                   Summary of weights #>  #> - Weight ranges: #>  #>        Min                                   Max #> all 0.0699 |---------------------------| 11.8181 #>  #> - Units with the 5 most extreme weights: #>                                              #>          407     395     375     354     308 #>  all 11.8181 11.8181 11.8181 11.8181 11.8181 #>  #> - Weight statistics: #>  #>     Coef of Var   MAD Entropy # Zeros #> all       1.244 0.839   0.524       0 #>  #> - Effective Sample Sizes: #>  #>             Total #> Unweighted 614.   #> Weighted   241.25   bal.tab(W3) #> Balance Measures #>             Type Corr.Adj #> age      Contin.   0.0031 #> educ     Contin.   0.0158 #> married   Binary   0.0410 #> nodegree  Binary  -0.0242 #> re74     Contin.   0.0927 #>  #> Effective sample sizes #>             Total #> Unadjusted 614.   #> Adjusted   241.25    #Using a t(3) density and illustrating the search for   #more trees.   W4a <- weightit(re75 ~ age + educ + married +                     nodegree + re74, data = lalonde,                   method = \"gbm\", density = \"dt_3\",                   criterion = \"p.max\",                   n.trees = 10000)    W4a$info$best.tree #10000; optimum hasn't been found #>     1  #> 10000    plot(W4a) #decreasing at right edge     W4b <- weightit(re75 ~ age + educ + married +                     nodegree + re74, data = lalonde,                   method = \"gbm\", density = \"dt_3\",                   criterion = \"p.max\",                   start.tree = 10000,                   n.trees = 20000)    W4b$info$best.tree #13417; optimum has been found #>     1  #> 13417    plot(W4b) #increasing at right edge     bal.tab(W4b) #> Balance Measures #>             Type Corr.Adj #> age      Contin.   0.0362 #> educ     Contin.   0.0502 #> married   Binary   0.0717 #> nodegree  Binary  -0.0665 #> re74     Contin.   0.1041 #>  #> Effective sample sizes #>             Total #> Unadjusted 614.   #> Adjusted   251.08    #Tuning hyperparameters   (W5 <- weightit(treat ~ age + educ + married +                     nodegree + re74, data = lalonde,                   method = \"gbm\", estimand = \"ATT\",                   criterion = \"ks.max\",                   interaction.depth = 2:4,                   distribution = c(\"bernoulli\", \"adaboost\"))) #> A weightit object #>  - method: \"gbm\" (propensity score weighting with GBM) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 2-category #>  - estimand: ATT (focal: 1) #>  - covariates: age, educ, married, nodegree, re74    W5$info$tune #>   interaction.depth shrinkage distribution use.offset best.ks.max best.tree #> 1                 2      0.01    bernoulli      FALSE  0.05635819      1707 #> 2                 3      0.01    bernoulli      FALSE  0.05494295      1758 #> 3                 4      0.01    bernoulli      FALSE  0.05729439       838 #> 4                 2      0.01     adaboost      FALSE  0.05683093      1640 #> 5                 3      0.01     adaboost      FALSE  0.05920056      1178 #> 6                 4      0.01     adaboost      FALSE  0.06517761       774    W5$info$best.tune #Best values of tuned parameters #>   interaction.depth shrinkage distribution use.offset best.ks.max best.tree #> 2                 3      0.01    bernoulli      FALSE  0.05494295      1758   plot(W5) #plot criterion values against number of trees     bal.tab(W5, stats = c(\"m\", \"ks\")) #> Balance Measures #>                Type Diff.Adj KS.Adj #> prop.score Distance   0.4921 0.2346 #> age         Contin.   0.0447 0.0549 #> educ        Contin.  -0.0018 0.0509 #> married      Binary  -0.0135 0.0135 #> nodegree     Binary   0.0392 0.0392 #> re74        Contin.   0.0307 0.0197 #>  #> Effective sample sizes #>            Control Treated #> Unadjusted   429.      185 #> Adjusted      33.2     185 # }"},{"path":"https://ngreifer.github.io/WeightIt/reference/method_glm.html","id":null,"dir":"Reference","previous_headings":"","what":"Propensity Score Weighting Using Generalized Linear Models — method_glm","title":"Propensity Score Weighting Using Generalized Linear Models — method_glm","text":"page explains details estimating weights generalized linear model-based propensity scores setting method = \"glm\" call weightit() weightitMSM(). method can used binary, multi-category, continuous treatments. general, method relies estimating propensity scores parametric generalized linear model converting propensity scores weights using formula depends desired estimand. binary multi-category treatments, binomial multinomial regression model used estimate propensity scores predicted probability treatment given covariates. ordinal treatments, ordinal regression model used estimate generalized propensity scores. continuous treatments, generalized linear model used estimate generalized propensity scores conditional density treatment given covariates.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_glm.html","id":"binary-treatments","dir":"Reference","previous_headings":"","what":"Binary Treatments","title":"Propensity Score Weighting Using Generalized Linear Models — method_glm","text":"binary treatments, method estimates propensity scores using glm(). additional argument link, uses options link family(). default link \"logit\", others, including \"probit\", allowed. following estimands allowed: ATE, ATT, ATC, ATO, ATM, ATOS. Weights can also computed using marginal mean weighting stratification ATE, ATT, ATC. See get_w_from_ps() details.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_glm.html","id":"multi-category-treatments","dir":"Reference","previous_headings":"","what":"Multi-Category Treatments","title":"Propensity Score Weighting Using Generalized Linear Models — method_glm","text":"multi-category treatments, propensity scores estimated using multinomial regression one functions depending argument supplied multi.method (see Additional Arguments ). following estimands allowed: ATE, ATT, ATC, ATO, ATM. weights estimand computed using standard formulas mentioned . Weights can also computed using marginal mean weighting stratification ATE, ATT, ATC. See get_w_from_ps() details. Ordinal treatments treated exactly non-order multi-category treatments except additional models available estimate generalized propensity score (e.g., ordinal logistic regression).","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_glm.html","id":"continuous-treatments","dir":"Reference","previous_headings":"","what":"Continuous Treatments","title":"Propensity Score Weighting Using Generalized Linear Models — method_glm","text":"continuous treatments, weights estimated \\(w_i = f_A(a_i) / f_{|X}(a_i)\\), \\(f_A(a_i)\\) (known stabilization factor) unconditional density treatment evaluated observed treatment value \\(f_{|X}(a_i)\\) (known generalized propensity score) conditional density treatment given covariates evaluated observed value treatment. shape \\(f_A(.)\\) \\(f_{|X}(.)\\) controlled density argument described (normal distributions default), predicted values used mean conditional density estimated using linear regression. Kernel density estimation can used instead assuming specific density numerator denominator setting density = \"kernel\". arguments density() can specified refine density estimation parameters.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_glm.html","id":"longitudinal-treatments","dir":"Reference","previous_headings":"","what":"Longitudinal Treatments","title":"Propensity Score Weighting Using Generalized Linear Models — method_glm","text":"longitudinal treatments, weights product weights estimated time point.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_glm.html","id":"sampling-weights","dir":"Reference","previous_headings":"","what":"Sampling Weights","title":"Propensity Score Weighting Using Generalized Linear Models — method_glm","text":"Sampling weights supported s.weights scenarios except multi-category treatments link = \"bayes.probit\" binary continuous treatments missing = \"saem\" (see ). Warning messages may appear otherwise non-integer successes, can ignored.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_glm.html","id":"missing-data","dir":"Reference","previous_headings":"","what":"Missing Data","title":"Propensity Score Weighting Using Generalized Linear Models — method_glm","text":"presence missing data, following value(s) missing allowed: \"ind\" (default) First, variable missingness, new missingness indicator variable created takes value 1 original covariate NA 0 otherwise. missingness indicators added model formula main effects. missing values covariates replaced covariate medians (value arbitrary affect estimation). weight estimation proceeds new formula set covariates. covariates output resulting weightit object original covariates NAs. \"saem\" binary treatments link = \"logit\" continuous treatments, stochastic approximation version EM algorithm (SAEM) used via misaem package. additional covariates created. See Jiang et al. (2019) information method. cases, suitable alternative multiple imputation.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_glm.html","id":"m-estimation","dir":"Reference","previous_headings":"","what":"M-estimation","title":"Propensity Score Weighting Using Generalized Linear Models — method_glm","text":"binary treatments, M-estimation supported link neither \"flic\" \"flac\" (see ). multi-category treatments, M-estimation supported multi.method \"weightit\" (default) \"glm\". continuous treatments, M-estimation supported density \"kernel\". conditional treatment variance unconditional treatment mean variance included parameters estimate, go calculation weights. treatment types, M-estimation supported missing = \"saem\". See glm_weightit() vignette(\"estimating-effects\") details. longitudinal treatments, M-estimation supported whenever underlying methods .","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_glm.html","id":"additional-arguments","dir":"Reference","previous_headings":"","what":"Additional Arguments","title":"Propensity Score Weighting Using Generalized Linear Models — method_glm","text":"binary treatments, following additional argument can specified: link link used generalized linear model propensity scores. link can allowed binomial() well \"loglog\" \"clog\". br. prefix can added (e.g., \"br.logit\"); changes fitting method bias-corrected generalized linear models implemented brglm2 package. link can also either \"flic\" \"flac\" fit corresponding Firth corrected logistic regression models implemented logistf package. multi-category treatments, following additional arguments can specified: multi.method method used estimate generalized propensity scores. Allowable options include \"weightit\" (default) use multinomial logistic regression implemented WeightIt, \"glm\" use series binomial models using glm(), \"mclogit\" use multinomial logistic regression implemented mclogitmblogit, \"mnp\" use Bayesian multinomial probit regression implemented MNPMNP, \"brmultinom\" use bias-reduced multinomial logistic regression implemented brglm2brmultinom. \"weightit\" \"mclogit\" give near-identical results, main difference increased robustness customizability using \"mclogit\" expense able use M-estimation compute standard errors weighting. ordered treatments, allowable options include \"weightit\" (default) use ordinal regression implemented WeightIt \"polr\" use ordinal regression implemented MASSpolr, unless link \"br.logit\", case bias-reduce ordinal logistic regression implemented brglm2bracl used. Ignored missing = \"saem\". Using defaults allows use M-estimation requires additional dependencies, packages may provide benefits speed flexibility. link link used multinomial, binomial, ordered regression model generalized propensity scores depending argument supplied multi.method. multi.method = \"glm\", link can allowed binomial(). treatment ordered multi.method \"weightit\" \"polr\", link can allowed MASS::polr() \"br.logit\". Otherwise, link \"logit\" specified. continuous treatments, following additional arguments may supplied: density function corresponding conditional density treatment. standardized residuals treatment model fed function produce numerator denominator generalized propensity score weights. blank, dnorm() used recommended Robins et al. (2000). can also supplied string containing name function called. string contains underscores, call split underscores latter splits supplied arguments second argument beyond. example, density = \"dt_2\" specified, density used t-distribution 2 degrees freedom. Using t-distribution can useful extreme outcome values observed (Naimi et al., 2014). Can also \"kernel\" use kernel density estimation, calls density() estimate numerator denominator densities weights. (used requested setting use.kernel = TRUE, now deprecated.) bw, adjust, kernel, n density = \"kernel\", arguments density(). defaults density() except n 10 times number units sample. plot density = \"kernel\", whether plot estimated densities. link link used fit linear model generalized propensity score. Can allowed gaussian(). Additional arguments glm() can specified well used fitting. method argument glm() renamed glm.method. can used supply alternative fitting functions, implemented glm2 package. arguments weightit() passed ... glm(). presence missing data link = \"logit\" missing = \"saem\", additional arguments passed misaemmiss.glm misaempredict.miss.glm, except method argument misaempredict.miss.glm replaced saem.method. continuous treatments presence missing data missing = \"saem\", additional arguments passed misaemmiss.lm misaempredict.miss.lm.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_glm.html","id":"additional-outputs","dir":"Reference","previous_headings":"","what":"Additional Outputs","title":"Propensity Score Weighting Using Generalized Linear Models — method_glm","text":"obj include.obj = TRUE, (generalized) propensity score model fit. binary treatments, output call glm() requested fitting function. multi-category treatments, output call fitting function (list thereof multi.method = \"glm\"). continuous treatments, output call glm() predicted values denominator density.","code":""},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/reference/method_glm.html","id":"binary-treatments-1","dir":"Reference","previous_headings":"","what":"Binary treatments","title":"Propensity Score Weighting Using Generalized Linear Models — method_glm","text":"estimand = \"ATO\" Li, F., Morgan, K. L., & Zaslavsky, . M. (2018). Balancing covariates via propensity score weighting. Journal American Statistical Association, 113(521), 390–400. doi:10.1080/01621459.2016.1260466 estimand = \"ATM\" Li, L., & Greene, T. (2013). Weighting Analogue Pair Matching Propensity Score Analysis. International Journal Biostatistics, 9(2). doi:10.1515/ijb-2012-0030 estimand = \"ATOS\" Crump, R. K., Hotz, V. J., Imbens, G. W., & Mitnik, O. . (2009). Dealing limited overlap estimation average treatment effects. Biometrika, 96(1), 187–199. doi:10.1093/biomet/asn055 estimands Austin, P. C. (2011). Introduction Propensity Score Methods Reducing Effects Confounding Observational Studies. Multivariate Behavioral Research, 46(3), 399–424. doi:10.1080/00273171.2011.568786 Marginal mean weighting stratification Hong, G. (2010). Marginal mean weighting stratification: Adjustment selection bias multilevel data. Journal Educational Behavioral Statistics, 35(5), 499–531. doi:10.3102/1076998609359785 Bias-reduced logistic regression See references brglm2 package. Firth corrected logistic regression Puhr, R., Heinze, G., Nold, M., Lusa, L., & Geroldinger, . (2017). Firth’s logistic regression rare events: Accurate effect estimates predictions? Statistics Medicine, 36(14), 2302–2317. doi:10.1002/sim.7273 SAEM logistic regression missing data Jiang, W., Josse, J., & Lavielle, M. (2019). Logistic regression missing covariates — Parameter estimation, model selection prediction within joint-modeling framework. Computational Statistics & Data Analysis, 106907. doi:10.1016/j.csda.2019.106907","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_glm.html","id":"multi-category-treatments-1","dir":"Reference","previous_headings":"","what":"Multi-Category Treatments","title":"Propensity Score Weighting Using Generalized Linear Models — method_glm","text":"estimand = \"ATO\" Li, F., & Li, F. (2019). Propensity score weighting causal inference multiple treatments. Annals Applied Statistics, 13(4), 2389–2415. doi:10.1214/19-AOAS1282 estimand = \"ATM\" Yoshida, K., Hernández-Díaz, S., Solomon, D. H., Jackson, J. W., Gagne, J. J., Glynn, R. J., & Franklin, J. M. (2017). Matching weights simultaneously compare three treatment groups: Comparison three-way matching. Epidemiology (Cambridge, Mass.), 28(3), 387–395. doi:10.1097/EDE.0000000000000627 estimands McCaffrey, D. F., Griffin, B. ., Almirall, D., Slaughter, M. E., Ramchand, R., & Burgette, L. F. (2013). Tutorial Propensity Score Estimation Multiple Treatments Using Generalized Boosted Models. Statistics Medicine, 32(19), 3388–3414. doi:10.1002/sim.5753 Marginal mean weighting stratification Hong, G. (2012). Marginal mean weighting stratification: generalized method evaluating multivalued multiple treatments nonexperimental data. Psychological Methods, 17(1), 44–60. doi:10.1037/a0024918","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_glm.html","id":"continuous-treatments-1","dir":"Reference","previous_headings":"","what":"Continuous treatments","title":"Propensity Score Weighting Using Generalized Linear Models — method_glm","text":"Robins, J. M., Hernán, M. Á., & Brumback, B. (2000). Marginal Structural Models Causal Inference Epidemiology. Epidemiology, 11(5), 550–560. Using non-normal conditional densities Naimi, . ., Moodie, E. E. M., Auger, N., & Kaufman, J. S. (2014). Constructing Inverse Probability Weights Continuous Exposures: Comparison Methods. Epidemiology, 25(2), 292–299. doi:10.1097/EDE.0000000000000053 SAEM linear regression missing data Jiang, W., Josse, J., & Lavielle, M. (2019). Logistic regression missing covariates — Parameter estimation, model selection prediction within joint-modeling framework. Computational Statistics & Data Analysis, 106907. doi:10.1016/j.csda.2019.106907","code":""},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/reference/method_glm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Propensity Score Weighting Using Generalized Linear Models — method_glm","text":"","code":"library(\"cobalt\") data(\"lalonde\", package = \"cobalt\")  #Balancing covariates between treatment groups (binary) (W1 <- weightit(treat ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"glm\", estimand = \"ATT\",                 link = \"probit\")) #> A weightit object #>  - method: \"glm\" (propensity score weighting with GLM) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 2-category #>  - estimand: ATT (focal: 1) #>  - covariates: age, educ, married, nodegree, re74 summary(W1) #>                   Summary of weights #>  #> - Weight ranges: #>  #>            Min                                  Max #> treated 1.0000                 ||            1.0000 #> control 0.0176 |---------------------------| 1.8338 #>  #> - Units with the 5 most extreme weights by group: #>                                          #>               5     4     3     2      1 #>  treated      1     1     1     1      1 #>             612   595   269   409    296 #>  control 1.2781 1.351 1.412 1.518 1.8338 #>  #> - Weight statistics: #>  #>         Coef of Var   MAD Entropy # Zeros #> treated       0.000 0.000   0.000       0 #> control       0.804 0.691   0.322       0 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted  429.       185 #> Weighted    260.83     185 bal.tab(W1) #> Balance Measures #>                Type Diff.Adj #> prop.score Distance   0.0252 #> age         Contin.   0.0716 #> educ        Contin.  -0.0565 #> married      Binary   0.0058 #> nodegree     Binary   0.0128 #> re74        Contin.  -0.0507 #>  #> Effective sample sizes #>            Control Treated #> Unadjusted  429.       185 #> Adjusted    260.83     185  #Balancing covariates with respect to race (multi-category) (W2 <- weightit(race ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"glm\", estimand = \"ATE\")) #> A weightit object #>  - method: \"glm\" (propensity score weighting with GLM) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 3-category (black, hispan, white) #>  - estimand: ATE #>  - covariates: age, educ, married, nodegree, re74 summary(W2) #>                   Summary of weights #>  #> - Weight ranges: #>  #>           Min                                   Max #> black  1.4528 |---------------------------| 30.8070 #> hispan 1.7990  |---------------------|      25.9532 #> white  1.1123 |-|                            3.9770 #>  #> - Units with the 5 most extreme weights by group: #>                                                #>             226     231    485     181     182 #>   black   7.532  7.6243 8.5246 12.3509  30.807 #>             392     564    345     269     371 #>  hispan 15.9922 16.7517 18.784 23.4106 25.9532 #>             432     589    437     404     599 #>   white   3.686  3.6864  3.712  3.7842   3.977 #>  #> - Weight statistics: #>  #>        Coef of Var   MAD Entropy # Zeros #> black        0.890 0.426   0.192       0 #> hispan       0.541 0.404   0.132       0 #> white        0.382 0.317   0.068       0 #>  #> - Effective Sample Sizes: #>  #>            black hispan  white #> Unweighted 243.   72.   299.   #> Weighted   135.8  55.86 261.02 bal.tab(W2) #> Balance summary across all treatment pairs #>             Type Max.Diff.Adj #> age      Contin.       0.0419 #> educ     Contin.       0.1276 #> married   Binary       0.0500 #> nodegree  Binary       0.0605 #> re74     Contin.       0.2023 #>  #> Effective sample sizes #>            black hispan  white #> Unadjusted 243.   72.   299.   #> Adjusted   135.8  55.86 261.02  #Balancing covariates with respect to re75 (continuous) #with kernel density estimate (W3 <- weightit(re75 ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"glm\", density = \"kernel\")) #> A weightit object #>  - method: \"glm\" (propensity score weighting with GLM) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: continuous #>  - covariates: age, educ, married, nodegree, re74 summary(W3) #>                   Summary of weights #>  #> - Weight ranges: #>  #>        Min                                    Max #> all 0.1076 |---------------------------| 101.6599 #>  #> - Units with the 5 most extreme weights: #>                                              #>         482     481     484     483      485 #>  all 69.644 69.7094 85.9204 94.3369 101.6599 #>  #> - Weight statistics: #>  #>     Coef of Var   MAD Entropy # Zeros #> all       2.908 1.049   1.156       0 #>  #> - Effective Sample Sizes: #>  #>             Total #> Unweighted 614.   #> Weighted    65.03 bal.tab(W3) #> Balance Measures #>             Type Corr.Adj #> age      Contin.  -0.0501 #> educ     Contin.   0.0016 #> married   Binary  -0.0427 #> nodegree  Binary  -0.0196 #> re74     Contin.  -0.0773 #>  #> Effective sample sizes #>             Total #> Unadjusted 614.   #> Adjusted    65.03"},{"path":"https://ngreifer.github.io/WeightIt/reference/method_ipt.html","id":null,"dir":"Reference","previous_headings":"","what":"Inverse Probability Tilting — method_ipt","title":"Inverse Probability Tilting — method_ipt","text":"page explains details estimating weights using inverse probability tilting setting method = \"ipt\" call weightit() weightitMSM(). method can used binary multi-category treatments. general, method relies estimating propensity scores using modification usual generalized linear model score equations enforce balance converting propensity scores weights using formula depends desired estimand. method relies code written WeightIt using rootSolvemultiroot.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_ipt.html","id":"binary-treatments","dir":"Reference","previous_headings":"","what":"Binary Treatments","title":"Inverse Probability Tilting — method_ipt","text":"binary treatments, method estimates weights using formulas described Graham, Pinto, Egel (2012). following estimands allowed: ATE, ATT, ATC. ATE requested, optimization run twice, treatment group.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_ipt.html","id":"multi-category-treatments","dir":"Reference","previous_headings":"","what":"Multi-Category Treatments","title":"Inverse Probability Tilting — method_ipt","text":"multi-category treatments, method estimates weights using modifications formulas described Graham, Pinto, Egel (2012). following estimands allowed: ATE ATT. ATE requested, estimation performed treatment group. ATT requested, estimation performed non-focal (.e., control) group.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_ipt.html","id":"continuous-treatments","dir":"Reference","previous_headings":"","what":"Continuous Treatments","title":"Inverse Probability Tilting — method_ipt","text":"Inverse probability tilting compatible continuous treatments.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_ipt.html","id":"longitudinal-treatments","dir":"Reference","previous_headings":"","what":"Longitudinal Treatments","title":"Inverse Probability Tilting — method_ipt","text":"longitudinal treatments, weights product weights estimated time point. method guaranteed yield exact balance time point. NOTE: use inverse probability tilting longitudinal treatments validated!","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_ipt.html","id":"sampling-weights","dir":"Reference","previous_headings":"","what":"Sampling Weights","title":"Inverse Probability Tilting — method_ipt","text":"Sampling weights supported s.weights scenarios.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_ipt.html","id":"missing-data","dir":"Reference","previous_headings":"","what":"Missing Data","title":"Inverse Probability Tilting — method_ipt","text":"presence missing data, following value(s) missing allowed: \"ind\" (default) First, variable missingness, new missingness indicator variable created takes value 1 original covariate NA 0 otherwise. missingness indicators added model formula main effects. missing values covariates replaced covariate medians (value arbitrary affect estimation). weight estimation proceeds new formula set covariates. covariates output resulting weightit object original covariates NAs.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_ipt.html","id":"m-estimation","dir":"Reference","previous_headings":"","what":"M-estimation","title":"Inverse Probability Tilting — method_ipt","text":"M-estimation supported scenarios. See glm_weightit() vignette(\"estimating-effects\") details.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_ipt.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Inverse Probability Tilting — method_ipt","text":"Inverse probability tilting (IPT) involves specifying estimating equations fit parameters two generalized linear models modification ensures exact balance covariate means. estimating equations solved, estimated parameters used (generalized) propensity score, used compute weights. Conceptually mathematically, IPT similar entropy balancing just-identified CBPS. ATT ATC, entropy balancing, just-identified CBPS, IPT yield identical results. ATE link specified something \"logit\", three methods differ. Treatment effect estimates binary treatments consistent true propensity score logistic regression outcome model linear covariates interaction treatments. entropy balancing, true ATT, just-identified CBPS, true effect modification covariates. way, IPT provides additional theoretical guarantees two methods, though potentially cost precision.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_ipt.html","id":"additional-arguments","dir":"Reference","previous_headings":"","what":"Additional Arguments","title":"Inverse Probability Tilting — method_ipt","text":"moments int accepted. See weightit() details. quantile named list quantiles (values 0 1) continuous covariate, used create additional variables balanced ensure balance corresponding quantile variable. example, setting quantile = list(x1 = c(.25, .5. , .75)) ensures 25th, 50th, 75th percentiles x1 treatment group balanced weighted sample. Can also single number (e.g., .5) unnamed list length 1 (e.g., list(c(.25, .5, .75))) request quantile(s) continuous covariates, named vector (e.g., c(x1 = .5, x2 = .75) request one quantile covariate. link link used determine inverse link computing (generalized) propensity scores. Default \"logit\", used original description method Graham, Pinto, Egel (2012), \"probit\", \"cauchit\", \"cloglog\", \"loglog\", \"log\", \"clog\" also allowed. Note negative weights possible last two used caution. object class \"link-glm\" can also supplied. argument passed quasibinomial(). stabilize argument ignored.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_ipt.html","id":"additional-outputs","dir":"Reference","previous_headings":"","what":"Additional Outputs","title":"Inverse Probability Tilting — method_ipt","text":"obj include.obj = TRUE, output call optim(), contains coefficient estimates convergence information. ATE fits multi-category treatments, list rootSolve::multiroot() outputs, one weighted group.","code":""},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/reference/method_ipt.html","id":"estimand-ate-","dir":"Reference","previous_headings":"","what":"estimand = \"ATE\"","title":"Inverse Probability Tilting — method_ipt","text":"Graham, B. S., De Xavier Pinto, C. C., & Egel, D. (2012). Inverse Probability Tilting Moment Condition Models Missing Data. Review Economic Studies, 79(3), 1053–1079. doi:10.1093/restud/rdr047","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_ipt.html","id":"estimand-att-","dir":"Reference","previous_headings":"","what":"estimand = \"ATT\"","title":"Inverse Probability Tilting — method_ipt","text":"Sant'Anna, P. H. C., & Zhao, J. (2020). Doubly robust difference--differences estimators. Journal Econometrics, 219(1), 101–122. doi:10.1016/j.jeconom.2020.06.003","code":""},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/reference/method_ipt.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Inverse Probability Tilting — method_ipt","text":"","code":"data(\"lalonde\", package = \"cobalt\")  #Balancing covariates between treatment groups (binary) (W1 <- weightit(treat ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"ipt\", estimand = \"ATT\")) #> A weightit object #>  - method: \"ipt\" (inverse probability tilting) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 2-category #>  - estimand: ATT (focal: 1) #>  - covariates: age, educ, married, nodegree, re74 summary(W1) #>                   Summary of weights #>  #> - Weight ranges: #>  #>            Min                                  Max #> treated 1.0000              ||               1.0000 #> control 0.0172 |---------------------------| 2.2625 #>  #> - Units with the 5 most extreme weights by group: #>                                             #>               5      4      3      2      1 #>  treated      1      1      1      1      1 #>             589    595    269    409    296 #>  control 1.4644 1.4848 1.5763 1.7434 2.2625 #>  #> - Weight statistics: #>  #>         Coef of Var   MAD Entropy # Zeros #> treated       0.000 0.000   0.000       0 #> control       0.839 0.707   0.341       0 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted  429.       185 #> Weighted    252.12     185 cobalt::bal.tab(W1) #> Balance Measures #>                Type Diff.Adj #> prop.score Distance   0.0164 #> age         Contin.  -0.0000 #> educ        Contin.   0.0000 #> married      Binary  -0.0000 #> nodegree     Binary  -0.0000 #> re74        Contin.  -0.0000 #>  #> Effective sample sizes #>            Control Treated #> Unadjusted  429.       185 #> Adjusted    252.12     185  #Balancing covariates with respect to race (multi-category) (W2 <- weightit(race ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"ipt\", estimand = \"ATE\")) #> A weightit object #>  - method: \"ipt\" (inverse probability tilting) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 3-category (black, hispan, white) #>  - estimand: ATE #>  - covariates: age, educ, married, nodegree, re74 summary(W2) #>                   Summary of weights #>  #> - Weight ranges: #>  #>           Min                                   Max #> black  1.5699  |------------|               15.7872 #> hispan 1.7112  |--------------------------| 29.0704 #> white  1.1023 |--|                           4.6925 #>  #> - Units with the 5 most extreme weights by group: #>                                                 #>             226     244     485     181     182 #>   black  6.5667  6.7697  7.0956  9.9758 15.7872 #>             392     564     269     345     371 #>  hispan 17.4359 21.6727 23.0335 24.2297 29.0704 #>              68     457     599     589     531 #>   white  3.8408  3.9124  3.9335  4.1771  4.6925 #>  #> - Weight statistics: #>  #>        Coef of Var   MAD Entropy # Zeros #> black        0.618 0.398   0.133       0 #> hispan       0.618 0.442   0.164       0 #> white        0.389 0.316   0.070       0 #>  #> - Effective Sample Sizes: #>  #>             black hispan  white #> Unweighted 243.     72.  299.   #> Weighted   176.11   52.3 259.76 cobalt::bal.tab(W2) #> Balance summary across all treatment pairs #>             Type Max.Diff.Adj #> age      Contin.            0 #> educ     Contin.            0 #> married   Binary            0 #> nodegree  Binary            0 #> re74     Contin.            0 #>  #> Effective sample sizes #>             black hispan  white #> Unadjusted 243.     72.  299.   #> Adjusted   176.11   52.3 259.76"},{"path":"https://ngreifer.github.io/WeightIt/reference/method_npcbps.html","id":null,"dir":"Reference","previous_headings":"","what":"Nonparametric Covariate Balancing Propensity Score Weighting — method_npcbps","title":"Nonparametric Covariate Balancing Propensity Score Weighting — method_npcbps","text":"page explains details estimating weights nonparametric covariate balancing propensity scores setting method = \"npcbps\" call weightit() weightitMSM(). method can used binary, multi-category, continuous treatments. general, method relies estimating weights maximizing empirical likelihood data subject balance constraints. method relies CBPSnpCBPS CBPS package.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_npcbps.html","id":"binary-treatments","dir":"Reference","previous_headings":"","what":"Binary Treatments","title":"Nonparametric Covariate Balancing Propensity Score Weighting — method_npcbps","text":"binary treatments, method estimates weights using CBPSnpCBPS. ATE estimand allowed. weights taken output npCBPS fit object.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_npcbps.html","id":"multi-category-treatments","dir":"Reference","previous_headings":"","what":"Multi-Category Treatments","title":"Nonparametric Covariate Balancing Propensity Score Weighting — method_npcbps","text":"multi-category treatments, method estimates weights using CBPSnpCBPS. ATE estimand allowed. weights taken output npCBPS fit object.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_npcbps.html","id":"continuous-treatments","dir":"Reference","previous_headings":"","what":"Continuous Treatments","title":"Nonparametric Covariate Balancing Propensity Score Weighting — method_npcbps","text":"continuous treatments, method estimates weights using CBPSnpCBPS. weights taken output npCBPS fit object.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_npcbps.html","id":"longitudinal-treatments","dir":"Reference","previous_headings":"","what":"Longitudinal Treatments","title":"Nonparametric Covariate Balancing Propensity Score Weighting — method_npcbps","text":"longitudinal treatments, weights product weights estimated time point. CBPSCBMSM estimates weights longitudinal treatments.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_npcbps.html","id":"sampling-weights","dir":"Reference","previous_headings":"","what":"Sampling Weights","title":"Nonparametric Covariate Balancing Propensity Score Weighting — method_npcbps","text":"Sampling weights supported method = \"npcbps\".","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_npcbps.html","id":"missing-data","dir":"Reference","previous_headings":"","what":"Missing Data","title":"Nonparametric Covariate Balancing Propensity Score Weighting — method_npcbps","text":"presence missing data, following value(s) missing allowed: \"ind\" (default) First, variable missingness, new missingness indicator variable created takes value 1 original covariate NA 0 otherwise. missingness indicators added model formula main effects. missing values covariates replaced covariate medians (value arbitrary affect estimation). weight estimation proceeds new formula set covariates. covariates output resulting weightit object original covariates NAs.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_npcbps.html","id":"m-estimation","dir":"Reference","previous_headings":"","what":"M-estimation","title":"Nonparametric Covariate Balancing Propensity Score Weighting — method_npcbps","text":"M-estimation supported.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_npcbps.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Nonparametric Covariate Balancing Propensity Score Weighting — method_npcbps","text":"Nonparametric CBPS involves specification constrained optimization problem weights. constraints correspond covariate balance, loss function empirical likelihood data given weights. npCBPS similar entropy balancing generally produce similar results. optimization problem npCBPS convex can slow converge converge , approximate balance allowed instead using cor.prior argument, controls average deviation zero correlation treatment covariates allowed.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_npcbps.html","id":"additional-arguments","dir":"Reference","previous_headings":"","what":"Additional Arguments","title":"Nonparametric Covariate Balancing Propensity Score Weighting — method_npcbps","text":"moments int accepted. See weightit() details. quantile named list quantiles (values 0 1) continuous covariate, used create additional variables balanced ensure balance corresponding quantile variable. example, setting quantile = list(x1 = c(.25, .5. , .75)) ensures 25th, 50th, 75th percentiles x1 treatment group balanced weighted sample. Can also single number (e.g., .5) unnamed list length 1 (e.g., list(c(.25, .5, .75))) request quantile(s) continuous covariates, named vector (e.g., c(x1 = .5, x2 = .75) request one quantile covariate. allowed binary multi-category treatments. arguments npCBPS() can passed weightit() weightitMSM(). arguments take defaults npCBPS().","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_npcbps.html","id":"additional-outputs","dir":"Reference","previous_headings":"","what":"Additional Outputs","title":"Nonparametric Covariate Balancing Propensity Score Weighting — method_npcbps","text":"obj include.obj = TRUE, nonparametric CB(G)PS model fit. output call CBPSnpCBPS.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_npcbps.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Nonparametric Covariate Balancing Propensity Score Weighting — method_npcbps","text":"Fong, C., Hazlett, C., & Imai, K. (2018). Covariate balancing propensity score continuous treatment: Application efficacy political advertisements. Annals Applied Statistics, 12(1), 156–177. doi:10.1214/17-AOAS1101","code":""},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/reference/method_npcbps.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Nonparametric Covariate Balancing Propensity Score Weighting — method_npcbps","text":"","code":"# Examples take a long time to run library(\"cobalt\") data(\"lalonde\", package = \"cobalt\") # \\donttest{   #Balancing covariates between treatment groups (binary)   (W1 <- weightit(treat ~ age + educ + married +                     nodegree + re74, data = lalonde,                   method = \"npcbps\", estimand = \"ATE\")) #> A weightit object #>  - method: \"npcbps\" (non-parametric covariate balancing propensity score weighting) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 2-category #>  - estimand: ATE #>  - covariates: age, educ, married, nodegree, re74   summary(W1) #>                   Summary of weights #>  #> - Weight ranges: #>  #>            Min                                  Max #> treated 0.5587 |---------------------------| 9.8862 #> control 0.5594 |---|                         2.1293 #>  #> - Units with the 5 most extreme weights by group: #>                                             #>             172     69     58    181    182 #>  treated 3.3634  4.199 8.3691 8.4396 9.8862 #>             411    595    269    409    296 #>  control 1.6451 1.6633 1.7413 1.8249 2.1293 #>  #> - Weight statistics: #>  #>         Coef of Var   MAD Entropy # Zeros #> treated       1.143 0.512   0.302       0 #> control       0.269 0.230   0.035       0 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted  429.    185.   #> Weighted    400.19   80.48   bal.tab(W1) #> Balance Measures #>             Type Diff.Adj #> age      Contin.   0.0295 #> educ     Contin.  -0.0135 #> married   Binary   0.0407 #> nodegree  Binary  -0.0121 #> re74     Contin.   0.0704 #>  #> Effective sample sizes #>            Control Treated #> Unadjusted  429.    185.   #> Adjusted    400.19   80.48    #Balancing covariates with respect to race (multi-category)   (W2 <- weightit(race ~ age + educ + married +                     nodegree + re74, data = lalonde,                   method = \"npcbps\", estimand = \"ATE\")) #> A weightit object #>  - method: \"npcbps\" (non-parametric covariate balancing propensity score weighting) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 3-category (black, hispan, white) #>  - estimand: ATE #>  - covariates: age, educ, married, nodegree, re74   summary(W2) #>                   Summary of weights #>  #> - Weight ranges: #>  #>           Min                                  Max #> black  0.6417  |--------------------------| 9.3668 #> hispan 0.2853 |---------------|             5.5253 #> white  0.4681 |-----|                       2.5147 #>  #> - Units with the 5 most extreme weights by group: #>                                            #>            226    244    605    181    182 #>   black 2.5367 2.7284 3.0888 4.5045 9.3668 #>            392    564    269    371    345 #>  hispan 2.0057 2.2325 3.0749 4.2753 5.5253 #>             68    457    599    589    531 #>   white 1.9505 1.9707 2.0169 2.1335 2.5147 #>  #> - Weight statistics: #>  #>        Coef of Var   MAD Entropy # Zeros #> black        0.753 0.413   0.160       0 #> hispan       0.820 0.462   0.220       0 #> white        0.414 0.331   0.079       0 #>  #> - Effective Sample Sizes: #>  #>            black hispan  white #> Unweighted 243.   72.   299.   #> Weighted   155.3  43.31 255.36   bal.tab(W2) #> Balance summary across all treatment pairs #>             Type Max.Diff.Adj #> age      Contin.       0.0310 #> educ     Contin.       0.0431 #> married   Binary       0.0225 #> nodegree  Binary       0.0158 #> re74     Contin.       0.0432 #>  #> Effective sample sizes #>            black hispan  white #> Unadjusted 243.   72.   299.   #> Adjusted   155.3  43.31 255.36 # }"},{"path":"https://ngreifer.github.io/WeightIt/reference/method_optweight.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimization-Based Weighting — method_optweight","title":"Optimization-Based Weighting — method_optweight","text":"page explains details estimating optimization-based weights (also known stable balancing weights) setting method = \"optweight\" call weightit() weightitMSM(). method can used binary, multi-category, continuous treatments. general, method relies estimating weights solving quadratic programming problem subject approximate exact balance constraints. method relies optweightoptweight optweight package. optweight() offers finer control uses syntax weightit(), recommended optweightoptweight used instead weightit() method = \"optweight\".","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_optweight.html","id":"binary-treatments","dir":"Reference","previous_headings":"","what":"Binary Treatments","title":"Optimization-Based Weighting — method_optweight","text":"binary treatments, method estimates weights using optweightoptweight. following estimands allowed: ATE, ATT, ATC. weights taken output optweight fit object.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_optweight.html","id":"multi-category-treatments","dir":"Reference","previous_headings":"","what":"Multi-Category Treatments","title":"Optimization-Based Weighting — method_optweight","text":"multi-category treatments, method estimates weights using optweightoptweight. following estimands allowed: ATE ATT. weights taken output optweight fit object.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_optweight.html","id":"continuous-treatments","dir":"Reference","previous_headings":"","what":"Continuous Treatments","title":"Optimization-Based Weighting — method_optweight","text":"binary treatments, method estimates weights using optweightoptweight. weights taken output optweight fit object.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_optweight.html","id":"longitudinal-treatments","dir":"Reference","previous_headings":"","what":"Longitudinal Treatments","title":"Optimization-Based Weighting — method_optweight","text":"longitudinal treatments, optweight() estimates weights simultaneously satisfy balance constraints time points, one model fit obtain weights. Using method = \"optweight\" weightitMSM() causes .MSM.method set TRUE default. Setting FALSE run one model time point multiply weights together, method recommended. NOTE: neither use optimization-based weights longitudinal treatments validated!","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_optweight.html","id":"sampling-weights","dir":"Reference","previous_headings":"","what":"Sampling Weights","title":"Optimization-Based Weighting — method_optweight","text":"Sampling weights supported s.weights scenarios, versions optweight 0.2.5.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_optweight.html","id":"missing-data","dir":"Reference","previous_headings":"","what":"Missing Data","title":"Optimization-Based Weighting — method_optweight","text":"presence missing data, following value(s) missing allowed: \"ind\" (default) First, variable missingness, new missingness indicator variable created takes value 1 original covariate NA 0 otherwise. missingness indicators added model formula main effects. missing values covariates replaced covariate medians (value arbitrary affect estimation). weight estimation proceeds new formula set covariates. covariates output resulting weightit object original covariates NAs.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_optweight.html","id":"m-estimation","dir":"Reference","previous_headings":"","what":"M-estimation","title":"Optimization-Based Weighting — method_optweight","text":"M-estimation supported.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_optweight.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimization-Based Weighting — method_optweight","text":"Stable balancing weights weights solve constrained optimization problem, constraints correspond covariate balance loss function variance (norm) weights. weights maximize effective sample size weighted sample subject user-supplied balance constraints. advantage method entropy balancing ability allow approximate, rather exact, balance tols argument, can increase precision even slight relaxations constraints. plot() can used output weightit() method = \"optweight\" display dual variables; see Examples plot.weightit() details.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_optweight.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Optimization-Based Weighting — method_optweight","text":"specification tols differs weightit() optweight(). weightit(), one tolerance value included per level factor variable, whereas optweight(), levels factor given tolerance, one value needs supplied factor variable. potential confusion ambiguity, recommended supply one value tols weightit() applies variables. finer control, use optweight() directly. Seriously, just use optweightoptweight. syntax almost identical compatible cobalt, .","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_optweight.html","id":"additional-arguments","dir":"Reference","previous_headings":"","what":"Additional Arguments","title":"Optimization-Based Weighting — method_optweight","text":"moments int accepted. See weightit() details. quantile named list quantiles (values 0 1) continuous covariate, used create additional variables balanced ensure balance corresponding quantile variable. example, setting quantile = list(x1 = c(.25, .5. , .75)) ensures 25th, 50th, 75th percentiles x1 treatment group balanced weighted sample. Can also single number (e.g., .5) unnamed list length 1 (e.g., list(c(.25, .5, .75))) request quantile(s) continuous covariates, named vector (e.g., c(x1 = .5, x2 = .75) request one quantile covariate. allowed binary multi-category treatments. arguments optweight() can passed weightit() weightitMSM(), following exception: targets used ignored. arguments take defaults optweight().","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_optweight.html","id":"additional-outputs","dir":"Reference","previous_headings":"","what":"Additional Outputs","title":"Optimization-Based Weighting — method_optweight","text":"info list one entry: duals data frame dual variables balance constraint. obj include.obj = TRUE, output call optweightoptweight.","code":""},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/reference/method_optweight.html","id":"binary-treatments-1","dir":"Reference","previous_headings":"","what":"Binary treatments","title":"Optimization-Based Weighting — method_optweight","text":"Wang, Y., & Zubizarreta, J. R. (2020). Minimal dispersion approximately balancing weights: Asymptotic properties practical considerations. Biometrika, 107(1), 93–105. doi:10.1093/biomet/asz050 Zubizarreta, J. R. (2015). Stable Weights Balance Covariates Estimation Incomplete Outcome Data. Journal American Statistical Association, 110(511), 910–922. doi:10.1080/01621459.2015.1023805","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_optweight.html","id":"multi-category-treatments-1","dir":"Reference","previous_headings":"","what":"Multi-Category Treatments","title":"Optimization-Based Weighting — method_optweight","text":"de los Angeles Resa, M., & Zubizarreta, J. R. (2020). Direct stable weight adjustment non-experimental studies multivalued treatments: Analysis effect earthquake post-traumatic stress. Journal Royal Statistical Society: Series (Statistics Society), n/(n/). doi:10.1111/rssa.12561","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_optweight.html","id":"continuous-treatments-1","dir":"Reference","previous_headings":"","what":"Continuous treatments","title":"Optimization-Based Weighting — method_optweight","text":"Greifer, N. (2020). Estimating Balancing Weights Continuous Treatments Using Constrained Optimization. doi:10.17615/DYSS-B342","code":""},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/reference/method_optweight.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimization-Based Weighting — method_optweight","text":"","code":"data(\"lalonde\", package = \"cobalt\")  #Balancing covariates between treatment groups (binary) (W1 <- weightit(treat ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"optweight\", estimand = \"ATT\",                 tols = 0)) #> A weightit object #>  - method: \"optweight\" (stable balancing weights) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 2-category #>  - estimand: ATT (focal: 1) #>  - covariates: age, educ, married, nodegree, re74 summary(W1) #>                   Summary of weights #>  #> - Weight ranges: #>  #>         Min                                  Max #> treated   1           ||                  1.0000 #> control   0 |---------------------------| 3.0426 #>  #> - Units with the 5 most extreme weights by group: #>                                             #>               5      4      3      2      1 #>  treated      1      1      1      1      1 #>             411    589    269    409    296 #>  control 2.5261 2.5415 2.6434 2.7396 3.0426 #>  #> - Weight statistics: #>  #>         Coef of Var   MAD Entropy # Zeros #> treated       0.000 0.000   0.000       0 #> control       0.788 0.697   0.393      83 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted  429.       185 #> Weighted    264.88     185 cobalt::bal.tab(W1) #> Balance Measures #>             Type Diff.Adj #> age      Contin.       -0 #> educ     Contin.       -0 #> married   Binary       -0 #> nodegree  Binary        0 #> re74     Contin.       -0 #>  #> Effective sample sizes #>            Control Treated #> Unadjusted  429.       185 #> Adjusted    264.88     185 plot(W1)   #Balancing covariates with respect to race (multi-category) (W2 <- weightit(race ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"optweight\", estimand = \"ATE\",                 tols = .01)) #> A weightit object #>  - method: \"optweight\" (stable balancing weights) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 3-category (black, hispan, white) #>  - estimand: ATE #>  - covariates: age, educ, married, nodegree, re74 summary(W2) #>                   Summary of weights #>  #> - Weight ranges: #>  #>           Min                                  Max #> black  0.4429     |-----------------------| 3.5741 #> hispan 0.0000 |-------------------|         2.5848 #> white  0.2574   |---------|                 1.6593 #>  #> - Units with the 5 most extreme weights by group: #>                                            #>            184    190    485    181    182 #>   black 2.3351 2.3723 2.5586 2.8367 3.5741 #>            392    345    269    564    371 #>  hispan 2.0459 2.0984 2.1887 2.1982 2.5848 #>             68    589    324    599    531 #>   white 1.5706 1.5706 1.5725 1.5968 1.6593 #>  #> - Weight statistics: #>  #>        Coef of Var   MAD Entropy # Zeros #> black        0.550 0.443   0.130       0 #> hispan       0.566 0.449   0.176       2 #> white        0.353 0.295   0.065       0 #>  #> - Effective Sample Sizes: #>  #>             black hispan  white #> Unweighted 243.     72.  299.   #> Weighted   186.76   54.7 266.01 cobalt::bal.tab(W2) #> Balance summary across all treatment pairs #>             Type Max.Diff.Adj #> age      Contin.         0.01 #> educ     Contin.         0.01 #> married   Binary         0.01 #> nodegree  Binary         0.01 #> re74     Contin.         0.01 #>  #> Effective sample sizes #>             black hispan  white #> Unadjusted 243.     72.  299.   #> Adjusted   186.76   54.7 266.01 plot(W2)   #Balancing covariates with respect to re75 (continuous)"},{"path":"https://ngreifer.github.io/WeightIt/reference/method_super.html","id":null,"dir":"Reference","previous_headings":"","what":"Propensity Score Weighting Using SuperLearner — method_super","title":"Propensity Score Weighting Using SuperLearner — method_super","text":"page explains details estimating weights SuperLearner-based propensity scores setting method = \"super\" call weightit() weightitMSM(). method can used binary, multi-category, continuous treatments. general, method relies estimating propensity scores using SuperLearner algorithm stacking predictions converting propensity scores weights using formula depends desired estimand. binary multi-category treatments, one binary classification algorithms used estimate propensity scores predicted probability treatment given covariates. continuous treatments, regression algorithms used estimate generalized propensity scores conditional density treatment given covariates. method relies SuperLearnerSuperLearner SuperLearner package.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_super.html","id":"binary-treatments","dir":"Reference","previous_headings":"","what":"Binary Treatments","title":"Propensity Score Weighting Using SuperLearner — method_super","text":"binary treatments, method estimates propensity scores using SuperLearnerSuperLearner. following estimands allowed: ATE, ATT, ATC, ATO, ATM, ATOS. Weights can also computed using marginal mean weighting stratification ATE, ATT, ATC. See get_w_from_ps() details.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_super.html","id":"multi-category-treatments","dir":"Reference","previous_headings":"","what":"Multi-Category Treatments","title":"Propensity Score Weighting Using SuperLearner — method_super","text":"multi-category treatments, propensity scores estimated using several calls SuperLearnerSuperLearner, one treatment group; treatment probabilities normalized sum 1. following estimands allowed: ATE, ATT, ATC, ATO, ATM. weights estimand computed using standard formulas mentioned . Weights can also computed using marginal mean weighting stratification ATE, ATT, ATC. See get_w_from_ps() details.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_super.html","id":"continuous-treatments","dir":"Reference","previous_headings":"","what":"Continuous Treatments","title":"Propensity Score Weighting Using SuperLearner — method_super","text":"continuous treatments, generalized propensity score estimated using SuperLearnerSuperLearner. addition, kernel density estimation can used instead assuming normal density numerator denominator generalized propensity score setting density = \"kernel\". arguments density() can specified refine density estimation parameters. plot = TRUE can specified plot density numerator denominator, can helpful diagnosing extreme weights.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_super.html","id":"longitudinal-treatments","dir":"Reference","previous_headings":"","what":"Longitudinal Treatments","title":"Propensity Score Weighting Using SuperLearner — method_super","text":"longitudinal treatments, weights product weights estimated time point.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_super.html","id":"sampling-weights","dir":"Reference","previous_headings":"","what":"Sampling Weights","title":"Propensity Score Weighting Using SuperLearner — method_super","text":"Sampling weights supported s.weights scenarios.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_super.html","id":"missing-data","dir":"Reference","previous_headings":"","what":"Missing Data","title":"Propensity Score Weighting Using SuperLearner — method_super","text":"presence missing data, following value(s) missing allowed: \"ind\" (default) First, variable missingness, new missingness indicator variable created takes value 1 original covariate NA 0 otherwise. missingness indicators added model formula main effects. missing values covariates replaced covariate medians (value arbitrary affect estimation). weight estimation proceeds new formula set covariates. covariates output resulting weightit object original covariates NAs.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_super.html","id":"m-estimation","dir":"Reference","previous_headings":"","what":"M-estimation","title":"Propensity Score Weighting Using SuperLearner — method_super","text":"M-estimation supported.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_super.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Propensity Score Weighting Using SuperLearner — method_super","text":"SuperLearner works fitting several machine learning models treatment covariates taking weighted combination generated predicted values use propensity scores, used construct weights. machine learning models used supplied using SL.library argument; models supplied, higher chance correctly modeling propensity score. good idea include parameteric models, flexible tree-based models, regularized models among models selected. predicted values combined using method supplied SL.method argument (nonnegative least squares default). benefit SuperLearner , asymptotically, guaranteed perform well better best-performing method included library. Using Balance SuperLearner setting SL.method = \"method.balance\" works selecting combination predicted values minimizes imbalance measure.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_super.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Propensity Score Weighting Using SuperLearner — method_super","text":"methods formerly available SuperLearner now SuperLearnerExtra, can found GitHub https://github.com/ecpolley/SuperLearnerExtra. criterion argument used called stop.method, name twang. stop.method still works backward compatibility. Additionally, criteria formerly named es.mean, es.max, es.rms renamed smd.mean, smd.max, smd.rms. former used twang still work weightit() backward compatibility. version 1.2.0, default behavior binary multi-category treatments stratify treatment performing cross-validation ensure treatment groups represented cross-validation. recover previous behavior, set cvControl = list(stratifyCV = FALSE).","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_super.html","id":"additional-arguments","dir":"Reference","previous_headings":"","what":"Additional Arguments","title":"Propensity Score Weighting Using SuperLearner — method_super","text":"discrete TRUE, uses discrete SuperLearner, simply selects best performing method. Default FALSE, finds optimal combination predictions libraries using SL.method. argument SL.library must supplied. see list available entries, use SuperLearnerlistWrappers. arguments SuperLearnerSuperLearner can passed weightit() weightitMSM(), following exceptions: obsWeights ignored sampling weights passed using s.weights. method SuperLearner() replaced argument SL.method weightit(). continuous treatments , following arguments may supplied: density function corresponding conditional density treatment. standardized residuals treatment model fed function produce numerator denominator generalized propensity score weights. blank, dnorm() used recommended Robins et al. (2000). can also supplied string containing name function called. string contains underscores, call split underscores latter splits supplied arguments second argument beyond. example, density = \"dt_2\" specified, density used t-distribution 2 degrees freedom. Using t-distribution can useful extreme outcome values observed (Naimi et al., 2014). Can also \"kernel\" use kernel density estimation, calls density() estimate numerator denominator densities weights. (used requested setting use.kernel = TRUE, now deprecated.) bw, adjust, kernel, n density = \"kernel\", arguments density(). defaults density() except n 10 times number units sample. plot density = \"kernel\", whether plot estimated densities.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_super.html","id":"balance-superlearner","dir":"Reference","previous_headings":"","what":"Balance SuperLearner","title":"Propensity Score Weighting Using SuperLearner — method_super","text":"addition methods allowed SuperLearner(), one can specify SL.method = \"method.balance\" use \"Balance SuperLearner\" described Pirracchio Carone (2018), wherein covariate balance used choose optimal combination predictions methods specified SL.library. Coefficients chosen (one prediction method) weights generated weighted combination predictions optimize balance criterion, must set criterion argument, described . criterion string describing balance criterion used select best weights. See cobaltbal.compute allowable options treatment type. binary multi-category treatments, default \"smd.mean\", minimizes average absolute standard mean difference among covariates treatment groups. continuous treatments, default \"p.mean\", minimizes average absolute Pearson correlation treatment covariates. Note implementation differs Pirracchio Carone (2018) , balance measured terms included model formula (.e., interactions unless specifically included), balance results sample weighted using estimated predicted values propensity scores, sample matched using propensity score matching predicted values. Binary continuous treatments supported, currently multi-category treatments .","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_super.html","id":"additional-outputs","dir":"Reference","previous_headings":"","what":"Additional Outputs","title":"Propensity Score Weighting Using SuperLearner — method_super","text":"info binary continuous treatments, list two entries, coef cvRisk. multi-category treatments, list lists two entries, one treatment level. coef coefficients linear combination predictions method SL.library. Higher values indicate corresponding method plays larger role determining resulting predicted value, values close zero indicate method plays little role determining predicted value. discrete = TRUE, correspond coefficients estimated discrete FALSE. cvRisk cross-validation risk method SL.library. Higher values indicate method worse cross-validation accuracy. SL.method = \"method.balance\", sample weighted balance statistic requested criterion. Higher values indicate worse balance. obj include.obj = TRUE, SuperLearner fit(s) used generate predicted values. binary continuous treatments, output call SuperLearnerSuperLearner. multi-category treatments, list outputs calls SuperLearner::SuperLearner().","code":""},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/reference/method_super.html","id":"binary-treatments-1","dir":"Reference","previous_headings":"","what":"Binary treatments","title":"Propensity Score Weighting Using SuperLearner — method_super","text":"Pirracchio, R., Petersen, M. L., & van der Laan, M. (2015). Improving Propensity Score Estimators’ Robustness Model Misspecification Using Super Learner. American Journal Epidemiology, 181(2), 108–119. doi:10.1093/aje/kwu253","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_super.html","id":"continuous-treatments-1","dir":"Reference","previous_headings":"","what":"Continuous treatments","title":"Propensity Score Weighting Using SuperLearner — method_super","text":"Kreif, N., Grieve, R., Díaz, ., & Harrison, D. (2015). Evaluation Effect Continuous Treatment: Machine Learning Approach Application Treatment Traumatic Brain Injury. Health Economics, 24(9), 1213–1228. doi:10.1002/hec.3189","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_super.html","id":"balance-superlearner-sl-method-method-balance-","dir":"Reference","previous_headings":"","what":"Balance SuperLearner (SL.method = \"method.balance\")","title":"Propensity Score Weighting Using SuperLearner — method_super","text":"Pirracchio, R., & Carone, M. (2018). Balance Super Learner: robust adaptation Super Learner improve estimation average treatment effect treated based propensity score matching. Statistical Methods Medical Research, 27(8), 2504–2518. doi:10.1177/0962280216682055 See method_glm additional references.","code":""},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/reference/method_super.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Propensity Score Weighting Using SuperLearner — method_super","text":"","code":"library(\"cobalt\") data(\"lalonde\", package = \"cobalt\")  #Note: for time, all exmaples use a small set of #      learners. Many more should be added if #      possible, including a variety of model #      types (e.g., parametric, flexible, tree- #.     based, regularized, etc.)  #Balancing covariates between treatment groups (binary) (W1 <- weightit(treat ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"super\", estimand = \"ATT\",                 SL.library = c(\"SL.glm\", \"SL.stepAIC\",                                \"SL.glm.interaction\"))) #> Loading required package: nnls #> A weightit object #>  - method: \"super\" (propensity score weighting with SuperLearner) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 2-category #>  - estimand: ATT (focal: 1) #>  - covariates: age, educ, married, nodegree, re74 summary(W1) #>                   Summary of weights #>  #> - Weight ranges: #>  #>            Min                                  Max #> treated 1.0000       ||                      1.0000 #> control 0.0077 |---------------------------| 4.7517 #>  #> - Units with the 5 most extreme weights by group: #>                                             #>               5      4      3      2      1 #>  treated      1      1      1      1      1 #>             411    589    269    409    296 #>  control 2.2351 2.3211 2.5066 3.0135 4.7517 #>  #> - Weight statistics: #>  #>         Coef of Var   MAD Entropy # Zeros #> treated       0.000 0.000   0.000       0 #> control       1.035 0.729   0.413       0 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted  429.       185 #> Weighted    207.44     185 bal.tab(W1) #> Balance Measures #>                Type Diff.Adj #> prop.score Distance   0.0988 #> age         Contin.  -0.0990 #> educ        Contin.   0.0204 #> married      Binary  -0.0051 #> nodegree     Binary   0.0170 #> re74        Contin.  -0.0276 #>  #> Effective sample sizes #>            Control Treated #> Unadjusted  429.       185 #> Adjusted    207.44     185  #Balancing covariates with respect to race (multi-category) (W2 <- weightit(race ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"super\", estimand = \"ATE\",                 SL.library = c(\"SL.glm\", \"SL.stepAIC\",                                \"SL.glm.interaction\"))) #> A weightit object #>  - method: \"super\" (propensity score weighting with SuperLearner) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 3-category (black, hispan, white) #>  - estimand: ATE #>  - covariates: age, educ, married, nodegree, re74 summary(W2) #>                   Summary of weights #>  #> - Weight ranges: #>  #>           Min                                   Max #> black  1.4148 |-----------------|           12.6839 #> hispan 1.9257  |--------------------------| 18.8468 #> white  1.0848 |---|                          4.4442 #>  #> - Units with the 5 most extreme weights by group: #>                                                 #>             190     184     485     182     181 #>   black  7.2352  7.9022  9.8189 11.7701 12.6839 #>             346     392     512     216     345 #>  hispan 14.2798 14.3775 14.5322  14.592 18.8468 #>              23     531     457     296     589 #>   white  3.9729  4.0126   4.069  4.1347  4.4442 #>  #> - Weight statistics: #>  #>        Coef of Var   MAD Entropy # Zeros #> black        0.617 0.385   0.130       0 #> hispan       0.416 0.333   0.088       0 #> white        0.385 0.318   0.069       0 #>  #> - Effective Sample Sizes: #>  #>             black hispan  white #> Unweighted 243.    72.   299.   #> Weighted   176.22  61.49 260.53 bal.tab(W2) #> Balance summary across all treatment pairs #>             Type Max.Diff.Adj #> age      Contin.       0.1762 #> educ     Contin.       0.0754 #> married   Binary       0.0326 #> nodegree  Binary       0.0139 #> re74     Contin.       0.0306 #>  #> Effective sample sizes #>             black hispan  white #> Unadjusted 243.    72.   299.   #> Adjusted   176.22  61.49 260.53  #Balancing covariates with respect to re75 (continuous) #assuming t(8) conditional density for treatment (W3 <- weightit(re75 ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"super\", density = \"dt_8\",                 SL.library = c(\"SL.glm\", \"SL.ridge\",                                \"SL.glm.interaction\"))) #> A weightit object #>  - method: \"super\" (propensity score weighting with SuperLearner) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: continuous #>  - covariates: age, educ, married, nodegree, re74 summary(W3) #>                   Summary of weights #>  #> - Weight ranges: #>  #>       Min                                   Max #> all 0.044 |---------------------------| 21.5982 #>  #> - Units with the 5 most extreme weights: #>                                             #>         431     483     484     485     354 #>  all 9.4757 16.4066 18.2289 19.5107 21.5982 #>  #> - Weight statistics: #>  #>     Coef of Var   MAD Entropy # Zeros #> all       1.348 0.506   0.345       0 #>  #> - Effective Sample Sizes: #>  #>             Total #> Unweighted 614.   #> Weighted   218.12 bal.tab(W3) #> Balance Measures #>             Type Corr.Adj #> age      Contin.   0.0322 #> educ     Contin.   0.0357 #> married   Binary   0.0611 #> nodegree  Binary  -0.0609 #> re74     Contin.   0.0406 #>  #> Effective sample sizes #>             Total #> Unadjusted 614.   #> Adjusted   218.12  #Balancing covariates between treatment groups (binary) # using balance SuperLearner to minimize the maximum # KS statistic (W4 <- weightit(treat ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"super\", estimand = \"ATT\",                 SL.library = c(\"SL.glm\", \"SL.stepAIC\",                                \"SL.lda\"),                 SL.method = \"method.balance\",                 criterion = \"ks.max\")) #> A weightit object #>  - method: \"super\" (propensity score weighting with SuperLearner) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 2-category #>  - estimand: ATT (focal: 1) #>  - covariates: age, educ, married, nodegree, re74 summary(W4) #>                   Summary of weights #>  #> - Weight ranges: #>  #>            Min                                  Max #> treated 1.0000               ||              1.0000 #> control 0.0222 |---------------------------| 2.0438 #>  #> - Units with the 5 most extreme weights by group: #>                                             #>               5      4      3      2      1 #>  treated      1      1      1      1      1 #>             411    595    269    409    296 #>  control 1.3303 1.4365 1.5004 1.6368 2.0438 #>  #> - Weight statistics: #>  #>         Coef of Var   MAD Entropy # Zeros #> treated       0.000 0.000    0.00       0 #> control       0.823 0.701    0.33       0 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted  429.       185 #> Weighted    255.99     185 bal.tab(W4, stats = c(\"m\", \"ks\")) #> Balance Measures #>                Type Diff.Adj KS.Adj #> prop.score Distance   0.0199 0.0944 #> age         Contin.   0.0460 0.2764 #> educ        Contin.  -0.0360 0.0601 #> married      Binary   0.0044 0.0044 #> nodegree     Binary   0.0080 0.0080 #> re74        Contin.  -0.0275 0.2839 #>  #> Effective sample sizes #>            Control Treated #> Unadjusted  429.       185 #> Adjusted    255.99     185"},{"path":"https://ngreifer.github.io/WeightIt/reference/method_user.html","id":null,"dir":"Reference","previous_headings":"","what":"User-Defined Functions for Estimating Weights — method_user","title":"User-Defined Functions for Estimating Weights — method_user","text":"page explains details estimating weights using user-defined function. function must take arguments passed weightit() weightitMSM() return vector weights list containing weights. supply user-defined function, function object entered directly method; example, function fun, method = fun.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_user.html","id":"point-treatments","dir":"Reference","previous_headings":"","what":"Point Treatments","title":"User-Defined Functions for Estimating Weights — method_user","text":"following arguments automatically passed user-defined function, named parameters corresponding : treat: vector treatment status unit. comes directly left hand side formula passed weightit() type (e.g., numeric, factor, etc.), may need converted. covs: data frame covariate values unit. comes directly right hand side formula passed weightit(). covariates processed columns numeric; factor variables split dummies interactions evaluated. levels factor variables given dummies, matrix covariates full rank. Users can use make_full_rank(), accepts numeric matrix data frame removes columns make full rank, full rank covariate matrix desired. s.weights: numeric vector sampling weights, one unit. ps: numeric vector propensity scores. subset: logical vector length treat TRUE units included estimation FALSE otherwise. used subset input objects exact used. treat, covs, s.weights, ps, supplied, already subsetted subset. estimand: character vector length 1 containing desired estimand. characters converted uppercase. \"ATC\" supplied estimand, weightit() sets focal control level (usually 0 lowest level treat) sets estimand \"ATT\". focal: character vector length 1 containing focal level treatment estimand ATT (ATC detailed ). weightit() ensures value focal level treat. stabilize: logical vector length 1. processed weightit() reaches fitting function. moments: numeric vector length 1. processed weightit() reaches fitting function except .integer() applied . used methods determine whether polynomials entered covariates used weight estimation. int: logical vector length 1. processed weightit() reaches fitting function. used methods determine whether interactions entered covariates used weight estimation. None parameters required fitting function. simply automatically available. addition, additional arguments supplied weightit() passed fitting function. weightit() ensures arguments correspond parameters fitting function throws error incorrectly named argument supplied fitting function include \\dots parameter. fitting function must output either numeric vector weights list (list-like object) entry named wither \"w\" \"weights\". list, list can contain named entries, entries named \"w\", \"weights\", \"ps\", \"fit.obj\" processed. \"ps\" vector propensity scores \"fit.obj\" object used fitting process user may want examine included weightit output object \"obj\" include.obj = TRUE. \"ps\" \"fit.obj\" components optional, \"weights\" \"w\" required.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_user.html","id":"longitudinal-treatments","dir":"Reference","previous_headings":"","what":"Longitudinal Treatments","title":"User-Defined Functions for Estimating Weights — method_user","text":"Longitudinal treatments can handled either running fitting function point treatments time point multiplying resulting weights together running method accommodates multiple time points outputs single set weights. former, weightitMSM() can used user-defined function just weightit(). latter method yet accommodated weightitMSM(), someday, maybe.","code":""},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/reference/method_user.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"User-Defined Functions for Estimating Weights — method_user","text":"","code":"library(\"cobalt\") data(\"lalonde\", package = \"cobalt\")  #A user-defined version of method = \"ps\" my.ps <- function(treat, covs, estimand, focal = NULL) {   covs <- make_full_rank(covs)   d <- data.frame(treat, covs)   f <- formula(d)   ps <- glm(f, data = d, family = \"binomial\")$fitted   w <- get_w_from_ps(ps, treat = treat, estimand = estimand,                      focal = focal)    list(w = w, ps = ps) }  #Balancing covariates between treatment groups (binary) (W1 <- weightit(treat ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = my.ps, estimand = \"ATT\")) #> A weightit object #>  - method: \"my.ps\" (a user-defined method) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 2-category #>  - estimand: ATT (focal: 1) #>  - covariates: age, educ, married, nodegree, re74 summary(W1) #>                   Summary of weights #>  #> - Weight ranges: #>  #>            Min                                  Max #> treated 1.0000               ||              1.0000 #> control 0.0222 |---------------------------| 2.0438 #>  #> - Units with the 5 most extreme weights by group: #>                                             #>               5      4      3      2      1 #>  treated      1      1      1      1      1 #>             411    595    269    409    296 #>  control 1.3303 1.4365 1.5005 1.6369 2.0438 #>  #> - Weight statistics: #>  #>         Coef of Var   MAD Entropy # Zeros #> treated       0.000 0.000    0.00       0 #> control       0.823 0.701    0.33       0 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted  429.       185 #> Weighted    255.99     185 bal.tab(W1) #> Balance Measures #>                Type Diff.Adj #> prop.score Distance   0.0199 #> age         Contin.   0.0459 #> educ        Contin.  -0.0360 #> married      Binary   0.0044 #> nodegree     Binary   0.0080 #> re74        Contin.  -0.0275 #>  #> Effective sample sizes #>            Control Treated #> Unadjusted  429.       185 #> Adjusted    255.99     185  data(\"msmdata\") (W2 <- weightitMSM(list(A_1 ~ X1_0 + X2_0,                         A_2 ~ X1_1 + X2_1 +                           A_1 + X1_0 + X2_0,                         A_3 ~ X1_2 + X2_2 +                           A_2 + X1_1 + X2_1 +                           A_1 + X1_0 + X2_0),                    data = msmdata,                    method = my.ps)) #> A weightitMSM object #>  - method: \"my.ps\" (a user-defined method) #>  - number of obs.: 7500 #>  - sampling weights: none #>  - number of time points: 3 (A_1, A_2, A_3) #>  - treatment: #>     + time 1: 2-category #>     + time 2: 2-category #>     + time 3: 2-category #>  - covariates: #>     + baseline: X1_0, X2_0 #>     + after time 1: X1_1, X2_1, A_1, X1_0, X2_0 #>     + after time 2: X1_2, X2_2, A_2, X1_1, X2_1, A_1, X1_0, X2_0  summary(W2) #>                         Time 1                         #>                   Summary of weights #>  #> - Weight ranges: #>  #>            Min                                    Max #> treated 1.0791 |---------------------------| 403.4833 #> control 1.2761 |-------------------|         284.7636 #>  #> - Units with the 5 most extreme weights by group: #>                                                       #>              5488     3440     3593     1286     5685 #>  treated  166.992 170.5549 196.4136 213.1934 403.4833 #>              2594     2932     5226     1875     2533 #>  control 155.6248  168.964 172.4195 245.8822 284.7636 #>  #> - Weight statistics: #>  #>         Coef of Var   MAD Entropy # Zeros #> treated       1.914 0.816   0.649       0 #> control       1.706 0.862   0.670       0 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted 3306.    4194.  #> Weighted    845.79   899.4 #>  #>                         Time 2                         #>                   Summary of weights #>  #> - Weight ranges: #>  #>            Min                                    Max #> treated 1.0791 |---------------------------| 403.4833 #> control 1.2761 |----------------|            245.8822 #>  #> - Units with the 5 most extreme weights by group: #>                                                       #>              2932     3440     3593     2533     5685 #>  treated  168.964 170.5549 196.4136 284.7636 403.4833 #>              2594     5488     5226     1286     1875 #>  control 155.6248  166.992 172.4195 213.1934 245.8822 #>  #> - Weight statistics: #>  #>         Coef of Var   MAD Entropy # Zeros #> treated       1.892 0.819   0.652       0 #> control       1.748 0.869   0.686       0 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted 3701.   3799.   #> Weighted    912.87  829.87 #>  #>                         Time 3                         #>                   Summary of weights #>  #> - Weight ranges: #>  #>            Min                                    Max #> treated 1.0791 |---------------------------| 403.4833 #> control 1.2761 |---------|                   148.1547 #>  #> - Units with the 5 most extreme weights by group: #>                                                       #>              3593     1286     1875     2533     5685 #>  treated 196.4136 213.1934 245.8822 284.7636 403.4833 #>              6862      168     3729     6158     3774 #>  control  88.0721  97.8273  104.623 121.8451 148.1547 #>  #> - Weight statistics: #>  #>         Coef of Var   MAD Entropy # Zeros #> treated       1.832 0.975   0.785       0 #> control       1.254 0.683   0.412       0 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted 4886.   2614.   #> Weighted   1900.26  600.12 #>  bal.tab(W2) #> Balance summary across all time points #>        Times    Type Max.Diff.Adj #> X1_0 1, 2, 3 Contin.       0.0342 #> X2_0 1, 2, 3  Binary       0.0299 #> X1_1    2, 3 Contin.       0.0657 #> X2_1    2, 3  Binary       0.0299 #> A_1     2, 3  Binary       0.0262 #> X1_2       3 Contin.       0.0643 #> X2_2       3  Binary       0.0096 #> A_2        3  Binary       0.0054 #>  #> Effective sample sizes #>  - Time 1 #>            Control Treated #> Unadjusted 3306.    4194.  #> Adjusted    845.79   899.4 #>  - Time 2 #>            Control Treated #> Unadjusted 3701.   3799.   #> Adjusted    912.87  829.87 #>  - Time 3 #>            Control Treated #> Unadjusted 4886.   2614.   #> Adjusted   1900.26  600.12  # Kernel balancing using the `kbal` package, available # using `pak::pak_install(\"chadhazlett/KBAL\")`. # Only the ATT and ATC are available.  if (FALSE) { # \\dontrun{   kbal.fun <- function(treat, covs, estimand, focal, verbose, ...) {     args <- list(...)      if (!estimand %in% c(\"ATT\", \"ATC\"))       stop('`estimand` must be \"ATT\" or \"ATC\".', call. = FALSE)      treat <- as.numeric(treat == focal)      args <- args[names(args) %in% names(formals(kbal::kbal))]     args$allx <- covs     args$treatment <- treat     args$printprogress <- verbose      cat_cols <- apply(covs, 2, function(x) length(unique(x)) <= 2)      if (all(cat_cols)) {       args$cat_data <- TRUE       args$mixed_data <- FALSE       args$scale_data <- FALSE       args$linkernel <- FALSE       args$drop_MC <- FALSE     }     else if (any(cat_cols)) {       args$cat_data <- FALSE       args$mixed_data <- TRUE       args$cat_columns <- colnames(covs)[cat_cols]       args$allx[,!cat_cols] <- scale(args$allx[,!cat_cols])       args$cont_scale <- 1     }     else {       args$cat_data <- FALSE       args$mixed_data <- FALSE     }      k.out <- do.call(kbal::kbal, args)     w <- k.out$w      list(w = w, fit.obj = k.out)   }    (Wk <- weightit(treat ~ age + educ + married +                     nodegree + re74, data = lalonde,                   method = kbal.fun, estimand = \"ATT\",                   include.obj = TRUE))   summary(Wk)   bal.tab(Wk, stats = c(\"m\", \"ks\")) } # }"},{"path":"https://ngreifer.github.io/WeightIt/reference/msmdata.html","id":null,"dir":"Reference","previous_headings":"","what":"Simulated data for a 3 time point sequential study — msmdata","title":"Simulated data for a 3 time point sequential study — msmdata","text":"simulated dataset 7500 units covariates treatment measured three times outcome measured end hypothetical observational study examining effect treatment delivered time point adverse event. data generated using simple simulation mechanism. details dataset built, see code data-raw/msmdata.R. dataset provided illustrate features weightitMSM() based realistic data-generating process, used benchmark. simulating realistic data known data-generating mechanism, consider using simcausal package.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/msmdata.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Simulated data for a 3 time point sequential study — msmdata","text":"","code":"msmdata"},{"path":"https://ngreifer.github.io/WeightIt/reference/msmdata.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Simulated data for a 3 time point sequential study — msmdata","text":"data frame 7500 observations following 10 variables. X1_0 count covariate measured baseline X2_0 binary covariate measured baseline A_1 binary indicator treatment status first time point X1_1 count covariate measured first time point (first treatment) X2_1 binary covariate measured first time point (first treatment) A_2 binary indicator treatment status second time point X1_2 count covariate measured second time point (second treatment) X2_2 binary covariate measured first time point (first treatment) A_3 binary indicator treatment status third time point Y_B binary indicator outcome event (e.g., death)","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/msmdata.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Simulated data for a 3 time point sequential study — msmdata","text":"","code":"data(\"msmdata\")"},{"path":"https://ngreifer.github.io/WeightIt/reference/plot.weightit.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot information about the weight estimation process — plot.weightit","title":"Plot information about the weight estimation process — plot.weightit","text":"plot.weightit() plots information weights depending estimated. Currently, weighting using method = \"gbm\" \"optweight\" supported. plot distribution weights, see plot.summary.weightit().","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/plot.weightit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot information about the weight estimation process — plot.weightit","text":"","code":"# S3 method for class 'weightit' plot(x, ...)"},{"path":"https://ngreifer.github.io/WeightIt/reference/plot.weightit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot information about the weight estimation process — plot.weightit","text":"x weightit object; output call weightit(). ... Unused.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/plot.weightit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot information about the weight estimation process — plot.weightit","text":"ggplot object.","code":""},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/reference/plot.weightit.html","id":"method-gbm-","dir":"Reference","previous_headings":"","what":"method = \"gbm\"","title":"Plot information about the weight estimation process — plot.weightit","text":"weighting generalized boosted modeling, plot() displays results tuning process used find optimal number trees (tuning parameter values, modified) used final weights. plot produced number trees x-axis value criterion y axis diamond optimal point. multiple parameters selected tuning, separate line displayed plot combination tuning parameters. used call weightit(), plot faceted variable. See method_gbm information selecting tuning parameters.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/plot.weightit.html","id":"method-optweight-","dir":"Reference","previous_headings":"","what":"method = \"optweight\"","title":"Plot information about the weight estimation process — plot.weightit","text":"estimating stable balancing weights, plot() displays values dual variables balance constraint bar graph. Large values dual variables indicate covariates balance constraint causing increases variability weights, .e., covariates relaxing imbalance tolerance yield greatest gains effective sample size. continuous treatments, dual variables split target (.e., ensuring mean covariate weighting equal unweighted mean) balance (.e., ensuring treatment-covariate correlations larger imbalance tolerance). essentially wrapper optweightplot.optweight. See method_optweight details.","code":""},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/reference/plot.weightit.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot information about the weight estimation process — plot.weightit","text":"","code":"# See example at the corresponding methods page"},{"path":"https://ngreifer.github.io/WeightIt/reference/predict.glm_weightit.html","id":null,"dir":"Reference","previous_headings":"","what":"Predictions for glm_weightit objects — predict.glm_weightit","title":"Predictions for glm_weightit objects — predict.glm_weightit","text":"predict() generates predictions models fit using glm_weightit(), ordinal_weightit(), multinom_weightit(), coxph_weightit(). page details predict() methods using glm_weightit(), ordinal_weightit(), multinom_weightit(). See survival::predict.coxph() predictions fitting Cox proportional hazards models using coxph_weightit().","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/predict.glm_weightit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predictions for glm_weightit objects — predict.glm_weightit","text":"","code":"# S3 method for class 'glm_weightit' predict(object, newdata = NULL, type = \"response\", na.action = na.pass, ...)  # S3 method for class 'ordinal_weightit' predict(   object,   newdata = NULL,   type = \"response\",   na.action = na.pass,   values = NULL,   ... )  # S3 method for class 'multinom_weightit' predict(   object,   newdata = NULL,   type = \"response\",   na.action = na.pass,   values = NULL,   ... )"},{"path":"https://ngreifer.github.io/WeightIt/reference/predict.glm_weightit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predictions for glm_weightit objects — predict.glm_weightit","text":"object glm_weightit object. newdata optionally, data frame look variables predict. omitted, fitted values applied original dataset used. type type prediction desired. Allowable options include \"response\", predictions scale original response variable (also \"probs\"); \"link\", predictions scale linear predictor (also \"lp\"); \"class\", modal predicted category ordinal multinomial models; \"mean\", expected value outcome ordinal multinomial models. See Details information. default \"response\" models, differs stats::predict.glm(). na.action function determining done missing values newdata. default predict NA. ... arguments passed methods. values type = \"mean\", numeric values level corresponds . supplied named vector outcome levels names. NULL outcome levels can converted numeric, used. See Details.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/predict.glm_weightit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predictions for glm_weightit objects — predict.glm_weightit","text":"numeric vector containing desired predictions, except following circumstances ordinal multinomial model fit: type = \"response\", numeric matrix row unit column level outcome predicted probability corresponding outcome cells type = \"class\", factor model predicted class unit; ordinal models, ordered factor.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/predict.glm_weightit.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Predictions for glm_weightit objects — predict.glm_weightit","text":"generalized linear models ordinal multinomial models, see stats::predict.glm() information predictions computed arguments can specified. Note standard errors computed predictions using predict.glm_weightit(). ordinal multinomial models, setting type = \"mean\" computes expected value outcome unit; corresponds sum values supplied values weighted predicted probability values. values omitted, predict() attempt convert outcome levels numeric values, done, error thrown. values specified named vector, e.g., values = c(one = 1, two = 2, three = 3), \"one\", \"two\", \"three\" original outcome levels 1, 2, 3 numeric values correspond . method makes sense use outcome levels meaningfully correspond numeric values. ordinal models, setting type = \"link\" (also \"lp\") computes linear predictor without including thresholds. can interpreted prediction latent variable underlying ordinal response. used multinomial models.","code":""},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/reference/predict.glm_weightit.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Predictions for glm_weightit objects — predict.glm_weightit","text":"","code":"data(\"lalonde\", package = \"cobalt\")  # Logistic regression model fit1 <- glm_weightit(   re78 > 0 ~ treat * (age + educ + race + married +                         re74 + re75),   data = lalonde, family = binomial, vcov = \"none\")  summary(predict(fit1)) #>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  #>  0.2747  0.6986  0.7868  0.7671  0.8429  1.0000   # G-computation using predicted probabilities p0 <- predict(fit1, type = \"response\",               newdata = transform(lalonde,                                   treat = 0))  p1 <- predict(fit1, type = \"response\",               newdata = transform(lalonde,                                   treat = 1))  mean(p1) - mean(p0) #> [1] 0.0921469  # Multinomial logistic regression model lalonde$re78_3 <- factor(findInterval(lalonde$re78,                                       c(0, 5e3, 1e4)),                          labels = c(\"low\", \"med\", \"high\"))  fit2 <- multinom_weightit(   re78_3 ~ treat * (age + educ + race + married +                       re74 + re75),   data = lalonde, vcov = \"none\")  # Predicted probabilities head(predict(fit2)) #>         low       med      high #> 1 0.4897563 0.1814665 0.3287772 #> 2 0.5027746 0.3133358 0.1838896 #> 3 0.5453691 0.1974158 0.2572151 #> 4 0.5809239 0.2219530 0.1971231 #> 5 0.5964825 0.2995950 0.1039225 #> 6 0.6217877 0.2666334 0.1115788  # Class assignment accuracy mean(predict(fit2, type = \"class\") == lalonde$re78_3) #> [1] 0.5635179  # G-computation using expected value of the outcome values <- c(\"low\" = 2500,             \"med\" = 7500,             \"high\" = 12500)  p0 <- predict(fit2, type = \"mean\", values = values,               newdata = transform(lalonde,                                   treat = 0))  p1 <- predict(fit2, type = \"mean\", values = values,               newdata = transform(lalonde,                                   treat = 1))  mean(p1) - mean(p0) #> [1] 677.4256 # \\donttest{ # Ordinal logistic regression fit3 <- ordinal_weightit(   re78 ~ treat * (age + educ + race + married +                     re74 + re75),   data = lalonde, vcov = \"none\")  # G-computation using expected value of the outcome; # using original outcome values p0 <- predict(fit3, type = \"mean\",               newdata = transform(lalonde,                                   treat = 0))  p1 <- predict(fit3, type = \"mean\",               newdata = transform(lalonde,                                   treat = 1))  mean(p1) - mean(p0) #> [1] 943.9958 # }"},{"path":"https://ngreifer.github.io/WeightIt/reference/sbps.html","id":null,"dir":"Reference","previous_headings":"","what":"Subgroup Balancing Propensity Score — sbps","title":"Subgroup Balancing Propensity Score — sbps","text":"Implements subgroup balancing propensity score (SBPS), algorithm attempts achieve balance subgroups sharing information overall sample subgroups (Dong, Zhang, Zeng, & Li, 2020; DZZL). subgroup can use either weights estimated using whole sample, weights estimated using just subgroup, combination two. optimal combination chosen minimizes imbalance criterion includes subgroup well overall balance.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/sbps.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Subgroup Balancing Propensity Score — sbps","text":"","code":"sbps(   obj,   obj2 = NULL,   moderator = NULL,   formula = NULL,   data = NULL,   smooth = FALSE,   full.search )"},{"path":"https://ngreifer.github.io/WeightIt/reference/sbps.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Subgroup Balancing Propensity Score — sbps","text":"obj weightit object containing weights estimated overall sample. obj2 weightit object containing weights estimated subgroups. Typically estimated including call weightit(). Either obj2 moderator must specified. moderator optional; string containing name variable data weighting done within subgroups one-sided formula subgrouping variable right-hand side. argument analogous argument weightit(), fact passed . Either obj2 moderator must specified. formula optional formula covariates balance optimized. specified, formula obj$call used. data optional data set form data frame contains variables formula moderator. smooth logical; whether smooth version SBPS used. compatible weightit methods return propensity score. full.search logical; smooth = FALSE, whether every combination subgroup overall weights evaluated. FALSE, stochastic search described DZZL used instead. TRUE, \\(2^R\\) combinations checked, \\(R\\) number subgroups, can take long time many subgroups. unspecified, default TRUE \\(R <= 8\\) FALSE otherwise.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/sbps.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Subgroup Balancing Propensity Score — sbps","text":"weightit.sbps object, inherits weightit. contains information obj weights, propensity scores, call, possibly covariates updated sbps(). addition, prop.subgroup component contains values coefficients C subgroups (either 0 1 standard SBPS), moderator component contains data.frame moderator. object summary method compatible cobalt functions. cluster argument used cobalt functions accurately reflect performance weights balancing subgroups.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/sbps.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Subgroup Balancing Propensity Score — sbps","text":"SBPS relies two sets weights: one estimated overall sample one estimated within subgroup. algorithm decides whether subgroup use weights estimated overall sample estimated subgroup. 2^R permutations overall subgroup weights, R number subgroups. optimal permutation chosen minimizes balance criterion described DZZL. balance criterion used , binary multi-category treatments, sum squared standardized mean differences within subgroups overall, computed using cobalt::col_w_smd(), continuous treatments, sum squared correlations covariate treatment within subgroups overall, computed using cobalt::col_w_corr(). smooth version estimates weights determine relative contribution overall subgroup propensity scores weighted average propensity score subgroup. P_O propensity scores estimated overall sample P_S propensity scores estimated subgroup, smooth SBPS finds R coefficients C subgroup, ultimate propensity score \\(C*P_S + (1-C)*P_O\\), weights computed propensity score. coefficients estimated using optim() method = \"L-BFGS-B\". C estimated 1 0 subgroup, smooth SBPS coincides standard SBPS. obj2 specified moderator , sbps() attempt refit model specified obj moderator argument. relies environment obj created intact can take time obj hard fit. safer estimate obj obj2 (latter simply including moderator argument) supply sbps().","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/sbps.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Subgroup Balancing Propensity Score — sbps","text":"Dong, J., Zhang, J. L., Zeng, S., & Li, F. (2020). Subgroup balancing propensity score. Statistical Methods Medical Research, 29(3), 659–676. doi:10.1177/0962280219870836","code":""},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/reference/sbps.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Subgroup Balancing Propensity Score — sbps","text":"","code":"library(\"cobalt\") data(\"lalonde\", package = \"cobalt\")  #Balancing covariates between treatment groups within races (W1 <- weightit(treat ~ age + educ + married +                 nodegree + race + re74, data = lalonde,                 method = \"glm\", estimand = \"ATT\")) #> A weightit object #>  - method: \"glm\" (propensity score weighting with GLM) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 2-category #>  - estimand: ATT (focal: 1) #>  - covariates: age, educ, married, nodegree, race, re74  (W2 <- weightit(treat ~ age + educ + married +                 nodegree + race + re74, data = lalonde,                 method = \"glm\", estimand = \"ATT\",                 by = \"race\")) #> A weightit object #>  - method: \"glm\" (propensity score weighting with GLM) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 2-category #>  - estimand: ATT (focal: 1) #>  - covariates: age, educ, married, nodegree, race, re74 #>  - by: race S <- sbps(W1, W2) print(S) #> A weightit.sbps object #>  - method: \"glm\" (propensity score weighting with GLM) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 2-category #>  - estimand: ATT (focal: 1) #>  - covariates: age, educ, married, nodegree, race, re74 #>  - moderator: race (3 subgroups) summary(S) #> Summary of weights: #>  #>  - Overall vs. subgroup proportion contribution: #>          race = black race = hispan race = white #> Overall             0             0            0 #> Subgroup            1             1            1 #>  #>  - - - - - - - Subgroup race = black - - - - - - - #> - Weight ranges: #>           Min                                  Max #> treated 1.000      ||                       1.0000 #> control 0.466 |---------------------------| 3.5903 #>  #> - Units with 5 greatest weights by group: #>                                            #>               1      2     3      4      5 #>  treated      1      1     1      1      1 #>             221    228   188    185    174 #>  control 2.9494 2.9494 3.006 3.0637 3.5903 #>  #>          Ratio Coef of Var #> treated 1.0000      0.0000 #> control 7.7042      0.4250 #> overall 7.7042      0.4616 #>  #> - Effective Sample Sizes: #>            Control Treated #> Unweighted  87.000     156 #> Weighted    73.818     156 #>  #>  - - - - - - - Subgroup race = hispan - - - - - - - #> - Weight ranges: #>            Min                                    Max #> treated 1.0000                              || 1.0000 #> control 0.0209   |------------|                0.5046 #>  #> - Units with 5 greatest weights by group: #>                                             #>               1      2      3      4      5 #>  treated      1      1      1      1      1 #>              56     54     49     48     47 #>  control 0.4117 0.4767 0.4835 0.4968 0.5046 #>  #>           Ratio Coef of Var #> treated  1.0000      0.0000 #> control 24.1741      0.7143 #> overall 47.9120      1.0352 #>  #> - Effective Sample Sizes: #>            Control Treated #> Unweighted  61.000      11 #> Weighted    40.616      11 #>  #>  - - - - - - - Subgroup race = white - - - - - - - #> - Weight ranges: #>            Min                                   Max #> treated 1.0000                              || 1.000 #> control 0.0002   |---------|                   0.385 #>  #> - Units with 5 greatest weights by group: #>                                            #>               1      2      3      4     5 #>  treated      1      1      1      1     1 #>             289    287    285    280   267 #>  control 0.2393 0.2699 0.2937 0.2956 0.385 #>  #>            Ratio Coef of Var #> treated    1.000      0.0000 #> control 1825.568      1.1538 #> overall 4742.156      1.9499 #>  #> - Effective Sample Sizes: #>            Control Treated #> Unweighted 281.000      18 #> Weighted   120.777      18 bal.tab(S, cluster = \"race\") #> Balance by cluster #>  #>  - - - Cluster: black - - -  #> Balance Measures #>                Type Diff.Adj #> prop.score Distance   0.0016 #> age         Contin.   0.0126 #> educ        Contin.  -0.0332 #> married      Binary   0.0030 #> nodegree     Binary   0.0062 #> re74        Contin.  -0.0826 #>  #> Effective sample sizes #>                0   1 #> Unadjusted 87.   156 #> Adjusted   73.82 156 #>  #>  - - - Cluster: hispan - - -  #> Balance Measures #>                Type Diff.Adj #> prop.score Distance  -0.2678 #> age         Contin.   0.1196 #> educ        Contin.  -0.0756 #> married      Binary   0.0217 #> nodegree     Binary   0.0018 #> re74        Contin.   0.0114 #>  #> Effective sample sizes #>                0  1 #> Unadjusted 61.   11 #> Adjusted   40.62 11 #>  #>  - - - Cluster: white - - -  #> Balance Measures #>                Type Diff.Adj #> prop.score Distance   0.0652 #> age         Contin.   0.0191 #> educ        Contin.  -0.0185 #> married      Binary  -0.0015 #> nodegree     Binary   0.0039 #> re74        Contin.  -0.0117 #>  #> Effective sample sizes #>                 0  1 #> Unadjusted 281.   18 #> Adjusted   120.78 18 #>  - - - - - - - - - - - - - -  #>   #Could also have run #  sbps(W1, moderator = \"race\")  S_ <- sbps(W1, W2, smooth = TRUE) print(S_) #> A weightit.sbps object #>  - method: \"glm\" (propensity score weighting with GLM) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 2-category #>  - estimand: ATT (focal: 1) #>  - covariates: age, educ, married, nodegree, race, re74 #>  - moderator: race (3 subgroups) summary(S_) #> Summary of weights: #>  #>  - Overall vs. subgroup proportion contribution: #>          race = black race = hispan race = white #> Overall          0.17          0.25            0 #> Subgroup         0.83          0.75            1 #>  #>  - - - - - - - Subgroup race = black - - - - - - - #> - Weight ranges: #>            Min                                  Max #> treated 1.0000      ||                       1.0000 #> control 0.4654 |---------------------------| 3.5703 #>  #> - Units with 5 greatest weights by group: #>                                             #>               1      2      3      4      5 #>  treated      1      1      1      1      1 #>             221    228    188    185    174 #>  control 2.9787 2.9787 3.0338 3.0899 3.5703 #>  #>          Ratio Coef of Var #> treated 1.0000      0.0000 #> control 7.6708      0.4264 #> overall 7.6708      0.4625 #>  #> - Effective Sample Sizes: #>            Control Treated #> Unweighted  87.000     156 #> Weighted    73.744     156 #>  #>  - - - - - - - Subgroup race = hispan - - - - - - - #> - Weight ranges: #>            Min                                    Max #> treated 1.0000                              || 1.0000 #> control 0.0254   |-----------|                 0.4743 #>  #> - Units with 5 greatest weights by group: #>                                             #>               1      2      3      4      5 #>  treated      1      1      1      1      1 #>              56     54     48     47     28 #>  control 0.3908 0.4496 0.4557 0.4704 0.4743 #>  #>           Ratio Coef of Var #> treated  1.0000      0.0000 #> control 18.6516      0.6795 #> overall 39.3245      1.0314 #>  #> - Effective Sample Sizes: #>            Control Treated #> Unweighted   61.00      11 #> Weighted     41.95      11 #>  #>  - - - - - - - Subgroup race = white - - - - - - - #> - Weight ranges: #>            Min                                   Max #> treated 1.0000                              || 1.000 #> control 0.0002   |---------|                   0.385 #>  #> - Units with 5 greatest weights by group: #>                                            #>               1      2      3      4     5 #>  treated      1      1      1      1     1 #>             289    287    285    280   267 #>  control 0.2393 0.2699 0.2937 0.2956 0.385 #>  #>            Ratio Coef of Var #> treated    1.000      0.0000 #> control 1825.568      1.1538 #> overall 4742.156      1.9499 #>  #> - Effective Sample Sizes: #>            Control Treated #> Unweighted 281.000      18 #> Weighted   120.777      18 bal.tab(S_, cluster = \"race\") #> Balance by cluster #>  #>  - - - Cluster: black - - -  #> Balance Measures #>                Type Diff.Adj #> prop.score Distance   0.0019 #> age         Contin.   0.0388 #> educ        Contin.  -0.0305 #> married      Binary   0.0096 #> nodegree     Binary   0.0086 #> re74        Contin.  -0.0561 #>  #> Effective sample sizes #>                0   1 #> Unadjusted 87.   156 #> Adjusted   73.74 156 #>  #>  - - - Cluster: hispan - - -  #> Balance Measures #>                Type Diff.Adj #> prop.score Distance  -0.1909 #> age         Contin.   0.0167 #> educ        Contin.  -0.0654 #> married      Binary   0.0314 #> nodegree     Binary   0.0084 #> re74        Contin.  -0.0175 #>  #> Effective sample sizes #>                0  1 #> Unadjusted 61.   11 #> Adjusted   41.95 11 #>  #>  - - - Cluster: white - - -  #> Balance Measures #>                Type Diff.Adj #> prop.score Distance   0.0652 #> age         Contin.   0.0191 #> educ        Contin.  -0.0185 #> married      Binary  -0.0015 #> nodegree     Binary   0.0039 #> re74        Contin.  -0.0117 #>  #> Effective sample sizes #>                 0  1 #> Unadjusted 281.   18 #> Adjusted   120.78 18 #>  - - - - - - - - - - - - - -  #>"},{"path":"https://ngreifer.github.io/WeightIt/reference/summary.weightit.html","id":null,"dir":"Reference","previous_headings":"","what":"Print and Summarize Output — summary.weightit","title":"Print and Summarize Output — summary.weightit","text":"summary() generates summary weightit weightitMSM object evaluate properties estimated weights. plot() plots distribution weights. nobs() extracts number observations.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/summary.weightit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print and Summarize Output — summary.weightit","text":"","code":"# S3 method for class 'weightit' summary(object, top = 5L, ignore.s.weights = FALSE, ...)  # S3 method for class 'summary.weightit' plot(x, binwidth = NULL, bins = NULL, ...)  # S3 method for class 'weightitMSM' summary(object, top = 5L, ignore.s.weights = FALSE, ...)  # S3 method for class 'summary.weightitMSM' plot(x, binwidth = NULL, bins = NULL, time = 1, ...)"},{"path":"https://ngreifer.github.io/WeightIt/reference/summary.weightit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print and Summarize Output — summary.weightit","text":"object weightit weightitMSM object; output call weightit() weightitMSM(). top many largest smallest weights display. Default 5. ignore.s.weights whether ignore sampling weights computing weight summary. FALSE, default, estimated weights multiplied sampling weights () values computed. ... plot(), additional arguments passed graphics::hist() determine number bins, though ggplot2::geom_histogram() actually used create plot. x summary.weightit summary.weightitMSM object; output call summary.weightit() summary.weightitMSM(). binwidth, bins arguments passed ggplot2::geom_histogram() control size /number bins. time numeric; time point display distribution weights. Default plot distribution first time points.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/summary.weightit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Print and Summarize Output — summary.weightit","text":"point treatments (.e., weightit objects), summary() returns summary.weightit object following elements: weight.range range (minimum maximum) weight treatment group. weight.top units greatest weights treatment group; many included determined top. coef..var (Coef Var) coefficient variation (standard deviation divided mean) weights treatment group overall. scaled.mad (MAD) mean absolute deviation weights treatment group overall divided mean weights corresponding group. negative entropy (Entropy) negative entropy (\\(\\sum w log(w)\\)) weights treatment group overall divided mean weights corresponding group. num.zeros number weights equal zero. effective.sample.size effective sample size treatment group weighting. See ESS(). longitudinal treatments (.e., weightitMSM objects), summary() returns list elements treatment period. plot() returns ggplot object histogram displaying distribution estimated weights. estimand ATT ATC, weights non-focal group(s) displayed (since weights focal group 1). dotted line displayed mean weights. nobs() returns single number. Note even units weights s.weights 0 included.","code":""},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/reference/summary.weightit.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Print and Summarize Output — summary.weightit","text":"","code":"# See example at ?weightit or ?weightitMSM"},{"path":"https://ngreifer.github.io/WeightIt/reference/trim.html","id":null,"dir":"Reference","previous_headings":"","what":"Trim (Winsorize) Large Weights — trim","title":"Trim (Winsorize) Large Weights — trim","text":"Trims (.e., winsorizes) large weights setting weights higher given quantile weight quantile 0. can useful controlling extreme weights, can reduce effective sample size enlarging variability weights. Note default, observations fully discarded using trim(), may differ uses word \"trim\" (see drop argument ).","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/trim.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Trim (Winsorize) Large Weights — trim","text":"","code":"trim(x, ...)  # S3 method for class 'weightit' trim(x, at = 0, lower = FALSE, drop = FALSE, ...)  # Default S3 method trim(x, at = 0, lower = FALSE, treat = NULL, drop = FALSE, ...)"},{"path":"https://ngreifer.github.io/WeightIt/reference/trim.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Trim (Winsorize) Large Weights — trim","text":"x weightit object vector weights. ... used. numeric; either quantile weights weights trimmed. single number .5 1, number weights trimmed (e.g., = 3 top 3 weights set 4th largest weight). lower logical; whether also trim lower quantile (e.g., = .9, trimming .1 .9, = 3, trimming top bottom 3 weights). Default FALSE trim higher weights. drop logical; whether set weights trimmed units 0 . Default FALSE retain trimmed units. Setting TRUE may change original targeted estimand ATT ATC. treat vector treatment status unit. always included x numeric, can get away leaving treatment continuous estimand ATE binary multi-category treatments.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/trim.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Trim (Winsorize) Large Weights — trim","text":"input weightit object, output weightit object weights replaced trimmed weights (0) additional attribute, \"trim\", equal quantile trimming. input numeric vector weights, output numeric vector trimmed weights, aforementioned attribute.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/trim.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Trim (Winsorize) Large Weights — trim","text":"trim() takes weightit object (output call weightit() weightitMSM()) numeric vector weights trims (winsorizes) specified quantile. weights quantile set weight quantile unless drop = TRUE, case set 0. lower = TRUE, weights 1 minus quantile trimmed. general, trimming weights can decrease balance also decreases variability weights, improving precision potential expense unbiasedness (Cole & Hernán, 2008). See Lee, Lessler, Stuart (2011) Thoemmes Ong (2015) discussions simulation results trimming weights various quantiles. Note trimming weights can also change target population therefore estimand. using trim() numeric vector weights, helpful include treatment vector well. helps determine type treatment estimand, used specify trimming performed. particular, estimand determined ATT ATC, weights target (.e., focal) group ignored, since equal 1. Otherwise, estimand ATE treatment continuous, weights considered trimming. general, weights group weights considered trimming.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/trim.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Trim (Winsorize) Large Weights — trim","text":"Cole, S. R., & Hernán, M. Á. (2008). Constructing Inverse Probability Weights Marginal Structural Models. American Journal Epidemiology, 168(6), 656–664. Lee, B. K., Lessler, J., & Stuart, E. . (2011). Weight Trimming Propensity Score Weighting. PLoS ONE, 6(3), e18174. Thoemmes, F., & Ong, . D. (2016). Primer Inverse Probability Treatment Weighting Marginal Structural Models. Emerging Adulthood, 4(1), 40–59.","code":""},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/reference/trim.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Trim (Winsorize) Large Weights — trim","text":"","code":"library(\"cobalt\") data(\"lalonde\", package = \"cobalt\")  (W <- weightit(treat ~ age + educ + married +                  nodegree + re74, data = lalonde,                method = \"glm\", estimand = \"ATT\")) #> A weightit object #>  - method: \"glm\" (propensity score weighting with GLM) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 2-category #>  - estimand: ATT (focal: 1) #>  - covariates: age, educ, married, nodegree, re74 summary(W) #>                   Summary of weights #>  #> - Weight ranges: #>  #>            Min                                  Max #> treated 1.0000               ||              1.0000 #> control 0.0222 |---------------------------| 2.0438 #>  #> - Units with the 5 most extreme weights by group: #>                                             #>               5      4      3      2      1 #>  treated      1      1      1      1      1 #>             411    595    269    409    296 #>  control 1.3303 1.4365 1.5005 1.6369 2.0438 #>  #> - Weight statistics: #>  #>         Coef of Var   MAD Entropy # Zeros #> treated       0.000 0.000    0.00       0 #> control       0.823 0.701    0.33       0 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted  429.       185 #> Weighted    255.99     185  #Trimming the top and bottom 5 weights trim(W, at = 5, lower = TRUE) #> Trimming the top and bottom 5 weights where treat is not 1. #> A weightit object #>  - method: \"glm\" (propensity score weighting with GLM) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 2-category #>  - estimand: ATT (focal: 1) #>  - covariates: age, educ, married, nodegree, re74 #>  - weights trimmed at the top and bottom 5  #Trimming at 90th percentile (W.trim <- trim(W, at = .9)) #> Trimming weights where treat is not 1 to 90%. #> A weightit object #>  - method: \"glm\" (propensity score weighting with GLM) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 2-category #>  - estimand: ATT (focal: 1) #>  - covariates: age, educ, married, nodegree, re74 #>  - weights trimmed at 90%  summary(W.trim) #>                   Summary of weights #>  #> - Weight ranges: #>  #>            Min                                    Max #> treated 1.0000                              || 1.0000 #> control 0.0222   |-------------------------|   0.9407 #>  #> - Units with the 5 most extreme weights by group: #>                                             #>               5      4      3      2      1 #>  treated      1      1      1      1      1 #>             303    296    285    269    264 #>  control 0.9407 0.9407 0.9407 0.9407 0.9407 #>  #> - Weight statistics: #>  #>         Coef of Var   MAD Entropy # Zeros #> treated       0.000 0.000   0.000       0 #> control       0.766 0.682   0.303       0 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted  429.       185 #> Weighted    270.58     185 #Note that only the control weights were trimmed  #Trimming a numeric vector of weights all.equal(trim(W$weights, at = .9, treat = lalonde$treat),           W.trim$weights) #> Trimming weights where treat is not 1 to 90%. #> [1] TRUE  #Dropping trimmed units (W.trim <- trim(W, at = .9, drop = TRUE)) #> Setting weights beyond 90% where treat is not 1 to 0. #> A weightit object #>  - method: \"glm\" (propensity score weighting with GLM) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 2-category #>  - estimand: ATT (focal: 1) #>  - covariates: age, educ, married, nodegree, re74 #>  - weights trimmed at 90%  summary(W.trim) #>                   Summary of weights #>  #> - Weight ranges: #>  #>            Min                                    Max #> treated 1.0000                              || 1.0000 #> control 0.0222   |-------------------------|   0.9407 #>  #> - Units with the 5 most extreme weights by group: #>                                             #>               5      4      3      2      1 #>  treated      1      1      1      1      1 #>             467    466    373    369    356 #>  control 0.9407 0.9407 0.9407 0.9407 0.9407 #>  #> - Weight statistics: #>  #>         Coef of Var   MAD Entropy # Zeros #> treated       0.000 0.000   0.000       0 #> control       0.881 0.757   0.303      40 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted  429.       185 #> Weighted    241.72     185 #Note that we now have zeros in the control group  #Using made up data and as.weightit() treat <- rbinom(500, 1, .3) weights <- rchisq(500, df = 2) W <- as.weightit(weights, treat = treat,                  estimand = \"ATE\") summary(W) #>                   Summary of weights #>  #> - Weight ranges: #>  #>            Min                                   Max #> treated 0.0141 |-------------------------|   10.6020 #> control 0.0069 |---------------------------| 11.2619 #>  #> - Units with the 5 most extreme weights by group: #>                                              #>             238    374    324     24     275 #>  treated 7.3089 7.6362 7.9896 9.2475  10.602 #>             171    299     75    154     346 #>  control 8.5277 8.5762 8.6373 9.8308 11.2619 #>  #> - Weight statistics: #>  #>         Coef of Var  MAD Entropy # Zeros #> treated       0.957 0.74   0.396       0 #> control       0.924 0.71   0.381       0 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted  345.    155.   #> Weighted    186.44   81.16 summary(trim(W, at = .95)) #> Trimming weights to 95%. #>                   Summary of weights #>  #> - Weight ranges: #>  #>            Min                                  Max #> treated 0.0141 |---------------------------| 6.1429 #> control 0.0069 |---------------------------| 6.1429 #>  #> - Units with the 5 most extreme weights by group: #>                                             #>             189    177     88     24      5 #>  treated 6.1429 6.1429 6.1429 6.1429 6.1429 #>             154    136    135     95     75 #>  control 6.1429 6.1429 6.1429 6.1429 6.1429 #>  #> - Weight statistics: #>  #>         Coef of Var   MAD Entropy # Zeros #> treated       0.875 0.714   0.357       0 #> control       0.844 0.690   0.346       0 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted  345.    155.   #> Weighted    201.82   88.05"},{"path":"https://ngreifer.github.io/WeightIt/reference/weightit.fit.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Balancing Weights with Minimal Input Processing — weightit.fit","title":"Generate Balancing Weights with Minimal Input Processing — weightit.fit","text":"weightit.fit() dispatches one weight estimation methods determined method. internal function called weightit() probably used except special cases. Unlike weightit(), weightit.fit() accept formula data frame interface instead requires covariates treatment supplied numeric matrix atomic vector, respectively. way, weightit.fit() weightit() lm.fit() lm() - thinner, slightly faster interface performs minimal argument checking.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/weightit.fit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Balancing Weights with Minimal Input Processing — weightit.fit","text":"","code":"weightit.fit(   covs,   treat,   method = \"glm\",   s.weights = NULL,   by.factor = NULL,   estimand = \"ATE\",   focal = NULL,   stabilize = FALSE,   ps = NULL,   moments = NULL,   int = FALSE,   subclass = NULL,   missing = NULL,   verbose = FALSE,   include.obj = FALSE,   ... )"},{"path":"https://ngreifer.github.io/WeightIt/reference/weightit.fit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Balancing Weights with Minimal Input Processing — weightit.fit","text":"covs numeric matrix covariates. treat vector treatment statuses. method string containing name method used estimate weights. See weightit() allowable options. default \"glm\" propensity score weighting using generalized linear model estimate propensity score. s.weights numeric vector sampling weights. See individual pages method information whether sampling weights can supplied. .factor factor variable weighting done within levels. Corresponds argument weightit(). estimand desired estimand. binary multi-category treatments, can \"ATE\", \"ATT\", \"ATC\", , methods, \"ATO\", \"ATM\", \"ATOS\". default \"ATE\". argument ignored continuous treatments. See individual pages method information estimands allowed method literature read interpret estimands. focal estimand set \"ATT\" \"ATC\", group consider \"treated\" \"control\" group. group weighted, groups weighted resemble focal group. specified, estimand automatically set \"ATT\" (warning estimand \"ATT\" \"ATC\"). See section estimand focal Details weightit(). stabilize logical; whether stabilize weights. methods involve estimating propensity scores, involves multiplying unit's weight proportion units treatment group. Default FALSE. Note differs use weightit(). ps vector propensity scores. specified, method ignored set \"glm\". moments, int, subclass arguments customize weight estimation. See weightit() details. missing character; missing data handled. options depend method used. NULL, covs checked NA values, present, missing set \"ind\". \"\", covs checked NA values; can faster known none. verbose logical; whether print additional information output fitting function. include.obj logical; whether include output fit objects created process estimating weights. example, method = \"glm\", glm objects containing propensity score model included. See individual pages method information object included TRUE. ... arguments functions called weightit.fit() control aspects fitting covered arguments.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/weightit.fit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Balancing Weights with Minimal Input Processing — weightit.fit","text":"weightit.fit object following elements: weights estimated weights, one unit. treat values treatment variable. estimand estimand requested. method weight estimation method specified. ps estimated provided propensity scores. Estimated propensity scores returned binary treatments method \"glm\", \"gbm\", \"cbps\", \"ipt\", \"super\", \"bart\". propensity score corresponds predicted probability treated; see section estimand focal Details weightit() treated group determined. s.weights provided sampling weights. focal focal treatment level ATT ATC requested. fit.obj include.obj = TRUE, fit object. info Additional information fitting. See individual methods pages included. weightit.fit object specialized print(), summary(), plot() methods. simply list containing components. Use .weightit() convert weightit object, methods. See Examples.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/weightit.fit.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Generate Balancing Weights with Minimal Input Processing — weightit.fit","text":"weightit.fit() called weightit() arguments weightit() checked processed. weightit.fit() dispatches function used actually estimate weights, passing supplied arguments directly. weightit.fit() meant used anyone experienced users specific use case mind. returned object contains limited information supplied arguments details estimation method; processed weightit(). Less argument checking processing occurs weightit.fit() weightit(), means supplying incorrect arguments can result errors, crashes, invalid weights, error warning messages may helpful diagnosing problem. weightit.fit() check make sure weights actually estimated, though. weightit.fit() may useful speeding simulation simulation studies use weightit() covariates can supplied numeric matrix, often generated simulations, without go potentially slow process extracting covariates treatment formula data frame. user certain arguments valid (e.g., ensuring estimated weights consistent estimated weightit() arguments), less time needs spent processing arguments. Also, returned object much smaller weightit object covariates returned alongside weights.","code":""},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/reference/weightit.fit.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate Balancing Weights with Minimal Input Processing — weightit.fit","text":"","code":"library(\"cobalt\") data(\"lalonde\", package = \"cobalt\")  # Balancing covariates between treatment groups (binary) covs <- lalonde[c(\"age\", \"educ\", \"race\", \"married\",                   \"nodegree\", \"re74\", \"re75\")] ## Create covs matrix, splitting any factors using ## cobalt::splitfactor() covs_mat <- as.matrix(splitfactor(covs))  WF1 <- weightit.fit(covs_mat, treat = lalonde$treat,                     method = \"glm\", estimand = \"ATT\") str(WF1) #> List of 10 #>  $ weights  : num [1:614] 1 1 1 1 1 1 1 1 1 1 ... #>  $ treat    : int [1:614] 1 1 1 1 1 1 1 1 1 1 ... #>   ..- attr(*, \"treat.type\")= chr \"binary\" #>   ..- attr(*, \"treated\")= int 1 #>  $ estimand : chr \"ATT\" #>  $ method   : chr \"glm\" #>  $ ps       : num [1:614] 0.639 0.225 0.678 0.776 0.702 ... #>  $ s.weights: num [1:614] 1 1 1 1 1 1 1 1 1 1 ... #>  $ focal    : int 1 #>  $ missing  : chr \"\" #>  $ fit.obj  : NULL #>  $ info     : Named list() #>  - attr(*, \"Mparts\")=List of 6 #>   ..$ psi_treat :function (Btreat, Xtreat, A, SW)   #>   ..$ wfun      :function (Btreat, Xtreat, A)   #>   ..$ dw_dBtreat:function (Btreat, Xtreat, A, SW)   #>   ..$ Xtreat    : num [1:614, 1:9] 1 1 1 1 1 1 1 1 1 1 ... #>   .. ..- attr(*, \"dimnames\")=List of 2 #>   .. .. ..$ : chr [1:614] \"1\" \"2\" \"3\" \"4\" ... #>   .. .. ..$ : chr [1:9] \"(Intercept)\" \"age\" \"educ\" \"race_hispan\" ... #>   ..$ A         : int [1:614] 1 1 1 1 1 1 1 1 1 1 ... #>   ..$ btreat    : Named num [1:9] 0.214 0.156 0.424 -2.082 -3.065 ... #>   .. ..- attr(*, \"names\")= chr [1:9] \"(Intercept)\" \"age\" \"educ\" \"race_hispan\" ... #>  - attr(*, \"class\")= chr \"weightit.fit\"  # Converting to a weightit object for use with # summary() and bal.tab() W1 <- as.weightit(WF1, covs = covs) W1 #> A weightit object #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 2-category #>  - estimand: ATT (focal: 1) #>  - covariates: age, educ, race, married, nodegree, re74, re75 summary(W1) #>                   Summary of weights #>  #> - Weight ranges: #>  #>            Min                                  Max #> treated 1.0000         ||                    1.0000 #> control 0.0092 |---------------------------| 3.7432 #>  #> - Units with the 5 most extreme weights by group: #>                                             #>               5      4      3      2      1 #>  treated      1      1      1      1      1 #>             597    573    381    411    303 #>  control 3.0301 3.0592 3.2397 3.5231 3.7432 #>  #> - Weight statistics: #>  #>         Coef of Var   MAD Entropy # Zeros #> treated       0.000 0.000   0.000       0 #> control       1.818 1.289   1.098       0 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted  429.       185 #> Weighted     99.82     185 bal.tab(W1) #> Balance Measures #>                 Type Diff.Adj #> prop.score  Distance  -0.0205 #> age          Contin.   0.1188 #> educ         Contin.  -0.0284 #> race_black    Binary  -0.0022 #> race_hispan   Binary   0.0002 #> race_white    Binary   0.0021 #> married       Binary   0.0186 #> nodegree      Binary   0.0184 #> re74         Contin.  -0.0021 #> re75         Contin.   0.0110 #>  #> Effective sample sizes #>            Control Treated #> Unadjusted  429.       185 #> Adjusted     99.82     185"},{"path":"https://ngreifer.github.io/WeightIt/reference/weightit.html","id":null,"dir":"Reference","previous_headings":"","what":"Estimate Balancing Weights — weightit","title":"Estimate Balancing Weights — weightit","text":"weightit() allows easy generation balancing weights using variety available methods binary, continuous, multi-category treatments. Many methods exist packages, weightit() calls; packages must installed use desired method.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/weightit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Estimate Balancing Weights — weightit","text":"","code":"weightit(   formula,   data = NULL,   method = \"glm\",   estimand = \"ATE\",   stabilize = FALSE,   focal = NULL,   by = NULL,   s.weights = NULL,   ps = NULL,   moments = NULL,   int = FALSE,   subclass = NULL,   missing = NULL,   verbose = FALSE,   include.obj = FALSE,   keep.mparts = TRUE,   ... )"},{"path":"https://ngreifer.github.io/WeightIt/reference/weightit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Estimate Balancing Weights — weightit","text":"formula formula treatment variable left hand side covariates balanced right hand side. See glm() details. Interactions functions covariates allowed. data optional data set form data frame contains variables formula. method string length 1 containing name method used estimate weights. See Details allowable options. default \"glm\" propensity score weighting using generalized linear model estimate propensity score. estimand desired estimand. binary multi-category treatments, can \"ATE\", \"ATT\", \"ATC\", , methods, \"ATO\", \"ATM\", \"ATOS\". default \"ATE\". argument ignored continuous treatments. See individual pages method information estimands allowed method literature read interpret estimands. stabilize whether stabilize weights. TRUE, unit's weight multiplied standardization factor, unconditional probability (density) unit's observed treatment value. formula, generalized linear model fit included predictors, inverse corresponding weight used standardization factor. Can used continuous treatments estimand = \"ATE\". Default FALSE standardization. See also num.formula argument weightitMSM(). continuous treatments, weights already stabilized, setting stabilize = TRUE ignored warning (supplying formula still works). focal estimand set \"ATT\" \"ATC\", group consider \"treated\" \"control\" group. group weighted, groups weighted resemble focal group. specified, estimand automatically set \"ATT\" (warning estimand \"ATT\" \"ATC\"). See section estimand focal Details . string containing name variable data weighting done within categories one-sided formula stratifying variable right-hand side. example, = \"gender\" = ~gender, separate propensity score model optimization occur within level variable \"gender\". one variable allowed; stratify multiply variables simultaneously, create new variable full cross variables using interaction(). s.weights vector sampling weights name variable data contains sampling weights. can also matching weights weighting used matched data. See individual pages method information whether sampling weights can supplied. ps vector propensity scores name variable data containing propensity scores. NULL, method ignored unless user-supplied function, propensity scores used create weights. formula must include treatment variable data, listed covariates play role weight estimation. Using ps similar calling get_w_from_ps() directly, produces full weightit object rather just producing weights. moments numeric; methods, greatest power covariate balanced. example, moments = 3, non-categorical covariate, covariate, square, cube balanced. argument ignored methods; balance powers covariates, appropriate functions must entered formula. See individual pages method information whether accept moments. int logical; methods, whether first-order interactions covariates balanced. argument ignored methods; balance interactions variables, appropriate functions must entered formula. See individual pages method information whether accept int. subclass numeric; number subclasses use computing weights using marginal mean weighting subclasses (MMWS). NULL, standard inverse probability weights (extensions) computed; number greater 1, subclasses formed weights computed based subclass membership. Attempting set non-NULL value methods compute propensity score result error; see method's help page information whether MMWS weights compatible method. See get_w_from_ps() details references. missing character; missing data handled. options defaults depend method used. Ignored missing data present. noted multiple imputation outperforms available missingness methods available weightit() probably used instead. Consider MatchThem package use weightit() multiply imputed data. verbose logical; whether print additional information output fitting function. include.obj logical; whether include output fit objects created process estimating weights. example, method = \"glm\", glm objects containing propensity score model included. See individual pages method information object included TRUE. keep.mparts logical; whether include output components necessary estimate standard errors account estimation weights glm_weightit(). Default TRUE parts present. See individual pages method whether components produced. Set FALSE keep output object smaller, e.g., standard errors computed using glm_weightit(). ... arguments functions called weightit() control aspects fitting covered arguments. See Details.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/weightit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Estimate Balancing Weights — weightit","text":"weightit object following elements: weights estimated weights, one unit. treat values treatment variable. covs covariates used fitting. includes raw covariates, may altered fitting process. estimand estimand requested. method weight estimation method specified. ps estimated provided propensity scores. Estimated propensity scores returned binary treatments method \"glm\", \"gbm\", \"cbps\", \"ipt\", \"super\", \"bart\". propensity score corresponds predicted probability treated; see section estimand focal Details treated group determined. s.weights provided sampling weights. focal focal treatment level ATT ATC requested. data.frame containing variable specified. obj include.obj = TRUE, fit object. info Additional information fitting. See individual methods pages included. keep.mparts TRUE (default) chosen method compatible M-estimation, components related M-estimation use glm_weightit() stored \"Mparts\" attribute. specified, keep.mparts set FALSE.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/weightit.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Estimate Balancing Weights — weightit","text":"primary purpose weightit() dispatcher functions perform estimation balancing weights using requested method. methods allowed links pages containing information , including additional arguments outputs (e.g., include.obj = TRUE), missing values treated, estimands allowed, whether sampling weights allowed. method can also supplied user-defined function; see method_user instructions examples. Setting method = NULL computes unit weights.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/weightit.html","id":"estimand-and-focal-for-binary-and-multi-category-treatments-the","dir":"Reference","previous_headings":"","what":"estimand and focal For binary and multi-category treatments, the","title":"Estimate Balancing Weights — weightit","text":"argument estimand determines distribution weighted sample resemble. set \"ATE\", requests group resemble full sample. set \"ATO\", \"ATM\", \"ATOS\" (methods allow ), requests group resemble \"overlap\" sample. set \"ATT\" \"ATC\", requests group resemble treated control group, respectively (termed \"focal\" group). Weights set 1 focal group. weightit() decide group treated group control? binary treatments, several heuristics used. first checking whether valid argument focal supplied containing name focal group, treated group estimand = \"ATT\" control group estimand = \"ATC\". focal supplied, guesses made using following criteria, evaluated order: treatment variable logical, TRUE considered treated FALSE control. treatment numeric (string factor values can coerced numeric values), 0 one values, considered control, otherwise, lower value considered control (considered treated). exactly one treatment values \"t\", \"tr\", \"treat\", \"treated\", \"exposed\", considered treated (control). exactly one treatment values \"c\", \"co\", \"ctrl\", \"control\", \"unexposed\", considered control (treated). treatment variable factor, first level considered control second treated. lowest value sorting sort() considered control treated. safe, best code binary treatment variable 0 control 1 treated. Otherwise, focal supplied requesting ATT ATC. multi-category treatments, focal required requesting ATT ATC; none heuristics used.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/weightit.html","id":"citing-weightit-when-using-weightit-please-cite-both-the","dir":"Reference","previous_headings":"","what":"Citing WeightIt When using weightit(), please cite both the","title":"Estimate Balancing Weights — weightit","text":"WeightIt package (using citation(\"WeightIt\")) paper(s) references section method used.","code":""},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/reference/weightit.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Estimate Balancing Weights — weightit","text":"","code":"library(\"cobalt\") data(\"lalonde\", package = \"cobalt\")  #Balancing covariates between treatment groups (binary) (W1 <- weightit(treat ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"glm\", estimand = \"ATT\")) #> A weightit object #>  - method: \"glm\" (propensity score weighting with GLM) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 2-category #>  - estimand: ATT (focal: 1) #>  - covariates: age, educ, married, nodegree, re74 summary(W1) #>                   Summary of weights #>  #> - Weight ranges: #>  #>            Min                                  Max #> treated 1.0000               ||              1.0000 #> control 0.0222 |---------------------------| 2.0438 #>  #> - Units with the 5 most extreme weights by group: #>                                             #>               5      4      3      2      1 #>  treated      1      1      1      1      1 #>             411    595    269    409    296 #>  control 1.3303 1.4365 1.5005 1.6369 2.0438 #>  #> - Weight statistics: #>  #>         Coef of Var   MAD Entropy # Zeros #> treated       0.000 0.000    0.00       0 #> control       0.823 0.701    0.33       0 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted  429.       185 #> Weighted    255.99     185 bal.tab(W1) #> Balance Measures #>                Type Diff.Adj #> prop.score Distance   0.0199 #> age         Contin.   0.0459 #> educ        Contin.  -0.0360 #> married      Binary   0.0044 #> nodegree     Binary   0.0080 #> re74        Contin.  -0.0275 #>  #> Effective sample sizes #>            Control Treated #> Unadjusted  429.       185 #> Adjusted    255.99     185  #Balancing covariates with respect to race (multi-category) (W2 <- weightit(race ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"ebal\", estimand = \"ATE\")) #> A weightit object #>  - method: \"ebal\" (entropy balancing) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 3-category (black, hispan, white) #>  - estimand: ATE #>  - covariates: age, educ, married, nodegree, re74 summary(W2) #>                   Summary of weights #>  #> - Weight ranges: #>  #>           Min                                  Max #> black  0.5530   |-------------------------| 5.3496 #> hispan 0.1408 |----------------|            3.3323 #> white  0.3978  |-------|                    1.9232 #>  #> - Units with the 5 most extreme weights by group: #>                                            #>            226    244    485    181    182 #>   black 2.5215 2.5492 2.8059 3.5551 5.3496 #>            392    564    269    345    371 #>  hispan 2.0464 2.5298 2.6323 2.7049 3.3323 #>             68    457    599    589    531 #>   white 1.7109 1.7226 1.7433 1.7741 1.9232 #>  #> - Weight statistics: #>  #>        Coef of Var   MAD Entropy # Zeros #> black        0.590 0.413   0.131       0 #> hispan       0.609 0.440   0.163       0 #> white        0.371 0.306   0.068       0 #>  #> - Effective Sample Sizes: #>  #>             black hispan  white #> Unweighted 243.    72.   299.   #> Weighted   180.47  52.71 262.93 bal.tab(W2) #> Balance summary across all treatment pairs #>             Type Max.Diff.Adj #> age      Contin.            0 #> educ     Contin.            0 #> married   Binary            0 #> nodegree  Binary            0 #> re74     Contin.            0 #>  #> Effective sample sizes #>             black hispan  white #> Unadjusted 243.    72.   299.   #> Adjusted   180.47  52.71 262.93  #Balancing covariates with respect to re75 (continuous) (W3 <- weightit(re75 ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"cbps\")) #> A weightit object #>  - method: \"cbps\" (covariate balancing propensity score weighting) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: continuous #>  - covariates: age, educ, married, nodegree, re74 summary(W3) #>                   Summary of weights #>  #> - Weight ranges: #>  #>        Min                                   Max #> all 0.0151 |---------------------------| 43.9963 #>  #> - Units with the 5 most extreme weights: #>                                              #>          482     180     481     483     185 #>  all 10.8239 11.0878 11.9703 13.1314 43.9963 #>  #> - Weight statistics: #>  #>     Coef of Var   MAD Entropy # Zeros #> all       1.942 0.528   0.454       0 #>  #> - Effective Sample Sizes: #>  #>             Total #> Unweighted 614.   #> Weighted   128.86 bal.tab(W3) #> Balance Measures #>             Type Corr.Adj #> age      Contin.   0.1773 #> educ     Contin.   0.1071 #> married   Binary   0.4522 #> nodegree  Binary   0.3308 #> re74     Contin.   0.5514 #>  #> Effective sample sizes #>             Total #> Unadjusted 614.   #> Adjusted   128.86"},{"path":"https://ngreifer.github.io/WeightIt/reference/weightitMSM.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Balancing Weights for Longitudinal Treatments — weightitMSM","title":"Generate Balancing Weights for Longitudinal Treatments — weightitMSM","text":"weightitMSM() allows easy generation balancing weights marginal structural models time-varying treatments using variety available methods binary, continuous, multi-category treatments. Many methods exist packages, weightit() calls; packages must installed use desired method.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/weightitMSM.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Balancing Weights for Longitudinal Treatments — weightitMSM","text":"","code":"weightitMSM(   formula.list,   data = NULL,   method = \"glm\",   stabilize = FALSE,   by = NULL,   s.weights = NULL,   num.formula = NULL,   moments = NULL,   int = FALSE,   missing = NULL,   verbose = FALSE,   include.obj = FALSE,   keep.mparts = TRUE,   is.MSM.method,   weightit.force = FALSE,   ... )"},{"path":"https://ngreifer.github.io/WeightIt/reference/weightitMSM.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Balancing Weights for Longitudinal Treatments — weightitMSM","text":"formula.list list formulas corresponding time point time-specific treatment variable left hand side pre-treatment covariates balanced right hand side. formulas must temporal order, must contain covariates balanced time point (.e., treatments covariates featured early formulas appear later ones). Interactions functions covariates allowed. data optional data set form data frame contains variables formulas formula.list. must wide data set exactly one row per unit. method string length 1 containing name method used estimate weights. See weightit() allowable options. default \"glm\", estimates weights using generalized linear models. stabilize logical; whether stabilize weights. Stabilizing weights involves fitting model predicting treatment time point treatment status prior time points. TRUE, fully saturated model fit (.e., interactions treatments time point), essentially using observed treatment probabilities numerator (binary multi-category treatments). may yield error combinations observed. Default FALSE. manually specify stabilization model formulas, e.g., specify non-saturated models, use num.formula. many time points, saturated models may time-consuming impossible fit. string containing name variable data weighting done within categories one-sided formula stratifying variable right-hand side. example, = \"gender\" = ~gender, separate propensity score model optimization occur within level variable \"gender\". one variable allowed; stratify multiply variables simultaneously, create new variable full cross variables using interaction(). s.weights vector sampling weights name variable data contains sampling weights. can also matching weights weighting used matched data. See individual pages method information whether sampling weights can supplied. num.formula optional; one-sided formula stabilization factors (previous treatments) right hand side, adds, time point, stabilization factors model saturated previous treatments. See Cole & Hernán (2008) discussion specify model; including stabilization factors can change estimand without proper adjustment, done caution. Can also list one-sided formulas, one time point. Unless know , recommend setting stabilize = TRUE ignoring num.formula. moments numeric; methods, greatest power covariate balanced. example, moments = 3, non-categorical covariate, covariate, square, cube balanced. argument ignored methods; balance powers covariates, appropriate functions must entered formula. See individual pages method information whether accept moments. int logical; methods, whether first-order interactions covariates balanced. argument ignored methods; balance interactions variables, appropriate functions must entered formula. See individual pages method information whether accept int. missing character; missing data handled. options defaults depend method used. Ignored missing data present. noted multiple imputation outperforms available missingness methods available weightit() probably used instead. Consider MatchThem package use weightit() multiply imputed data. verbose logical; whether print additional information output fitting function. include.obj whether include output list fit objects created process estimating weights time point. example, method = \"glm\", list glm objects containing propensity score models time point included. See help pages method information object included TRUE. keep.mparts logical; whether include output components necessary estimate standard errors account estimation weights glm_weightit(). Default TRUE parts present. See individual pages method whether components produced. Set FALSE keep output object smaller, e.g., standard errors computed using glm_weightit(). .MSM.method whether method estimates weights multiple time points (TRUE) estimating weights time point multiplying together (FALSE). relevant user-specified functions. weightit.force several methods valid estimating weights longitudinal treatments, produce error message attempted. Set TRUE bypass error message. ... arguments functions called weightit() control aspects fitting covered arguments. See Details weightit().","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/weightitMSM.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Balancing Weights for Longitudinal Treatments — weightitMSM","text":"weightitMSM object following elements: weights estimated weights, one unit. treat.list list values time-varying treatment variables. covs.list list covariates used fitting time point. includes raw covariates, may altered fitting process. data data.frame originally entered weightitMSM(). estimand \"ATE\", currently estimand MSMs binary multi-category treatments. method weight estimation method specified. ps.list list estimated propensity scores () time point. s.weights provided sampling weights. data.frame containing variable specified. stabilization stabilization factors, . keep.mparts TRUE (default) chosen method compatible M-estimation, components related M-estimation use glm_weightit() stored \"Mparts.list\" attribute. specified, keep.mparts set FALSE.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/weightitMSM.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Generate Balancing Weights for Longitudinal Treatments — weightitMSM","text":"Currently \"wide\" data sets, row corresponds unit's entire variable history, supported. can use reshape() functions transform data format; see example . general, weightitMSM() works separating estimation weights separate procedures time period based formulas provided. formula, weightitMSM() simply calls weightit() formula, collects weights time period, multiplies together arrive longitudinal balancing weights. formula contain covariates balanced . example, formula corresponding second time period contain baseline covariates, treatment variable first time period, time-varying covariates took values first treatment second. Currently, wide data sets supported, unit represented exactly one row contains covariate treatment history encoded separate variables. \"cbps\" method, calls CBPS() CBPS, yield different results CBMSM() CBPS CBMSM() takes different approach generating weights simply estimating several time-specific models.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/weightitMSM.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Generate Balancing Weights for Longitudinal Treatments — weightitMSM","text":"Cole, S. R., & Hernán, M. . (2008). Constructing Inverse Probability Weights Marginal Structural Models. American Journal Epidemiology, 168(6), 656–664. doi:10.1093/aje/kwn164","code":""},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/reference/weightitMSM.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate Balancing Weights for Longitudinal Treatments — weightitMSM","text":"","code":"library(\"cobalt\")  data(\"msmdata\") (W1 <- weightitMSM(list(A_1 ~ X1_0 + X2_0,                         A_2 ~ X1_1 + X2_1 +                           A_1 + X1_0 + X2_0,                         A_3 ~ X1_2 + X2_2 +                           A_2 + X1_1 + X2_1 +                           A_1 + X1_0 + X2_0),                    data = msmdata,                    method = \"glm\")) #> A weightitMSM object #>  - method: \"glm\" (propensity score weighting with GLM) #>  - number of obs.: 7500 #>  - sampling weights: none #>  - number of time points: 3 (A_1, A_2, A_3) #>  - treatment: #>     + time 1: 2-category #>     + time 2: 2-category #>     + time 3: 2-category #>  - covariates: #>     + baseline: X1_0, X2_0 #>     + after time 1: X1_1, X2_1, A_1, X1_0, X2_0 #>     + after time 2: X1_2, X2_2, A_2, X1_1, X2_1, A_1, X1_0, X2_0 summary(W1) #>                         Time 1                         #>                   Summary of weights #>  #> - Weight ranges: #>  #>            Min                                    Max #> treated 1.0791 |---------------------------| 403.4833 #> control 1.2761 |-------------------|         284.7636 #>  #> - Units with the 5 most extreme weights by group: #>                                                       #>              5488     3440     3593     1286     5685 #>  treated  166.992 170.5549 196.4136 213.1934 403.4833 #>              2594     2932     5226     1875     2533 #>  control 155.6248  168.964 172.4195 245.8822 284.7636 #>  #> - Weight statistics: #>  #>         Coef of Var   MAD Entropy # Zeros #> treated       1.914 0.816   0.649       0 #> control       1.706 0.862   0.670       0 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted 3306.    4194.  #> Weighted    845.79   899.4 #>  #>                         Time 2                         #>                   Summary of weights #>  #> - Weight ranges: #>  #>            Min                                    Max #> treated 1.0791 |---------------------------| 403.4833 #> control 1.2761 |----------------|            245.8822 #>  #> - Units with the 5 most extreme weights by group: #>                                                       #>              2932     3440     3593     2533     5685 #>  treated  168.964 170.5549 196.4136 284.7636 403.4833 #>              2594     5488     5226     1286     1875 #>  control 155.6248  166.992 172.4195 213.1934 245.8822 #>  #> - Weight statistics: #>  #>         Coef of Var   MAD Entropy # Zeros #> treated       1.892 0.819   0.652       0 #> control       1.748 0.869   0.686       0 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted 3701.   3799.   #> Weighted    912.87  829.87 #>  #>                         Time 3                         #>                   Summary of weights #>  #> - Weight ranges: #>  #>            Min                                    Max #> treated 1.0791 |---------------------------| 403.4833 #> control 1.2761 |---------|                   148.1547 #>  #> - Units with the 5 most extreme weights by group: #>                                                       #>              3593     1286     1875     2533     5685 #>  treated 196.4136 213.1934 245.8822 284.7636 403.4833 #>              6862      168     3729     6158     3774 #>  control  88.0721  97.8273  104.623 121.8451 148.1547 #>  #> - Weight statistics: #>  #>         Coef of Var   MAD Entropy # Zeros #> treated       1.832 0.975   0.785       0 #> control       1.254 0.683   0.412       0 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted 4886.   2614.   #> Weighted   1900.26  600.12 #>  bal.tab(W1) #> Balance summary across all time points #>        Times    Type Max.Diff.Adj #> X1_0 1, 2, 3 Contin.       0.0342 #> X2_0 1, 2, 3  Binary       0.0299 #> X1_1    2, 3 Contin.       0.0657 #> X2_1    2, 3  Binary       0.0299 #> A_1     2, 3  Binary       0.0262 #> X1_2       3 Contin.       0.0643 #> X2_2       3  Binary       0.0096 #> A_2        3  Binary       0.0054 #>  #> Effective sample sizes #>  - Time 1 #>            Control Treated #> Unadjusted 3306.    4194.  #> Adjusted    845.79   899.4 #>  - Time 2 #>            Control Treated #> Unadjusted 3701.   3799.   #> Adjusted    912.87  829.87 #>  - Time 3 #>            Control Treated #> Unadjusted 4886.   2614.   #> Adjusted   1900.26  600.12  #Using stabilization factors W2 <- weightitMSM(list(A_1 ~ X1_0 + X2_0,                         A_2 ~ X1_1 + X2_1 +                           A_1 + X1_0 + X2_0,                         A_3 ~ X1_2 + X2_2 +                           A_2 + X1_1 + X2_1 +                           A_1 + X1_0 + X2_0),                    data = msmdata,                    method = \"glm\",                    stabilize = TRUE,                    num.formula = list(~ 1,                                       ~ A_1,                                       ~ A_1 + A_2))  #Same as above but with fully saturated stabilization factors #(i.e., making the last entry in 'num.formula' A_1*A_2) W3 <- weightitMSM(list(A_1 ~ X1_0 + X2_0,                         A_2 ~ X1_1 + X2_1 +                           A_1 + X1_0 + X2_0,                         A_3 ~ X1_2 + X2_2 +                           A_2 + X1_1 + X2_1 +                           A_1 + X1_0 + X2_0),                    data = msmdata,                    method = \"glm\",                    stabilize = TRUE)"},{"path":"https://ngreifer.github.io/WeightIt/news/index.html","id":"weightit-development-version","dir":"Changelog","previous_headings":"","what":"WeightIt (development version)","title":"WeightIt (development version)","text":"Fixed bug output bread() factor -1. doesn’t affect use sandwich::sandwich(). Typo fixes vignettes documentation.","code":""},{"path":"https://ngreifer.github.io/WeightIt/news/index.html","id":"weightit-140","dir":"Changelog","previous_headings":"","what":"WeightIt 1.4.0","title":"WeightIt 1.4.0","text":"CRAN release: 2025-02-24 Entropy balancing works slightly differently sampling weights supplied. negative entropy estimated weights product sampling weights base weights () now quantity minimized optimization. Previously, negative entropy product sampling weights estimated weights base weights minimized. new behavior ensures entropy balancing consistent mathematically equivalent methods (.e., CBPS IPT ATT) prevents counter-intuitive results, like ESS weighting larger weighting. Note cause results differ previous versions WeightIt. method = \"cbps\", estimand can now set \"ATO\" binary multi-category treatments. binary treatments default link, yield identical weights using method = \"glm\" estimand = \"ATO\". Two new links can supplied method = \"glm\", \"cbps\", \"ipt\": \"loglog\" log-log link \"clog\" complementary log link. link argument can also now supplied link-glm object (e.g., output call make.link()). allows flexibility link function used estimate propensity score. new solver argument can supplied weightit() method = \"ebal\" method = \"cbps\" = FALSE (default); argument controls whether use rootSolve::multiroot() stats::optim() solve optimization problem weights. multiroot() used default rootSolve installed quicker accurate. methods, analytic formulas derivatives used M-estimation now used instead relying numeric differentiation. increases speed accuracy computing standard errors adjust estimation weights. formulas benchmarked numeric differentiation results. Added new method argument calibrate() support isotonic regression calibration described van der Laan el al. (2024). Added clearer error warning messages several functions, notably glm_weightit() friends, missing values present model variables. update() method glm_weightit objects friends now bit sophisticated. data s.weights supplied, weightit object () refit refitting glm_weightit model. makes easy performing bootstrapping simply calling update() fitted object new dataset bootstrap weights. estfun() bread() methods glm_weightit objects friends now correctly extract estimating function bread matrices used computing sandwich covariance matrix using sandwich::sandwich(). estfun() (thereby sandwich()) optional asympt argument, , controls whether asymptotic covariance matrix accounting estimation weights used. Improved estimation convergence method = \"cbps\". particular, -identified CBPS, generalized inverse now used GMM weight matrix singular. Previously, always used. Improved processing estimand focal binary treatments. weightit() now better guessing level treatment considered “treated”, focal can used identify focal group requesting ATT ATC. (#77) Improved estimation asymptotic HC0 covariance matrices glm_weightit() friends. using method = \"super\" SL.method = \"method.balance\", new algorithm used compute optimal combination predictions, yield better performance. may cause results differ past versions. Fixed bug using fractional weighted bootstrap glm_weightit() friends entropy balancing. bug still present optimization-based weighting (.e., method = \"optweight\"), fractional weighted bootstrap longer allowed method. Sampling weights can longer used method = \"optweight\" bug sorted . Performance enhancements. Added new tests CBPS, IPT, entropy balancing. Updated documentation msmdata. Thanks @larsentom. (#79)","code":""},{"path":"https://ngreifer.github.io/WeightIt/news/index.html","id":"weightit-132","dir":"Changelog","previous_headings":"","what":"WeightIt 1.3.2","title":"WeightIt 1.3.2","text":"CRAN release: 2024-11-05 Fixes tests CRAN. Improvements weight calculation continuous treatments small densities.","code":""},{"path":"https://ngreifer.github.io/WeightIt/news/index.html","id":"weightit-131","dir":"Changelog","previous_headings":"","what":"WeightIt 1.3.1","title":"WeightIt 1.3.1","text":"CRAN release: 2024-10-04 vcov(), summary(), anova(), confint() glm_weightit objects (relatives) now vcov argument can used specify variance matrix computed. makes possible compute variance matrix different one specified model fitting call without refit model. anova() now displays variance matrix used. Added update() methods glm_weightit, multinom_weightit, ordinal_weightit, coxph_weightit objects update model formula, dataset, variance matrix. Updating dataset also refits weightit object included, . anova() glm_weightit objects gets help page help(\"anova.glm_weightit()\"). Changed defaults missing = \"saem\" binary multi-category treatments bypass bug misaem code. (#71) Preemptively fixed bugs related use missing, including missing used . missingness method () now included output weightit(), weightitMSM(), weightit.fit() printed using print() method objects. missing = \"saem\", using vcov = \"FWB\" glm_weightit(), etc., now appropriately results error. (#71) model.matrix.ordinal_weightit() now excludes (Intercept) column. Fixed bug predict.multinom_weightit() predict.ordinal_weightit() outcome included newdata. Typo fixes documentation.","code":""},{"path":"https://ngreifer.github.io/WeightIt/news/index.html","id":"weightit-130","dir":"Changelog","previous_headings":"","what":"WeightIt 1.3.0","title":"WeightIt 1.3.0","text":"CRAN release: 2024-08-24 Added anova() methods glm_weightit, multinom_weightit, ordinal_weightit, coxph_weightit objects perform Wald tests comparing nested models. models symbolically nested. Added new user-facing object .weightit_methods, contains information method options allowed . used within WeightIt checking arguments can also used package developers call functions WeightIt. See help(\".weightit_methods\") details. plot.weightit() can used method = \"optweight\" display dual variables. missing longer allows partial matching. moments can now set 0 quantile supplied ensure balance quantiles without moments methods accepts quantiles. Thanks @BERENZ suggestion. ordinal_weightit objects, summary() now option omit thresholds output. Fixed bug ordinal_weightit() Hessian (therefore HC0 robust variance) calculated incorrectly come coefficients aliased (.e., due linearly dependent predictors). Fixed bug print.summary.glm_weightit() confidence intervals requested. new printing function used produces slightly nicer tables. Fixes vignettes tests satisfy CRAN checks. Minor bug, performance, readability fixes.","code":""},{"path":"https://ngreifer.github.io/WeightIt/news/index.html","id":"weightit-120","dir":"Changelog","previous_headings":"","what":"WeightIt 1.2.0","title":"WeightIt 1.2.0","text":"CRAN release: 2024-07-26 Added two new functions, multinom_weightit() ordinal_weightit() multinomial logistic regression ordinal regression capabilities estimate covariance matrix accounts estimation weights using M-estimation. Previously, multinomial logistic regression requested using glm_weightit() family = \"multinomial\"; deprecated. M-estimation can now used weighting ordinal regression weights multi-category ordered treatments method = \"glm\". M-estimation can now used bias-reduced regression implemented brglm2 propensity score (method = \"glm\" link = \"br.{.}\") outcome model (glm_weightit() br = TRUE). Thanks Ioannis Kosmidis supplying starter code implement . weighting methods continuous treatments support density argument specify numerator denominator densities weights, density can now specified \"kernel\" request kernel density estimation. Previously, requested setting use.kernel = TRUE, now deprecated. Standard errors now correctly computed offset included glm_weightit(). Thanks @zeynepbaskurt. (#63) Improved robustness get_w_from_ps() propensity scores 0 1. Updates weightit() method = \"gbm\": use.offset now tunable. random seed used across specifications requested @mitchellcameron123. (#64) binary multi-category treatments cross-validation used criterion, class.stratify.cv now set TRUE default stratify treatment. continuous treatments, default density now corresponds distribution requested. plot() can used output weightit() display results tuning process; see help(\"plot.weightit\") details. Fixed bug distribution included output tuned. Fixed bug propensity scores estimated 0 1. Thanks @mitchellcameron123. Propensity scores now shifted slightly away 0 1. (#64) using weightit() method = \"super\" binary multi-category treatments, cross-validation now stratifies treatment, recommended Phillips et al. (2023). Fixed bug clarified error messages using ordered treatments method = \"glm\". Thanks Steve Worthington pointing . Updated help page get_w_from_ps() include formulas weights.","code":""},{"path":"https://ngreifer.github.io/WeightIt/news/index.html","id":"weightit-110","dir":"Changelog","previous_headings":"","what":"WeightIt 1.1.0","title":"WeightIt 1.1.0","text":"CRAN release: 2024-05-04 Added new function, coxph_weightit(), fitting Cox proportional hazards models weighted sample, option accounting estimation weights computing standard errors via bootstrapping. function uses summary() print() methods glm_weightit objects, different coxph objects. glm_weightit() gets new print() method omits invalid statistics displayed print() method glm objects displays type standard error estimated. summary.glm_weightit() (also used coxph_weightit objects) gets new argument, transform, can used transform displayed coefficients confidence interval bounds (requested), e.g., exponentiating . M-estimation now supported method = \"glm\" continuous treatments. new estimator now used method = \"cbps\" longitudinal treatments (.e., using weightitMSM()). Previously, weights CBPS applied time point multiplied together. Now, balance time points optimized using single set weights. implementation close described Huffman van Gameren (2018), Imai Ratkovic (2015). new estimator now used method = \"cbps\" continuous treatments. unconditional mean variance now included parameters estimated. just-identified CBPS, typically improve balance, results depart found using CBPS::CBPS(). point treatments (.e., using weightit()), stabilize argument new behavior. can now specified formula, stabilization factor estimated separately included M-estimation allowed. can now used estimand = \"ATE\" (weights estimands stabilized). binary treatments method = \"glm\", link can now specified \"flic\" \"flac\" use Firth corrected logistic regression implemented logistf package. method = \"gbm\", error now thrown criterion (formerly known stop.method) supplied anything string. binary continuous treatments method = \"gbm\", new argument, use.offset, can supplied, , TRUE, uses linear predictor generalized linear model offset boosting model, can improve performance. Added section conducting moderation analysis estimating effect vignette (vignette(\"estimating-effects\")). Fixed bug using M-estimation sequential treatments weightitMSM() stabilize = TRUE. Standard errors incorrectly accounted estimation stabilization factor; now correct. Fixed bug using method = \"ipt\" ATE. Fixed bug coefficients aliased glm_weightit(). Thanks @kkwi5241. Updated kernel balancing example method_user. Improved warnings errors bad models throughout package.","code":""},{"path":"https://ngreifer.github.io/WeightIt/news/index.html","id":"weightit-100","dir":"Changelog","previous_headings":"","what":"WeightIt 1.0.0","title":"WeightIt 1.0.0","text":"CRAN release: 2024-03-23 Added new function, glm_weightit() (along wrapper lm_weightit()) associated methods fitting generalized linear models weighted sample, option accounting estimation weights computing standard errors via M-estimation two forms bootstrapping. glm_weightit() also supports multinomial logistic regression addition models supported glm(). Cluster-robust standard errors supported, output compatible functions accept glm() objects. weighting methods support M-estimation, , new component added weightit output object. Currently, GLM propensity scores, entropy balancing, just-identified CBPS, inverse probability tilting (described ) support M-estimation-based standard errors glm_weightit(). Added inverse probability tilting (IPT) described Graham, Pinto, Egel (2012), can requested setting method = \"ipt\". similar entropy balancing CBPS enforces exact balance yields propensity score, theoretical advantages methods. IPT rely packages runs quickly. Estimating covariate balancing propensity score weights (.e., method = \"cbps\") longer depends CBPS package. default now just-identified versions method; -identified version can requested setting = TRUE. ATT multi-category treatments now supported, arbitrary numbers treatment groups (CBPS natively support 4 groups ATE multi-category treatments). binary treatments, generalized linear models logistic regression now supported (e.g., probit Poisson regression). New function calibrate() apply Platt scaling calibrate propensity scores recommended Gutman et al. (2024). new argument quantile can supplied weightit() methods accept moments int (\"ebal\", \"cbps\", \"ipt\", \"npcbps\", \"optweight\", \"energy\"). allows one request balance quantiles covariates, can add robustness demonstrated Beręsewicz (2023). .weightit() now method weightit.fit objects, now additional components included output. trim() now drop argument; setting TRUE sets weights trimmed units 0 (effectively dropping ). using weightit() continuous treatment method estimates generalized propensity score (e.g., \"glm\", \"gbm\", \"super\"), sampling weights now incorporated density use.kernel = FALSE (default) supplied s.weights. Previously ignored calculating density, always remain used modeling treatment (allowed). Fixed bug criterion specified using method = \"gbm\". Fixed bug ps supplied continuous treatments. Thanks @taylordunn. (#53) Warning messages now display immediately rather end evaluation. vignettes changed use slightly different estimator weighted g-computation. estimated weights longer included call avg_comparisons(), etc.; , used fit outcome model. makes estimators consistent software, including teffects ipwra Stata, literature weighted g-computation. Note effect estimates ATT ATC yield minor changes ATE. estimands (e.g., ATO), weights still included. word “multinomial” describe treatments two categories replaced “multi-category” documentation messages. Transferred help files Roxygen reorganized package scripts. Reorganization functions.","code":""},{"path":"https://ngreifer.github.io/WeightIt/news/index.html","id":"weightit-0142","dir":"Changelog","previous_headings":"","what":"WeightIt 0.14.2","title":"WeightIt 0.14.2","text":"CRAN release: 2023-05-23 Fixed bug using estimand = \"ATC\" multi-category treatments. (#47) Fixed bug Estimating Effects vignette. (#46)","code":""},{"path":"https://ngreifer.github.io/WeightIt/news/index.html","id":"weightit-0141","dir":"Changelog","previous_headings":"","what":"WeightIt 0.14.1","title":"WeightIt 0.14.1","text":"CRAN release: 2023-05-09 cobalt version 4.5.1 greater now required. Fixed bug using balance Super Learner cobalt 4.5.1. Added section Estimating Effects vignette (vignette(\"estimating-effects\")) estimating effect continuous treatment weighting.","code":""},{"path":"https://ngreifer.github.io/WeightIt/news/index.html","id":"weightit-0140","dir":"Changelog","previous_headings":"","what":"WeightIt 0.14.0","title":"WeightIt 0.14.0","text":"CRAN release: 2023-04-12 Added energy balancing continuous treatments, requested using method = \"energy\", described Huling et al. (2023). weights minimize distance covariance treatment covariates maintaining representativeness. method supports exact balance constraints, distributional balance constraints, sampling weights. implementation similar independenceWeights package. See ?method_energy details. Added new vignette estimating effects weighting, accessible using vignette(\"estimating-effects\", package = \"WeightIt\"). new workflow relies marginaleffects package. main vignette (vignette(\"WeightIt\")) modernized well. Added new dataset, msmdata, demonstrate capabilities longitudinal treatments. twang longer dependency. Methods use balance criterion select tuning parameter, particular GBM balance Super Learner, now rely cobalt’s bal.init() bal.compute() functionality, adds new balance criteria. stop.method argument functions renamed criterion help(\"stop.method\") removed; page now available help(\"bal.compute\", package = \"cobalt\"), describes additional statistics available. also fixes bugs present balance criteria. Renamed method = \"ps\" method = \"glm\". \"ps\" continues work always back compatibility. \"glm\" descriptive name since many methods use propensity scores; distinguishes method uses generalized linear models. Using method = \"ebcw\" empirical balancing calibration weighting longer available ATE package removed. Use method = \"ebal\" entropy balancing instead, essentially identical. Updated trim() documentation clarify form trimming implemented (.e., winsorizing). Suggested David Novgorodsky. Fixed bugs s.weights equal zero method = \"ebal\", “cbps\", \"energy\". Suggested @statzhero. (#41) Improved performance method = \"energy\" ATT. Fixed bug using method = \"energy\" . method = \"energy\", setting int = TRUE automatically sets moments = 1 unspecified. Errors warnings updated use chk. missingness indicator approach now imputes variable median rather 0 missing values. change performance methods, change others, doesn’t affect balance assessment.","code":""},{"path":"https://ngreifer.github.io/WeightIt/news/index.html","id":"weightit-0131","dir":"Changelog","previous_headings":"","what":"WeightIt 0.13.1","title":"WeightIt 0.13.1","text":"CRAN release: 2022-06-28 ordinal multi-category treatments, setting link = \"br.logit\" now uses brglm2::bracl() fit bias-reduced ordinal regression model. Added vignette “Installing Supporting Packages” explain install various packages might needed WeightIt use certain methods, including package CRAN. See vignette vignette(\"installing-packages\"). Fixed bug occur factor character predictor single level passed weightit(). Improved code entropy balancing, fixing bug using s.weights continuous treatment improving messages optimization fails converge. (#33) Improved robustness documentation missing packages. Updated logo, thanks Ben Stillerman.","code":""},{"path":"https://ngreifer.github.io/WeightIt/news/index.html","id":"weightit-0130","dir":"Changelog","previous_headings":"","what":"WeightIt 0.13.0","title":"WeightIt 0.13.0","text":"Fixed bug occur formula.tools package loaded, occur commonly logistf loaded. cause error treatment covariates must number units. (#25) Fixed bug info component included output weightit() using method = \"super\". Added ability specify num.formula list formulas weightitMSM(). primarily get around fact stabilize = TRUE, fully saturated model treatments used compute stabilization factor, , many time points, time-consuming may impossible (especially treatment combinations observed). Thanks @maellecoursonnais bringing issue (#27). ps.cont() retired since functionality available using weightit() method = \"gbm\" twangContinuous package. method = \"energy\", new argument, lambda, can supplied, puts penalty square weights control effective sample size. Typically needed can help balancing aggressive. method = \"energy\", min.w can now negative, allowing negative weights. method = \"energy\", dist.mat can now supplied name method compute distance matrix: \"scaled_euclidean\", \"mahalanobis\", \"euclidean\". Support negative weights added summary(). Negative weights possible (though default) using method = \"energy\" method = \"optweight\". Fixed bug glm() fail converge method = \"ps\" binary treatments due bad starting values. (#31) miss = \"saem\" can used method = \"ps\" missing values present covariates. Fixed bugs processing input formulas. error now thrown incorrect link supplied method = \"ps\".","code":""},{"path":"https://ngreifer.github.io/WeightIt/news/index.html","id":"weightit-0120","dir":"Changelog","previous_headings":"","what":"WeightIt 0.12.0","title":"WeightIt 0.12.0","text":"CRAN release: 2021-04-03 use method = \"twang\" retired now give error message. Use method = \"gbm\" nearly identical functionality options, detailed ?method_gbm. multinomial treatments link = \"logit\" (default), mclogit package installed, can requested estimating propensity score setting option use.mclogit = TRUE, uses mclogit::mblogit(). give results default, uses mlogit, can faster recommended. Added plot() method summary.weightitMSM objects functions just like plot.summary.weightit() time point. Fixed bug summary.weightit() labels top weights incorrect. Thanks Adam Lilly. Fixed bug sbps() using stochastic search (.e., full.search = FALSE 8 moderator levels). (#17) Fixed bug occur weights treatment group NA. Bad weights (.e., ) now produce warning rather error weights can diagnosed manually. (#18) Fixed bug using method = \"energy\" estimand = \"ATE\" improved = TRUE (default). -treatment energy distance contribution half ; now corrected. Added L1 median measure balance criterion. See ?stop.method details. Fixed bug logical treatments yield error. (#21) Fixed bug Warning: Deprecated appear sometimes purrr (part tidyverse) loaded. (#22) Thanks MrFlick StackOverflow solution.","code":""},{"path":"https://ngreifer.github.io/WeightIt/news/index.html","id":"weightit-0110","dir":"Changelog","previous_headings":"","what":"WeightIt 0.11.0","title":"WeightIt 0.11.0","text":"CRAN release: 2021-02-02 Added support estimating propensity scores using Bayesian additive regression trees (BART) method = \"bart\". method fits BART model treatment using functions dbarts package estimate propensity scores used weights. Binary, multinomial, continuous treatments supported. BART uses Bayesian priors hyperparameters, hyperparameter tuning necessary get well-performing predictions. Fixed bug using method = \"gbm\" stop.method = \"cv{#}\". Fixed bug setting estimand = \"ATC\" methods produce propensity score. past, output propensity score probability control group; now, probability treated group, estimands. affect weights. Setting method = \"twang\" now deprecated. Use method = \"gbm\" improved performance increased functionality. method = \"twang\" relies twang package; method = \"gbm\" calls gbm directly. Using method = \"ebal\" longer requires ebal package. Instead, optim() used, continuous treatments. Balance little better, options removed. using method = \"ebal\" continuous treatments, new argument, d.moments, can now specified. controls number moments covariate treatment distributions constrained weighted sample original sample. Vegetabile et al. (2020) recommend setting d.moments least 3 ensure generalizability reduce bias due effect modification. Made minor changes summary.weightit() plot.summary.weightit(). Fixed negative entropy computed. option use.mnlogit weightit() multi-category treatments method = \"ps\" removed mnlogit appears uncooperative. Fixed bug (#16) using method = \"cbps\" factor variables, thanks @danielebottigliengo. Fixed bug using binary factor treatments, thanks Darren Stewart. Cleaned documentation.","code":""},{"path":"https://ngreifer.github.io/WeightIt/news/index.html","id":"weightit-0102","dir":"Changelog","previous_headings":"","what":"WeightIt 0.10.2","title":"WeightIt 0.10.2","text":"CRAN release: 2020-08-27 Fixed bug treatment values accidentally switched methods.","code":""},{"path":"https://ngreifer.github.io/WeightIt/news/index.html","id":"weightit-0101","dir":"Changelog","previous_headings":"","what":"WeightIt 0.10.1","title":"WeightIt 0.10.1","text":"CRAN release: 2020-08-12 method = \"gbm\", added ability tune hyperparameters like interaction.depth distribution using criteria used select optimal tree. summary tuning results included info weightit output object. Fixed bug moments int ignored unless specified. Effective sample sizes now print two digits (believe , don’t need three) print cleanly whole numbers. Fixed bug using , thanks @frankpopham. (#11) Fixed bug using weightitMSM methods process int moments (though probably shouldn’t use anyway). Thanks Sven Reiger. Fixed bug using method = \"npcbps\" weights excessively small mistaken . weights now sum number units.","code":""},{"path":"https://ngreifer.github.io/WeightIt/news/index.html","id":"weightit-0100","dir":"Changelog","previous_headings":"","what":"WeightIt 0.10.0","title":"WeightIt 0.10.0","text":"CRAN release: 2020-07-07 Added support energy balancing method = \"energy\". method minimizes energy distance samples, multivariate distance measure. method uses code written specifically WeightIt (.e., call package specifically designed energy balancing) using osqp package optimization (optweight). See Huling & Mak (2020) details method. Also included option require exact balance moments covariates minimizing energy distance. method works binary multinomial treatments ATE, ATT, ATC. Sampling weights supported. method requires calculation manipulation distance matrix units, can slow /memory intensive large datasets. Improvements method = \"gbm\" method = \"super\" SL.method = \"method.balance\". new suite stop.methods allowed. binary treatments, include energy distance, sample Mahalanobis distance, pseudo-R2 weighted treatment model, among others. See ?stop.method allowable options. addition, performance quite bit faster. multinomial treatments link = \"logit\" (default), mnlogit package installed, can requested estimating propensity score setting option use.mnlogit = TRUE. give results default, uses mlogit, can faster large datasets. Added option estimand = \"ATOS\" “optimal subset” treatment effect described Crump et al. (2009). estimand finds subset units , ATE weights applied, yields treatment effect lowest variance, assuming homoscedasticity (assumptions). available binary treatments method = \"ps\". general makes sense use estimand = \"ATO\" want low-variance estimate don’t care target population, added completeness. available get_w_from_ps() well. make_full_rank() now faster. Cleaning error messages. Fixed bug using link = \"log\" method = \"ps\" binary treatments. Fixed bug using method = \"cbps\" continuous treatments sampling weights. Previously returned weights included sampling weights multiplied ; now separated, scenarios methods. Improved processing non-0/1 binary treatments, including method = \"gbm\". guess made treatment considered “treated”; affects produced propensity scores weights. Changed default value trim() .99 0. Added output number weights equal zero summary.weightit. can especially helpful using \"optweight\" \"energy\" methods using estimand = \"ATOS\".","code":""},{"path":"https://ngreifer.github.io/WeightIt/news/index.html","id":"weightit-090","dir":"Changelog","previous_headings":"","what":"WeightIt 0.9.0","title":"WeightIt 0.9.0","text":"CRAN release: 2020-02-11 Added support entropy balancing (method = \"ebal\") continuous treatments described Tübbicke (2020). Relies hand-written code contributed Stefan Tübbicke rather another R package. Sampling weights base weights supported binary multi-category treatments. Added support Balance SuperLearner described Pirracchio Carone (2018) method = \"super\". Rather using NNLS choose optimal combination predictions, can now optimize balance. , set SL.method = \"method.balance\". need set argument stop.method, works identically method = \"gbm\". example, stop.method = \"es.max\", predicted values given combination predicted values minimizes largest absolute standardized mean difference covariates sample weighted using predicted values propensity scores. Changed statistics displayed using summary(): weight ratio gone (weights can 0, problematic explode ratio), mean absolute deviation entropy weights now present. Added crayon prettier printing summary() output.","code":""},{"path":"https://ngreifer.github.io/WeightIt/news/index.html","id":"weightit-080","dir":"Changelog","previous_headings":"","what":"WeightIt 0.8.0","title":"WeightIt 0.8.0","text":"CRAN release: 2020-01-12 Formula interfaces now accept poly(x, .) matrix-generating functions variables, including rms-class-generating functions rms package (e.g., pol(), rcs(), etc.) (rms package must loaded use latter ones) basis-class-generating functions splines package (.e., bs() ns()). bug early version found @ahinton-mmc. Added support marginal mean weighting stratification (MMWS) described Hong (2010, 2012) weightit() get_w_from_ps() subclass argument (see References ?get_w_from_ps). method, subclasses formed based propensity score weights computed based number units subclass. MMWS can used method produces propensity score. implementation ensures subclasses least one member filling empty subclasses neighboring units. Added stabilize option get_w_from_ps(). new missing argument added weightit() choose missing data covariates handled. methods, \"ind\" (.e., missing indicators single-value imputation) allowed, \"ps\", \"gbm\", \"twang\", methods possible. method = \"ps\", stochastic approximation EM algorithm (SAEM) can used misaem package setting missing = \"saem\". continuous treatments \"ps\", \"gbm\", \"super\" methods (.e., conditional density treatment needs estimated), user can now supply density string function rather using normal density kernel density estimation. example, use density t-distribution 3 degrees freedom, one can set density = \"dt_3\". T-distributions often work better normal distributions extreme values treatment. methods now info component output object. contains information might useful diagnosing reporting method. example, method = \"gbm\", info contains tree used compute weights balance resulting trees, can plotted using plot(). method = \"super\", info contains coefficients stacking model cross-validation risk component methods. method = \"gbm\", best tree can chosen using cross validation rather balance setting stop.method = \"cv5\", e.g., 5-fold cross-validation. method = \"gbm\", new optional argument start.tree can set select tree balance begins computed. can speed things know best tree within first 100 trees, example. using method = \"gbm\" multi-category treatments estimands ATE, ATT, ATC used standardized mean differences stopping rule, mean differences weighted overall sample treatment group. Otherwise, efficiency improvements. using method = \"ps\" multi-category treatments, use use.mlogit = FALSE request multiple binary regressions instead multinomial regression now documented associated bug now fixed, thanks @ahinton-mmc. use method = \"super\", one can now set discrete = TRUE use discrete SuperLearner instead stacked SuperLearner, probably shouldn’t. moments int can now used method = \"npcbps\". Performance enhancements.","code":""},{"path":"https://ngreifer.github.io/WeightIt/news/index.html","id":"weightit-071","dir":"Changelog","previous_headings":"","what":"WeightIt 0.7.1","title":"WeightIt 0.7.1","text":"CRAN release: 2019-10-30 Fixed bug using weightit() inside another function passed argument explicitly. Also changed syntax ; must now either string (always possible) one-sided formula stratifying variable right-hand side. use variable data, must use formula interface. Fixed bug trying use ps weightit().","code":""},{"path":"https://ngreifer.github.io/WeightIt/news/index.html","id":"weightit-070","dir":"Changelog","previous_headings":"","what":"WeightIt 0.7.0","title":"WeightIt 0.7.0","text":"CRAN release: 2019-10-16 Added new sbps() function estimating subgroup balancing propensity score weights, including standard method new smooth version. Setting method = \"gbm\" method = \"twang\" now two different things. method = \"gbm\" uses gbm cobalt functions estimate weights much faster, method = \"twang\" uses twang functions estimate weights. results similar two methods. Prior version, method = \"gbm\" method = \"twang\" method = \"twang\" now. Bug fixes stabilize = TRUE, thanks @ulriksartipy Sven Rieger. Fixes using base.weight argument method = \"ebal\". Now supplied vector length equal number units dataset (contrast use ebalance, requires length equal number control units). Restored dependency cobalt examples vignette. method = \"ps\" treatment ordered (.e., ordinal), MASS::polr() used fit ordinal regression. Make treatment un-ordered use multinomial regression instead. Added support using bias-reduced fitting functions method = \"ps\" provided brglm2 package. can accessed changing link , example, \"br.logit\" \"br.probit\". multinomial treatments, setting link = \"br.logit\" fits bias-reduced multinomial regression model using brglm2::brmultinom(). can helpful regular maximum likelihood models fail converge, though may also sign lack overlap.","code":""},{"path":"https://ngreifer.github.io/WeightIt/news/index.html","id":"weightit-060","dir":"Changelog","previous_headings":"","what":"WeightIt 0.6.0","title":"WeightIt 0.6.0","text":"CRAN release: 2019-09-05 Bug fixes. Functions now work better used inside functions (e.g., lapply). Behavior weightit() presence non-NULL focal changed. focal specified, estimand assumed ATT. Previously, focal ignored unless estimand = \"ATT\". Processing estimand focal improved. Functions smarter guessing group focal group one isn’t specified, especially non-numeric treatments. focal can now used estimand = \"ATC\" indicate group control group, \"ATC\" \"ATT\" now function similarly. Added function get_w_from_ps() transform propensity scores weights (instead go weightit()). Added functions .weightit() .weightitMSM() convert weights treatments components weightit objects summary.weightit() can used . Updated documentation describe missing data covariates handled. bugs related missing data fixed well, thanks Yong Hao Pua. ps.cont() “z-transformed correlation” options removed simplify output. function supporting functions deprecated soon new version twang released. using method = \"ps\" method = \"super\" continuous treatments, setting use.kernel = TRUE plot = TRUE, plot now made ggplot2 rather base R plots. Added plot.summary.weightit() plot distribution weights (feature also optweight). Removed dependency cobalt temporarily, means examples vignette won’t run. Added ggplot2 Imports.","code":""},{"path":"https://ngreifer.github.io/WeightIt/news/index.html","id":"weightit-051","dir":"Changelog","previous_headings":"","what":"WeightIt 0.5.1","title":"WeightIt 0.5.1","text":"CRAN release: 2019-01-16 Fixed bug using ps argument weightit(). Fixed bug setting include.obj = TRUE weightitMSM(). Added warnings using certain methods longitudinal treatments validated may lead incorrect inferences.","code":""},{"path":"https://ngreifer.github.io/WeightIt/news/index.html","id":"weightit-050","dir":"Changelog","previous_headings":"","what":"WeightIt 0.5.0","title":"WeightIt 0.5.0","text":"CRAN release: 2018-11-22 Added super method estimate propensity scores using SuperLearner package. Added optweight method estimate weights using optimization (probably just use optweight package). weightit() now uses correct formula estimate weights ATO multinomial treatments described Li & Li (2018). Added include.obj option weightit() weightitMSM() include fitted object output object inspection. example, method = \"ps\", glm object containing propensity score model included output. Rearranged help pages. method now documentation page, linked weightit help page. Propensity scores now included output binary treatments gbm cbps methods. Thanks @Blanch-Font suggestion. bug fixes minor changes.","code":""},{"path":"https://ngreifer.github.io/WeightIt/news/index.html","id":"weightit-040","dir":"Changelog","previous_headings":"","what":"WeightIt 0.4.0","title":"WeightIt 0.4.0","text":"CRAN release: 2018-06-25 Added trim() function trim weights. Added ps.cont() function, estimates generalized propensity score weights continuous treatments using generalized boosted modeling, twang. function uses syntax ps() twang, can also accessed using weightit() method = \"gbm\". Support functions added make compatible twang functions assessing balance (e.g., summary, bal.table, plot). Thanks Donna Coffman enlightening method providing code implement . input formula now much forgiving, allowing objects environment included. data argument weightit() now optional. simplify things, output object longer contains data field. --hood changes facilitate adding new features debugging. aspects output objects slightly changed, shouldn’t affect use users. Fixed bug variables thrown method = \"ebal\".","code":""},{"path":"https://ngreifer.github.io/WeightIt/news/index.html","id":"weightit-032","dir":"Changelog","previous_headings":"","what":"WeightIt 0.3.2","title":"WeightIt 0.3.2","text":"CRAN release: 2018-03-14 Added new moments int options weightit() methods easily specify moments interactions covariates. Fixed bug using objects data set weightit(). Behavior changed include transformed covariates entered formula weightit() output. Fixed bug resulting potential collinearity using ebal ebcw. Added vignette.","code":""},{"path":"https://ngreifer.github.io/WeightIt/news/index.html","id":"weightit-031","dir":"Changelog","previous_headings":"","what":"WeightIt 0.3.1","title":"WeightIt 0.3.1","text":"CRAN release: 2018-03-03 Edits code help files protect missing CBPS package. Corrected sampling weights functionality work correctly. Also expanded sampling weights able used methods, including natively allow sampling weights (e.g., ATE). Minor bug fixes spelling corrections.","code":""},{"path":"https://ngreifer.github.io/WeightIt/news/index.html","id":"weightit-030","dir":"Changelog","previous_headings":"","what":"WeightIt 0.3.0","title":"WeightIt 0.3.0","text":"CRAN release: 2018-01-14 Added weightitMSM() function (supporting print() summary() functions) estimate weights marginal structural models time-varying treatments covariates. Fixed bugs, including using CBPS continuous treatments, using focal incorrectly.","code":""},{"path":"https://ngreifer.github.io/WeightIt/news/index.html","id":"weightit-020","dir":"Changelog","previous_headings":"","what":"WeightIt 0.2.0","title":"WeightIt 0.2.0","text":"CRAN release: 2017-11-12 Added method = \"sbw\" stable balancing weights (now removed replaced method = \"optweight\") Allowed estimation multinomial propensity scores using multiple binary regressions mlogit installed Allowed estimation multinomial CBPS using multiple binary CBPS 4 groups Added README NEWS","code":""},{"path":"https://ngreifer.github.io/WeightIt/news/index.html","id":"weightit-010","dir":"Changelog","previous_headings":"","what":"WeightIt 0.1.0","title":"WeightIt 0.1.0","text":"CRAN release: 2017-10-17 First version!","code":""}]

[{"path":"https://ngreifer.github.io/WeightIt/articles/WeightIt.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Using WeightIt to Estimate Balancing Weights","text":"WeightIt contains several functions estimating assessing balancing weights observational studies. weights can used estimate causal parameters marginal structural models. go basics causal inference methods . good introductory articles, see Austin (2011), Austin Stuart (2015), Robins, Hernán, Brumback (2000), Thoemmes Ong (2016). Typically, analysis observation study might proceed follows: identify covariates balance required; assess quality data available, including missingness measurement error; estimate weights balance covariates adequately; estimate treatment effect corresponding standard error confidence interval. guide go steps two observational studies: estimating causal effect point treatment outcome, estimating causal parameters marginal structural model multiple treatment periods. meant definitive guide, rather introduction relevant issues.","code":""},{"path":"https://ngreifer.github.io/WeightIt/articles/WeightIt.html","id":"balancing-weights-for-a-point-treatment","dir":"Articles","previous_headings":"","what":"Balancing Weights for a Point Treatment","title":"Using WeightIt to Estimate Balancing Weights","text":"First use Lalonde dataset estimate effect point treatment. ’ll use version data set resides within cobalt package, use later well. , interested average treatment effect treated (ATT). outcome (re78), treatment (treat), covariates balance desired (age, educ, race, married, nodegree, re74, re75). Using cobalt, can examine initial imbalance covariates: Based output, can see variables imbalanced sense standardized mean differences (continuous variables) differences proportion (binary variables) greater .05 variables. particular, re74 re75 quite imbalanced, troubling given likely strong predictors outcome. estimate weights using weightit() try attain balance covariates. First, ’ll start simple, use inverse probability weights propensity scores generated logistic regression. need supply weightit() formula model, data set, estimand (ATT), method estimation (\"glm\") generalized linear model propensity score weights). Printing output weightit() displays summary weights estimated. Let’s examine quality weights using summary(). Weights low variability desirable improve precision estimator. variability presented several ways: ratio largest weight smallest group, coefficient variation (standard deviation divided mean) weights group, effective sample size computed weights. want small ratio, smaller coefficient variation, large effective sample size (ESS). constitutes values mostly relative, though, must balanced constraints, including covariate balance. metrics best used comparing weighting methods, ESS can give sense much information remains weighted sample familiar scale. weights quite high variability, yield ESS close 100 control group. Let’s see weights managed yield balance covariates. nearly covariates, weights yielded good balance. age remained imbalanced, standardized mean difference greater .05 variance ratio greater 2. Let’s see can better. ’ll choose different method: entropy balancing (Hainmueller 2012), guarantees perfect balance specified moments covariates minimizing entropy (measure dispersion) weights. variability weights changed much, let’s see gains terms balance: Indeed, achieved perfect balance means covariates. However, variance ratio age still quite high. continue try adjust imbalance, reason believe unlikely affect outcome, may best leave . (can try adding (age^2) formula see changes causes.) Now weights stored W., let’s extract estimate treatment effect. confidence interval treat contains 0, isn’t evidence treat effect re78. Although authors recommend using “robust” sandwich standard errors adjust weights (Robins, Hernán, Brumback 2000; Hainmueller 2012) , others believe can misleading recommend bootstrapping instead (Reifeis Hudgens 2020; Chan, Yam, Zhang 2016). methods described detail vignette(\"estimating-effects\").","code":"library(\"cobalt\") ##  cobalt (Version 4.5.1, Build Date: 2023-04-27) data(\"lalonde\", package = \"cobalt\") head(lalonde) ##   treat age educ   race married nodegree re74 re75    re78 ## 1     1  37   11  black       1        1    0    0  9930.0 ## 2     1  22    9 hispan       0        1    0    0  3595.9 ## 3     1  30   12  black       0        0    0    0 24909.5 ## 4     1  27   11  black       0        1    0    0  7506.1 ## 5     1  33    8  black       0        1    0    0   289.8 ## 6     1  22    9  black       0        1    0    0  4056.5 bal.tab(treat ~ age + educ + race + married + nodegree + re74 + re75,         data = lalonde, estimand = \"ATT\", thresholds = c(m = .05)) ## Balance Measures ##                Type Diff.Un      M.Threshold.Un ## age         Contin.  -0.309 Not Balanced, >0.05 ## educ        Contin.   0.055 Not Balanced, >0.05 ## race_black   Binary   0.640 Not Balanced, >0.05 ## race_hispan  Binary  -0.083 Not Balanced, >0.05 ## race_white   Binary  -0.558 Not Balanced, >0.05 ## married      Binary  -0.324 Not Balanced, >0.05 ## nodegree     Binary   0.111 Not Balanced, >0.05 ## re74        Contin.  -0.721 Not Balanced, >0.05 ## re75        Contin.  -0.290 Not Balanced, >0.05 ##  ## Balance tally for mean differences ##                     count ## Balanced, <0.05         0 ## Not Balanced, >0.05     9 ##  ## Variable with the greatest mean difference ##  Variable Diff.Un      M.Threshold.Un ##      re74  -0.721 Not Balanced, >0.05 ##  ## Sample sizes ##     Control Treated ## All     429     185 library(\"WeightIt\") W.out <- weightit(treat ~ age + educ + race + married + nodegree + re74 + re75,                   data = lalonde, estimand = \"ATT\", method = \"glm\") W.out #print the output ## A weightit object ##  - method: \"glm\" (propensity score weighting with GLM) ##  - number of obs.: 614 ##  - sampling weights: none ##  - treatment: 2-category ##  - estimand: ATT (focal: 1) ##  - covariates: age, educ, race, married, nodegree, re74, re75 summary(W.out) ##                  Summary of weights ##  ## - Weight ranges: ##  ##            Min                                 Max ## treated 1.0000         ||                    1.000 ## control 0.0092 |---------------------------| 3.743 ##  ## - Units with the 5 most extreme weights by group: ##                                             ##               6      5      3      2      1 ##  treated      1      1      1      1      1 ##             597    573    381    411    303 ##  control 3.0301 3.0592 3.2397 3.5231 3.7432 ##  ## - Weight statistics: ##  ##         Coef of Var   MAD Entropy # Zeros ## treated       0.000 0.000  -0.000       0 ## control       1.818 1.289   1.098       0 ##  ## - Effective Sample Sizes: ##  ##            Control Treated ## Unweighted  429.       185 ## Weighted     99.82     185 bal.tab(W.out, stats = c(\"m\", \"v\"), thresholds = c(m = .05)) ## Balance Measures ##                 Type Diff.Adj         M.Threshold V.Ratio.Adj ## prop.score  Distance   -0.021     Balanced, <0.05       1.032 ## age          Contin.    0.119 Not Balanced, >0.05       0.458 ## educ         Contin.   -0.028     Balanced, <0.05       0.664 ## race_black    Binary   -0.002     Balanced, <0.05           . ## race_hispan   Binary    0.000     Balanced, <0.05           . ## race_white    Binary    0.002     Balanced, <0.05           . ## married       Binary    0.019     Balanced, <0.05           . ## nodegree      Binary    0.018     Balanced, <0.05           . ## re74         Contin.   -0.002     Balanced, <0.05       1.321 ## re75         Contin.    0.011     Balanced, <0.05       1.394 ##  ## Balance tally for mean differences ##                     count ## Balanced, <0.05         9 ## Not Balanced, >0.05     1 ##  ## Variable with the greatest mean difference ##  Variable Diff.Adj         M.Threshold ##       age    0.119 Not Balanced, >0.05 ##  ## Effective sample sizes ##            Control Treated ## Unadjusted  429.       185 ## Adjusted     99.82     185 W.out <- weightit(treat ~ age + educ + race + married + nodegree + re74 + re75,                   data = lalonde, estimand = \"ATT\", method = \"ebal\") summary(W.out) ##                  Summary of weights ##  ## - Weight ranges: ##  ##            Min                                 Max ## treated 1.0000    ||                         1.000 ## control 0.0188 |---------------------------| 9.419 ##  ## - Units with the 5 most extreme weights by group: ##                                             ##               5      4      3      2      1 ##  treated      1      1      1      1      1 ##             608    381    597    303    411 ##  control 7.1268 7.5013 7.9979 9.0355 9.4195 ##  ## - Weight statistics: ##  ##         Coef of Var   MAD Entropy # Zeros ## treated       0.000 0.000   0.000       0 ## control       1.834 1.287   1.101       0 ##  ## - Effective Sample Sizes: ##  ##            Control Treated ## Unweighted  429.       185 ## Weighted     98.46     185 bal.tab(W.out, stats = c(\"m\", \"v\"), thresholds = c(m = .05)) ## Balance Measures ##                Type Diff.Adj     M.Threshold V.Ratio.Adj ## age         Contin.        0 Balanced, <0.05       0.410 ## educ        Contin.        0 Balanced, <0.05       0.664 ## race_black   Binary        0 Balanced, <0.05           . ## race_hispan  Binary       -0 Balanced, <0.05           . ## race_white   Binary       -0 Balanced, <0.05           . ## married      Binary       -0 Balanced, <0.05           . ## nodegree     Binary       -0 Balanced, <0.05           . ## re74        Contin.       -0 Balanced, <0.05       1.326 ## re75        Contin.       -0 Balanced, <0.05       1.335 ##  ## Balance tally for mean differences ##                     count ## Balanced, <0.05         9 ## Not Balanced, >0.05     0 ##  ## Variable with the greatest mean difference ##  Variable Diff.Adj     M.Threshold ##   married       -0 Balanced, <0.05 ##  ## Effective sample sizes ##            Control Treated ## Unadjusted  429.       185 ## Adjusted     98.46     185 # Attach weights to dataset lalonde$weights <- W.out$weights  # Fit outcome model fit <- lm(re78 ~ treat * (age + educ + race + married +                             nodegree + re74 + re75),           data = lalonde, weights = weights)  # G-computation for the treatment effect library(\"marginaleffects\") avg_comparisons(fit, variables = \"treat\",                 vcov = \"HC3\",                 newdata = subset(lalonde, treat == 1),                 wts = \"weights\") ##  ##   Term Contrast Estimate Std. Error    z Pr(>|z|) 2.5 % 97.5 % ##  treat    1 - 0     1273        823 1.55    0.122  -339   2886 ##  ## Columns: term, contrast, estimate, std.error, statistic, p.value, conf.low, conf.high"},{"path":"https://ngreifer.github.io/WeightIt/articles/WeightIt.html","id":"balancing-weights-for-a-longitudinal-treatment","dir":"Articles","previous_headings":"","what":"Balancing Weights for a Longitudinal Treatment","title":"Using WeightIt to Estimate Balancing Weights","text":"WeightIt can estimate weights longitudinal treatment marginal structural models well. time, ’ll use sample data set msmdata estimate weights. Data must “wide” format, one row per unit. binary outcome variable (Y_B), pre-treatment time-varying variables (X1_0 X2_0, measured first treatment, X1_1 X2_1 measured first second treatments, X1_2 X2_2 measured second third treatments), three time-varying binary treatment variables (A_1, A_2, A_3). interested joint, unique, causal effects treatment period outcome. treatment time point, need achieve balance variables measured prior treatment, including previous treatments. Using cobalt, can examine initial imbalance time point overall: bal.tab() indicates significant imbalance covariates time points, need work eliminate imbalance weighted data set. ’ll use weightitMSM() function specify weight models. syntax similar weightit() point treatments bal.tab() longitudinal treatments. ’ll use method = \"glm\" stabilize = TRUE stabilized propensity score weights estimated using logistic regression. matter method selected, weightitMSM() estimates separate weights time period takes product weights individual arrive final estimated weights. Printing output weightitMSM() provides details function call output. can take look quality weights summary(), just point treatments. Displayed summaries weights perform time point respect variability. Next, ’ll examine well perform respect covariate balance. setting .time = .none bal.tab(), can focus overall balance assessment, displays greatest imbalance covariate across time points. can see estimated weights balance covariates time points respect means KS statistics. Now can estimate treatment effects. First, fit marginal structural model outcome using glm() weights included: , compute average expected potential outcomes treatment regime using marginaleffects::avg_predictions(): can compare expected potential outcomes regime using marginaleffects::hypotheses(). get pairwise comparisons, supply avg_predictions() output hypotheses(., \"pairwise\"). compare individual regimes, can use hypotheses(), identifying rows avg_predictions() output. example, compare regimes treatment three time points vs. regime treatment three time points, run results indicate receive treatment time points reduces risk outcome relative receiving treatment .","code":"data(\"msmdata\") head(msmdata) ##   X1_0 X2_0 A_1 X1_1 X2_1 A_2 X1_2 X2_2 A_3 Y_B ## 1    2    0   1    5    1   0    4    1   0   0 ## 2    4    0   1    9    0   1   10    0   1   1 ## 3    4    1   0    5    0   1    4    0   0   1 ## 4    4    1   0    4    0   0    6    1   0   1 ## 5    6    1   1    5    0   1    6    0   0   1 ## 6    5    1   0    4    0   1    4    0   1   0 library(\"cobalt\") #if not already attached bal.tab(list(A_1 ~ X1_0 + X2_0,              A_2 ~ X1_1 + X2_1 +                A_1 + X1_0 + X2_0,              A_3 ~ X1_2 + X2_2 +                A_2 + X1_1 + X2_1 +                A_1 + X1_0 + X2_0),         data = msmdata, stats = c(\"m\", \"ks\"),         which.time = .all) ## Balance by Time Point ##  ##  - - - Time: 1 - - -  ## Balance Measures ##         Type Diff.Un KS.Un ## X1_0 Contin.   0.690 0.276 ## X2_0  Binary  -0.325 0.325 ##  ## Sample sizes ##     Control Treated ## All    3306    4194 ##  ##  - - - Time: 2 - - -  ## Balance Measures ##         Type Diff.Un KS.Un ## X1_1 Contin.   0.874 0.340 ## X2_1  Binary  -0.299 0.299 ## A_1   Binary   0.127 0.127 ## X1_0 Contin.   0.528 0.201 ## X2_0  Binary  -0.060 0.060 ##  ## Sample sizes ##     Control Treated ## All    3701    3799 ##  ##  - - - Time: 3 - - -  ## Balance Measures ##         Type Diff.Un KS.Un ## X1_2 Contin.   0.475 0.212 ## X2_2  Binary  -0.594 0.594 ## A_2   Binary   0.162 0.162 ## X1_1 Contin.   0.573 0.237 ## X2_1  Binary  -0.040 0.040 ## A_1   Binary   0.100 0.100 ## X1_0 Contin.   0.361 0.148 ## X2_0  Binary  -0.040 0.040 ##  ## Sample sizes ##     Control Treated ## All    4886    2614 ##  - - - - - - - - - - - Wmsm.out <- weightitMSM(list(A_1 ~ X1_0 + X2_0,                              A_2 ~ X1_1 + X2_1 +                                A_1 + X1_0 + X2_0,                              A_3 ~ X1_2 + X2_2 +                                A_2 + X1_1 + X2_1 +                                A_1 + X1_0 + X2_0),                         data = msmdata, method = \"glm\",                         stabilize = TRUE) Wmsm.out ## A weightitMSM object ##  - method: \"glm\" (propensity score weighting with GLM) ##  - number of obs.: 7500 ##  - sampling weights: none ##  - number of time points: 3 (A_1, A_2, A_3) ##  - treatment:  ##     + time 1: 2-category ##     + time 2: 2-category ##     + time 3: 2-category ##  - covariates:  ##     + baseline: X1_0, X2_0 ##     + after time 1: X1_1, X2_1, A_1, X1_0, X2_0 ##     + after time 2: X1_2, X2_2, A_2, X1_1, X2_1, A_1, X1_0, X2_0 ##  - stabilized; stabilization factors: ##     + baseline: (none) ##     + after time 1: A_1 ##     + after time 2: A_1, A_2, A_1:A_2 summary(Wmsm.out) ##                  Summary of weights ##  ##                        Time 1                        ##                  Summary of weights ##  ## - Weight ranges: ##  ##            Min                                 Max ## treated 0.1527 |---------------------------| 57.08 ## control 0.1089 |--------|                    20.46 ##  ## - Units with the 5 most extreme weights by group: ##                                                 ##             4390    3440    3774   3593    5685 ##  treated 22.1008 24.1278 25.6999 27.786 57.0794 ##             6659    6284    1875   6163    2533 ##  control 12.8943   13.09 14.5234 14.705  20.465 ##  ## - Weight statistics: ##  ##         Coef of Var   MAD Entropy # Zeros ## treated       1.779 0.775   0.573       0 ## control       1.331 0.752   0.486       0 ##  ## - Mean of Weights = 0.99 ##  ## - Effective Sample Sizes: ##  ##            Control Treated ## Unweighted    3306    4194 ## Weighted      1193    1007 ##  ##                        Time 2                        ##                  Summary of weights ##  ## - Weight ranges: ##  ##            Min                                 Max ## treated 0.1089 |---------------------------| 57.08 ## control 0.1501 |--------|                    20.49 ##  ## - Units with the 5 most extreme weights by group: ##                                                  ##             4390    3440    3774    3593    5685 ##  treated 22.1008 24.1278 25.6999  27.786 57.0794 ##             1875    6163    6862    1286    6158 ##  control 14.5234  14.705 14.8079 16.2311 20.4862 ##  ## - Weight statistics: ##  ##         Coef of Var   MAD Entropy # Zeros ## treated       1.797 0.779   0.580       0 ## control       1.359 0.750   0.488       0 ##  ## - Mean of Weights = 0.99 ##  ## - Effective Sample Sizes: ##  ##            Control Treated ## Unweighted    3701  3799.  ## Weighted      1300   898.2 ##  ##                        Time 3                        ##                  Summary of weights ##  ## - Weight ranges: ##  ##            Min                                 Max ## treated 0.1089 |---------------------------| 57.08 ## control 0.2085 |-----------|                 25.70 ##  ## - Units with the 5 most extreme weights by group: ##                                                  ##             3576    4390    3440    3593    5685 ##  treated 20.5828 22.1008 24.1278  27.786 57.0794 ##             6163    6862     168    6158    3774 ##  control  14.705 14.8079 16.9698 20.4862 25.6999 ##  ## - Weight statistics: ##  ##         Coef of Var   MAD Entropy # Zeros ## treated       2.008 0.931   0.753       0 ## control       1.269 0.672   0.407       0 ##  ## - Mean of Weights = 0.99 ##  ## - Effective Sample Sizes: ##  ##            Control Treated ## Unweighted    4886  2614.  ## Weighted      1871   519.8 bal.tab(Wmsm.out, stats = c(\"m\", \"ks\"), which.time = .none) ## Balance summary across all time points ##              Times     Type Max.Diff.Adj Max.KS.Adj ## prop.score 1, 2, 3 Distance        0.095      0.074 ## X1_0       1, 2, 3  Contin.        0.033      0.018 ## X2_0       1, 2, 3   Binary        0.018      0.018 ## X1_1          2, 3  Contin.        0.087      0.039 ## X2_1          2, 3   Binary        0.031      0.031 ## A_1           2, 3   Binary        0.130      0.130 ## X1_2             3  Contin.        0.104      0.054 ## X2_2             3   Binary        0.007      0.007 ## A_2              3   Binary        0.154      0.154 ##  ## Effective sample sizes ##  - Time 1 ##            Control Treated ## Unadjusted    3306    4194 ## Adjusted      1193    1007 ##  - Time 2 ##            Control Treated ## Unadjusted    3701  3799.  ## Adjusted      1300   898.2 ##  - Time 3 ##            Control Treated ## Unadjusted    4886  2614.  ## Adjusted      1871   519.8 # Attach weights to dataset msmdata$weights <- Wmsm.out$weights  # Fit outcome model fit <- glm(Y_B ~ A_1 * A_2 * A_3 * (X1_0 + X2_0),            data = msmdata, weights = weights,            family = quasibinomial) library(\"marginaleffects\") (p <- avg_predictions(fit,                       vcov = \"HC3\",                       newdata = datagridcf(A_1 = 0:1, A_2 = 0:1, A_3 = 0:1),                       by = c(\"A_1\", \"A_2\", \"A_3\"),                       wts = \"weights\",                       type = \"response\")) ##  ##  A_1 A_2 A_3 Estimate Std. Error    z Pr(>|z|) 2.5 % 97.5 % ##    0   0   0    0.685     0.0169 40.6   <0.001 0.652  0.718 ##    0   0   1    0.518     0.0398 13.0   <0.001 0.440  0.596 ##    0   1   0    0.488     0.0217 22.5   <0.001 0.446  0.531 ##    0   1   1    0.434     0.0309 14.1   <0.001 0.373  0.494 ##    1   0   0    0.600     0.0218 27.6   <0.001 0.558  0.643 ##    1   0   1    0.541     0.0334 16.2   <0.001 0.475  0.606 ##    1   1   0    0.375     0.0170 22.0   <0.001 0.342  0.409 ##    1   1   1    0.419     0.0285 14.7   <0.001 0.363  0.475 ##  ## Columns: A_1, A_2, A_3, estimate, std.error, statistic, p.value, conf.low, conf.high hypotheses(p, \"b8 - b1 = 0\") ##  ##     Term Estimate Std. Error     z Pr(>|z|)  2.5 % 97.5 % ##  b8-b1=0   -0.266     0.0182 -14.6   <0.001 -0.301  -0.23 ##  ## Columns: term, estimate, std.error, statistic, p.value, conf.low, conf.high"},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/articles/estimating-effects.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Estimating Effects After Weighting","text":"assessing balance deciding weighting specification, comes time estimate effect treatment weighted sample. effect estimated interpreted depends desired estimand effect measure used. addition estimating effects, estimating uncertainty effects critical communicating assessing whether observed effect compatible effect population. guide explains estimate effects weighting point longitudinal treatments various outcome types. guide structured follows: first, information concepts related effect standard error (SE) estimation presented . , instructions estimate effects SEs described standard case (weighting ATE binary treatment continuous outcome) common circumstances. Finally, recommendations reporting results tips avoid making common mistakes presented. (Note: much vignette copied vignette(\"estimating-effects\", package = \"MatchIt\") adapted context weighting.)","code":""},{"path":"https://ngreifer.github.io/WeightIt/articles/estimating-effects.html","id":"identifying-the-estimand","dir":"Articles","previous_headings":"Introduction","what":"Identifying the estimand","title":"Estimating Effects After Weighting","text":"effect estimated, estimand must specified clarified. Although aspects estimand depend effect estimated weighting also weighting method , aspects must considered time effect estimation interpretation. , consider three aspects estimand: population effect meant generalize (target population), effect measure, whether effect marginal conditional. target population. Different weighting methods allow estimate effects can generalize different target populations. common estimand weighting average treatment effect population (ATE), average effect treatment population sample random sample. common estimands include average treatment effect treated (ATT), average treatment effect control (ATC), average treatment effect overlap (ATO). defined explained Greifer Stuart (2021). estimand weighting controlled estimand argument call weightit(). allowable estimands weighting methods include average treatment effect matched sample (ATM) average treatment effect optimal subset (ATOS). treated just like ATO differentiated . Marginal conditional effects. marginal effect comparison expected potential outcome treatment expected potential outcome control. quantity estimated randomized trials without blocking covariate adjustment particularly useful quantifying overall effect policy population-wide intervention. conditional effect comparison expected potential outcomes treatment groups within strata. useful identifying effect treatment individual patient subset population. Effect measures. outcome types consider continuous, effect measured mean difference; binary, effect measured risk difference (RD), risk ratio (RR), odds ratio (); time--event (.e., survival), effect measured hazard ratio (HR). RR, , HR noncollapsible effect measures, means marginal effect scale (possibly) weighted average conditional effects within strata, even stratum-specific effects magnitude. effect measures, critical distinguish marginal conditional effects different statistical methods target different types effects. mean difference RD collapsible effect measures, methods can used estimate marginal conditional effects. primary focus marginal effects, appropriate effect measures, easily interpretable, require modeling assumptions.","code":""},{"path":"https://ngreifer.github.io/WeightIt/articles/estimating-effects.html","id":"g-computation","dir":"Articles","previous_headings":"Introduction","what":"G-computation","title":"Estimating Effects After Weighting","text":"estimate marginal effects, use method known g-computation (Snowden, Rose, Mortimer 2011) regression estimation (Schafer Kang 2008). involves first specifying model outcome function treatment covariates. , unit, compute predicted values outcome setting treatment status treated, control, leaving us two predicted outcome values unit, estimates potential outcomes treatment level. compute mean estimated potential outcomes across entire sample, leaves us two average estimated potential outcomes. Finally, contrast average estimated potential outcomes (e.g., difference ratio, depending effect measure desired) estimate treatment effect. g-computation weighting, additional considerations required. First, outcome model fit incorporating estimated weights (e.g., using weighted least squares weighted maximum likelihood estimation). Second, take average estimated potential outcomes treatment level, must weighted average incorporates estimated weights. Third, want target ATT ATC, estimate potential outcomes treated control group, respectively (though still generate predicted values treatment control). G-computation framework estimating effects weighting number advantages approaches. works regardless form outcome model type outcome (e.g., whether linear model used continuous outcome logistic model used binary outcome); difference might average expected potential outcomes contrasted final step. simple cases, estimated effect numerically identical effects estimated using methods; example, covariates included outcome model, g-computation estimate equal difference means t-test coefficient treatment linear model outcome. analytic bootstrap approximations SEs g-computation estimate. analytic approximation computed using delta method, technique computing SE quantity derived regression model parameters, g-computation estimate. reasons , use weighted g-computation possible effect estimates, even simpler methods yield estimates. Using single workflow (slight modifications depending context; see ) facilitates implementing best practices regardless choices user makes. methods incorporate outcome model estimation treatment effect, best studied augmented inverse probability weighting (AIPW), also involves g-computation step. describe weighted g-computation conceptual simplicity, ease implementation, connection best practices estimating effects matching.","code":""},{"path":"https://ngreifer.github.io/WeightIt/articles/estimating-effects.html","id":"modeling-the-outcome","dir":"Articles","previous_headings":"Introduction","what":"Modeling the Outcome","title":"Estimating Effects After Weighting","text":"goal outcome model generate good predictions use g-computation procedure described . type form outcome model depend outcome type. continuous outcomes, one can use linear model regressing outcome treatment; binary outcomes, one can use generalized linear model , e.g., logistic link; time--event outcomes, one can use Cox proportional hazards model. additional decision make whether () include covariates outcome model. One may ask, use weighting going model outcome covariates anyway? Weighting reduces dependence effect estimate correct specification outcome model; central thesis Ho et al. (2007) (though applied matching case). Including covariates outcome model weighting several functions: can increase precision effect estimate, reduce bias due residual imbalance, make effect estimate “doubly robust”, means consistent either weighting reduces sufficient imbalance covariates outcome model correct. reasons, recommend covariate adjustment weighting possible. evidence covariate adjustment helpful covariates standardized mean differences greater .1 (Nguyen et al. 2017), covariates covariates thought highly predictive outcome prioritized treatment effect models can included due sample size constraints. Although many possible ways include covariates (e.g., just main effects interactions, smoothing terms like splines, nonlinear transformations), important engage specification search (.e., trying many outcomes models search “best” one). can invalidate results yield conclusion fails replicate. reason, recommend including terms included weighting model unless strong priori justifiable reason model outcome differently. important interpret coefficients tests covariates outcome model. causal effects estimates may severely confounded. treatment effect estimate can interpreted causal assuming relevant assumptions unconfoundedness met. Inappropriately interpreting coefficients covariates outcome model known Table 2 fallacy (Westreich Greenland 2013). avoid , display results g-computation procedure examine interpret outcome models .","code":""},{"path":"https://ngreifer.github.io/WeightIt/articles/estimating-effects.html","id":"estimating-standard-errors-and-confidence-intervals","dir":"Articles","previous_headings":"Introduction","what":"Estimating Standard Errors and Confidence Intervals","title":"Estimating Effects After Weighting","text":"Uncertainty estimation (.e., SEs, confidence intervals, p-values) may consider variety sources uncertainty present analysis, including (limited !) estimation propensity score (used) estimation treatment effect (.e., sampling error). methods, methods analytically computing correct asymptotic SE described. often require custom coding limited specific weighting methods estimands1. Instead, consider two approximations analytic SE: robust SEs bootstrap, described .","code":""},{"path":"https://ngreifer.github.io/WeightIt/articles/estimating-effects.html","id":"robust-and-cluster-robust-standard-errors","dir":"Articles","previous_headings":"Introduction > Estimating Standard Errors and Confidence Intervals","what":"Robust and Cluster-Robust Standard Errors","title":"Estimating Effects After Weighting","text":"Robust standard errors. Also known sandwich SEs (due form formula computing ), heteroscedasticity-consistent SEs, Huber-White SEs, robust SEs adjustment usual maximum likelihood ordinary least squares SEs robust violations assumptions required usual SEs valid (MacKinnon White 1985). Robust SEs shown conservative (.e., yield overly large SEs wide confidence intervals) estimating ATE forms weighting (Robins, Hernán, Brumback 2000), though can either conservative weighting methods estimands, ATT (Reifeis Hudgens 2020) entropy balancing (Chan, Yam, Zhang 2016). Robust SEs treat estimated weights fixed known, ignoring uncertainty estimation. Although quick simple estimate using functionality sandwich survey packages, used caution, bootstrap (described ) preferred cases.","code":""},{"path":"https://ngreifer.github.io/WeightIt/articles/estimating-effects.html","id":"bootstrapping","dir":"Articles","previous_headings":"Introduction > Estimating Standard Errors and Confidence Intervals","what":"Bootstrapping","title":"Estimating Effects After Weighting","text":"problems robust SEs include fail take account estimation weights approximation used compute derived quantities nonlinear models, often true using g-computation estimate effects. One solution problems bootstrapping, technique used simulate sampling distribution estimator repeatedly drawing samples replacement estimating effect bootstrap sample (Efron Tibshirani 1986). bootstrap distribution, SEs confidence intervals can computed several ways, including using standard deviation bootstrap estimates SE estimate using 2.5 97.5 percentiles 95% confidence interval bounds. Bootstrapping tends useful analytic estimator SE possible derived yet. Bootstrapping found effective estimating SEs confidence intervals weighting, often performing better even analytic asymptotic formula available (Austin 2022). Typically, bootstrapping involves performing entire estimation process bootstrap sample, including estimation weights effect. bootstrapping, bootstrap replications always better can take time increase chances least one error occur within bootstrap analysis (e.g., bootstrap sample zero treated units zero units event). general, numbers replications upwards 999 recommended, values one less multiple 100 preferred avoid interpolation using percentiles confidence interval limits (MacKinnon 2006). several methods computing bootstrap confidence intervals, bias-corrected accelerated (BCa) bootstrap confidence interval often performs well easy implement, simply setting type = \"bca\" call boot::boot.ci() running boot::boot()2.","code":""},{"path":"https://ngreifer.github.io/WeightIt/articles/estimating-effects.html","id":"estimating-treatment-effects-and-standard-errors-after-weighting","dir":"Articles","previous_headings":"","what":"Estimating Treatment Effects and Standard Errors After Weighting","title":"Estimating Effects After Weighting","text":", describe effect estimation weighting. focus evaluating methods simply demonstrating . cases, correct propensity score model used. present faster simpler method uses robust SEs complicated accurate method using bootstrapping. ’ll using simulated toy dataset d several outcome treatment types. Code generate dataset end document. display first six rows d: binary treatment variable, X1 X9 covariates, Y_C continuous outcome, Y_B binary outcome, Y_S survival outcome. Ac multi-category continuous treatment variables, respectively. need following packages perform analyses: marginaleffects provides avg_comparisons() function performing g-computation estimating SEs confidence intervals average estimate potential outcomes treatment effects sandwich used internally marginaleffects compute robust cluster-robust SEs survival provides coxph() estimate coefficients Cox-proportional hazards model marginal hazard ratio, use survival outcomes boot provides boot() boot.ci() performing bootstrapping computing bootstrap confidence intervals course, also need WeightIt perform weighting. Effect estimates computed using marginaleffects::avg_comparions(), even use may superfluous (e.g., comparing weighted difference means). previously mentioned, useful single workflow works matter situation, perhaps slight modifications accommodate different contexts. Using avg_comparisons() several advantages, even alternatives simple: provides effect estimate, coefficients; automatically incorporates robust SEs requested; always produces average marginal effects correct population requested. packages may use used . alternatives marginaleffects package computing average marginal effects, including margins stdReg. survey package can used estimate robust SEs incorporating weights provides functions survey-weighted generalized linear models Cox-proportional hazards models. Much code can adapted used survey, demonstrate well.","code":"head(d) ##   A Am      Ac      X1      X2      X3       X4 X5      X6      X7      X8       X9     Y_C Y_B    Y_S ## 1 0 C1 -2.2185  0.1725 -1.4283 -0.4103 -2.36059  1 -1.1199  0.6398 -0.4840 -0.59385 -3.5907   0  857.7 ## 2 0 C2 -2.2837 -1.0959  0.8463  0.2456 -0.12333  1 -2.2687 -1.4491 -0.5514 -0.31439 -1.5481   0  311.6 ## 3 0 C1 -1.1362  0.1768  0.7905 -0.8436  0.82366  1 -0.2221  0.2971 -0.6966 -0.69516  6.0714   0  241.2 ## 4 0 C1 -0.8865 -0.4595  0.1726  1.9542 -0.62661  1 -0.4019 -0.8294 -0.5384  0.20729  2.4906   1  142.4 ## 5 1  T  0.8613  0.3563 -1.8121  0.8135 -0.67189  1 -0.8297  1.7297 -0.6439 -0.02648 -0.6687   0  206.8 ## 6 0 C2 -2.1697 -2.4313 -1.7984 -1.2940  0.04609  1 -1.2419 -1.1252 -1.8659 -0.56513 -9.8504   0 1962.9 library(\"WeightIt\") library(\"marginaleffects\")"},{"path":"https://ngreifer.github.io/WeightIt/articles/estimating-effects.html","id":"the-standard-case-binary-treatment-with-robust-ses","dir":"Articles","previous_headings":"Estimating Treatment Effects and Standard Errors After Weighting","what":"The Standard Case: Binary Treatment with Robust SEs","title":"Estimating Effects After Weighting","text":"weighting methods, estimating effect weighting straightforward involves fitting model outcome incorporates estimated weights, estimating treatment effect using g-computation (.e., using marginaleffects::avg_comparisons()) robust SE account pair membership. procedure continuous binary outcomes without covariates. adjustments need made certain scenarios, describe section “Adjustments Standard Case”. adjustments include following cases: weighting ATT ATC, estimating effects binary outcomes, estimating effects survival outcomes. Estimation estimands ATT ATC proceeds ATE. must read Standard Case understand basic procedure reading special scenarios. also demonstrate estimate effects multi-category continuous treatments. , demonstrate faster analytic approach estimating confidence intervals; bootstrap approach, see section “Using Bootstrapping Estimate Confidence Intervals” . First, perform propensity score weighting ATE. Remember, weighting methods use exact procedure slight variation, section critical even using different weighting method. Typically one assess balance ensure weighting specification works, skip step focus effect estimation. See vignette(\"WeightIt\") vignette(\"cobalt\", package = \"cobalt\") information necessary step. First, fit model outcome given treatment (optionally) covariates. ’s usually good idea include treatment-covariate interactions, , always necessary, especially excellent balance achieved. Next, use marginaleffects::avg_comparisons() estimate ATE. Let’s break call avg_comparisons(): first argument, supply model fit, fit; variables argument, name treatment (\"\"); vcov argument, name robust SE want request (case, HC3, perform well relative robust SEs); wts argument, names variable d containing matching weights (\"weights\") ensure included analysis. arguments differ depending specifics outcome type; see sections information. , addition effect estimate, want average estimated potential outcomes, can use marginaleffects::avg_predictions(), demonstrate . Note interpretation resulting estimates expected potential outcomes valid covariates present outcome model () interacted treatment. can see difference potential outcome means equal average treatment effect computed previously3. arguments avg_predictions() avg_comparisons().","code":"#PS weighting for the ATE with a logistic regression PS W <- weightit(A ~ X1 + X2 + X3 + X4 + X5 +                  X6 + X7 + X8 + X9, data = d,               method = \"glm\", estimand = \"ATE\") W ## A weightit object ##  - method: \"glm\" (propensity score weighting with GLM) ##  - number of obs.: 2000 ##  - sampling weights: none ##  - treatment: 2-category ##  - estimand: ATE ##  - covariates: X1, X2, X3, X4, X5, X6, X7, X8, X9 #Bring weights into the dataset d$weights <- W$weights  #Linear model with covariates fit <- lm(Y_C ~ A * (X1 + X2 + X3 + X4 + X5 +                          X6 + X7 + X8 + X9),            data = d, weights = weights) avg_comparisons(fit,                 variables = \"A\",                 vcov = \"HC3\",                 wts = \"weights\") ##  ##  Term Contrast Estimate Std. Error   z Pr(>|z|) 2.5 % 97.5 % ##     A    1 - 0     1.81      0.477 3.8   <0.001 0.875   2.74 ##  ## Columns: term, contrast, estimate, std.error, statistic, p.value, conf.low, conf.high avg_predictions(fit,                 variables = \"A\",                 vcov = \"HC3\",                 wts = \"weights\") ##  ##  A Estimate Std. Error     z Pr(>|z|) 2.5 % 97.5 % ##  0     1.42      0.136 10.44   <0.001  1.15   1.69 ##  1     3.17      0.380  8.35   <0.001  2.42   3.91 ##  ## Columns: A, estimate, std.error, statistic, p.value, conf.low, conf.high"},{"path":"https://ngreifer.github.io/WeightIt/articles/estimating-effects.html","id":"adjustments-to-the-standard-case","dir":"Articles","previous_headings":"Estimating Treatment Effects and Standard Errors After Weighting","what":"Adjustments to the Standard Case","title":"Estimating Effects After Weighting","text":"section explains procedure might differ following special circumstances occur.","code":""},{"path":"https://ngreifer.github.io/WeightIt/articles/estimating-effects.html","id":"weighting-for-the-att-or-atc","dir":"Articles","previous_headings":"Estimating Treatment Effects and Standard Errors After Weighting > Adjustments to the Standard Case","what":"Weighting for the ATT or ATC","title":"Estimating Effects After Weighting","text":"weighting ATT, everything identical Standard Case except calls avg_comparisons() avg_predictions(), newdata argument must additionally supplied avg_comparisons() avg_predictions() requests g-computation done treated units. ATC, replace 1 0.","code":"newdata = subset(d, A == 1)"},{"path":"https://ngreifer.github.io/WeightIt/articles/estimating-effects.html","id":"binary-outcomes","dir":"Articles","previous_headings":"Estimating Treatment Effects and Standard Errors After Weighting > Adjustments to the Standard Case","what":"Binary outcomes","title":"Estimating Effects After Weighting","text":"Estimating effects binary outcomes essentially continuous outcomes. main difference several measures effect one can consider, include odds ratio (), risk ratio/relative risk (RR), risk difference (RD), syntax avg_comparisons() depends one desired. outcome model one appropriate binary outcomes (e.g., logistic regression) unrelated desired effect measure can compute effect measures using avg_comparisons() logistic regression. fit logistic regression model, change lm() glm() set family = quasibinomial4. compute marginal RD, can use exactly syntax Standard Case; nothing needs change5. compute marginal RR, need add comparison = \"lnratioavg\" avg_comparisons(); computes marginal log RR. get marginal RR, need add transform = \"exp\" avg_comparisons(), exponentiates marginal log RR confidence interval. code computes effects displays statistics interest: output displays marginal RR, Z-value, p-value Z-test log RR 0, confidence interval. (Note even though Contrast label still suggests log RR, RR actually displayed.) view log RR standard error, omit transform argument. marginal , thing needs change comparison set \"lnoravg\".","code":"#Logistic regression model with covariates fit <- glm(Y_B ~ A * (X1 + X2 + X3 + X4 + X5 +                          X6 + X7 + X8 + X9),            data = d, weights = weights,            family = quasibinomial)  #Compute effects; RR and confidence interval avg_comparisons(fit,                 variables = \"A\",                 vcov = \"HC3\",                 wts = \"weights\",                 comparison = \"lnratioavg\",                 transform = \"exp\") ##  ##  Term              Contrast Estimate Pr(>|z|) 2.5 % 97.5 % ##     A ln(mean(1) / mean(0))     1.71   <0.001  1.42   2.05 ##  ## Columns: term, contrast, estimate, p.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo"},{"path":"https://ngreifer.github.io/WeightIt/articles/estimating-effects.html","id":"survival-outcomes","dir":"Articles","previous_headings":"Estimating Treatment Effects and Standard Errors After Weighting > Adjustments to the Standard Case","what":"Survival outcomes","title":"Estimating Effects After Weighting","text":"several measures effect size survival outcomes, described Mao et al. (2018). using Cox proportional hazards model, quantity interest hazard ratio (HR) treated control groups. , HR non-collapsible, means estimated HR valid estimate marginal HR covariates included model. effect measures, difference mean survival times probability survival given time, can treated just like continuous binary outcomes previously described. HR, compute average marginal effects must use coefficient treatment Cox model fit without covariates. means use procedures Standard Case. describe estimating marginal HR using coxph() survival package. (See help(\"coxph\", package = \"survival\") information model.) Robust SEs HRs studied Austin (2016) found conservative; requested automatically weights supplied coxph(). formulas developed estimating standard errors accurately (Mao et al. 2018; Hajage et al. 2018), though Austin (2016) also found bootstrap adequate. coef column contains log HR, exp(coef) contains HR. Remember always use robust se SE log HR. displayed z-test p-value results using robust SE.","code":"library(\"survival\")  #Cox Regression for marginal HR coxph(Surv(Y_S) ~ A, data = d, weights = weights) ## Call: ## coxph(formula = Surv(Y_S) ~ A, data = d, weights = weights) ##  ##   coef exp(coef) se(coef) robust se z     p ## A 0.39      1.47     0.03      0.10 4 6e-05 ##  ## Likelihood ratio test=151  on 1 df, p=<2e-16 ## n= 2000, number of events= 2000"},{"path":"https://ngreifer.github.io/WeightIt/articles/estimating-effects.html","id":"using-the-survey-package","dir":"Articles","previous_headings":"Estimating Treatment Effects and Standard Errors After Weighting > Adjustments to the Standard Case","what":"Using the survey package","title":"Estimating Effects After Weighting","text":"survey package often recommended use estimating treatment effects propensity score weighting. combined marginaleffects functions, yields identical estimates similar standard errors methods described use lm() glm() manually request robust SE. main benefits using survey don’t need request robust SEs call avg_comparisons() can incorporate survey design information, e.g., study comes complex survey. adjust standard case use survey instead: Note need supply \"(weights)\" wts argument correctly incorporated. might notice standard errors computed using survey different using lm() glm() HC3 robust SEs; get similar SEs using lm() glm(), set vcov = \"HC0\" call avg_comparisons(). approach recommended HC3 standard errors tend perform better, one reason avoid using survey. many cases, though, results similar.","code":"library(\"survey\")  #Declare a survey design using the estimated weights des <- svydesign(~1, weights = ~weights, data = d)  #Fit the outcome model fit <- svyglm(Y_C ~ A * (X1 + X2 + X3 + X4 + X5 +                             X6 + X7 + X8 + X9),               design = des)  #G-computation for the difference in means avg_comparisons(fit,                 variables = \"A\",                 wts = \"(weights)\") ##  ##  Term Contrast Estimate Std. Error    z Pr(>|z|) 2.5 % 97.5 % ##     A    1 - 0     1.81      0.356 5.08   <0.001  1.11   2.51 ##  ## Columns: term, contrast, estimate, std.error, statistic, p.value, conf.low, conf.high"},{"path":"https://ngreifer.github.io/WeightIt/articles/estimating-effects.html","id":"multi-category-treatments","dir":"Articles","previous_headings":"Estimating Treatment Effects and Standard Errors After Weighting","what":"Multi-Category Treatments","title":"Estimating Effects After Weighting","text":"Multi-category treatments work essentially way binary treatments. main practical differences choosing estimand estimating weights. ATE ATO straightforward. ATT requires choosing one group treated “focal” group. Effects estimated members group. contrast focal group expected potential outcomes non-focal treatment expected potential outcomes focal (actual) treatment can interpreted similarly ATTs interpreted binary treatments. contrasts focal group expected potential outcomes pair non-focal treatments can interpreted contrast ATTs non-focal treatments. Weights may estimated differently multi-category treatments binary treatments; see individual methods pages differ. , ’ll estimate ATTs multi-category treatment focal level. focal treatment group, \"T\", two control groups \"C1\" \"C2\". expect ATTs two control groups since assigned randomly within original control group. First, estimate weights using entropy balancing (Hainmueller 2012), identifying focal group using focal: Typically one assess performance weights (balance effective sample size) skip now. Next, fit outcome model perform weighted g-computation. use avg_predictions() first compute expected potential outcome treatment focal group, use hypotheses() test pairwise comparisons. find significant ATTs focal treatment control levels (T - C1 T - C2), difference control levels (C2 - C1), can interpreted difference ATTs, nonsignificant, expected. Remember, ATT, bootstrapping usually provides valid inference robust SEs used .","code":"table(d$Am) ##  ##  C1  C2   T  ## 751 801 448 W <- weightit(Am ~ X1 + X2 + X3 + X4 + X5 +                  X6 + X7 + X8 + X9, data = d,               method = \"ebal\", estimand = \"ATT\",               focal = \"T\") W ## A weightit object ##  - method: \"ebal\" (entropy balancing) ##  - number of obs.: 2000 ##  - sampling weights: none ##  - treatment: 3-category (C1, C2, T) ##  - estimand: ATT (focal: T) ##  - covariates: X1, X2, X3, X4, X5, X6, X7, X8, X9 #Bring weights into the dataset d$weights <- W$weights  #Fit the outcome model fit <- lm(Y_C ~ Am * (X1 + X2 + X3 + X4 + X5 +                          X6 + X7 + X8 + X9),           data = d, weights = weights)  #G-computation p <- avg_predictions(fit,                      variables = \"Am\",                      vcov = \"HC3\",                      newdata = subset(boot_data, A == 1),                      wts = \"weights\") p ##  ##  Am Estimate Std. Error    z Pr(>|z|) 2.5 % 97.5 % ##  C1     1.73      0.275 6.27   <0.001 1.187   2.27 ##  C2     1.40      0.233 6.00   <0.001 0.942   1.86 ##  T      2.98      0.320 9.30   <0.001 2.350   3.60 ##  ## Columns: Am, estimate, std.error, statistic, p.value, conf.low, conf.high hypotheses(p, \"revpairwise\") ##  ##     Term Estimate Std. Error      z Pr(>|z|)  2.5 % 97.5 % ##  C2 - C1   -0.327      0.353 -0.925  0.35479 -1.020  0.366 ##  T - C1     1.251      0.410  3.055  0.00225  0.448  2.054 ##  T - C2     1.578      0.396  3.984  < 0.001  0.802  2.355 ##  ## Columns: term, estimate, std.error, statistic, p.value, conf.low, conf.high"},{"path":"https://ngreifer.github.io/WeightIt/articles/estimating-effects.html","id":"continuous-treatments","dir":"Articles","previous_headings":"Estimating Treatment Effects and Standard Errors After Weighting","what":"Continuous Treatments","title":"Estimating Effects After Weighting","text":"continuous treatments, one estimand interest average dose-response function (ADRF), links value treatment expected potential outcome treatment value across full sample. provide detailed account ADRF demonstrate estimate weights balance covariates respect continuous treatment estimate plot ADRF weighted sample. ’ll use continuous treatment Ac, ranges -10.37 6.26, estimate effect continuous outcome Y_C. First, ’ll estimate distance covariance optimal weights (Huling, Greifer, Chen 2023) using weightit(). Typically one assess performance weights (balance effective sample size) skip now. Next, fit outcome model perform weighted g-computation. outcome model, use natural cubic spline Ac 4 df. can use transformation treatment, e.g., polynomial transformation using poly(), long flexible enough capture possible ADRF; purely nonparametric methods like kernel regression can used well, inference challenging. Covariates can included model, many covariates, many basis functions treatment, full set treatment-covariate interactions, resulting estimates may imprecise. Next use avg_predictions() first compute expected potential outcome representative set treatment values. ’ll examine 51 treatment values 10th 90th percentiles Ac estimates outside ranges tend imprecise. Although one can examine expected potential outcomes, often useful see plotted. can generate plot ADRF pointwise confidence band using ggplot2:  can see ADRF elbow-shape, zone evidence treatment effect followed zone increasing values outcome treatment increases. Another way characterize effect continuous treatments examine average marginal effect function (AMEF), function relates value treatment derivative ADRF. derivative different zero, evidence treatment effect corresponding value treatment. , use avg_slopes() compute pointwise derivatives ADRF across levels Ac plot .  can see values -1 .5, evidence positive effect Ac Y_C (.e., confidence intervals slope ADRF values Ac exclude 0). line observation treatment appears effect higher values Ac.","code":"W <- weightit(Ac ~ X1 + X2 + X3 + X4 + X5 +                  X6 + X7 + X8 + X9, data = d,               method = \"energy\") W ## A weightit object ##  - method: \"energy\" (energy balancing) ##  - number of obs.: 2000 ##  - sampling weights: none ##  - treatment: continuous ##  - covariates: X1, X2, X3, X4, X5, X6, X7, X8, X9 #Bring weights into the dataset d$weights <- W$weights  #Fit the outcome model fit <- lm(Y_C ~ splines::ns(Ac, df = 4) *             (X1 + X2 + X3 + X4 + X5 +                 X6 + X7 + X8 + X9),           data = d, weights = weights) #Represenative values of Ac: values <- with(d, seq(quantile(Ac, .1),                       quantile(Ac, .9),                       length.out = 51))  #G-computation p <- avg_predictions(fit,                      variables = list(Ac = values),                      vcov = \"HC3\",                      wts = \"weights\") library(\"ggplot2\") ggplot(p, aes(x = Ac)) +   geom_line(aes(y = estimate)) +   geom_ribbon(aes(ymin = conf.low, ymax = conf.high),               alpha = .3) +   labs(x = \"Ac\", y = \"E[Y|A]\") +   theme_bw() # Estimate the pointwise derivatives at representative # values of Ac s <- avg_slopes(fit,                 variables = \"Ac\",                 newdata = datagridcf(Ac = values),                 by = \"Ac\",                 vcov = \"HC3\",                 wts = \"weights\")  # Plot the AMEF ggplot(s, aes(x = Ac)) +   geom_line(aes(y = estimate)) +   geom_ribbon(aes(ymin = conf.low, ymax = conf.high),               alpha = .3) +   geom_hline(yintercept = 0, linetype = \"dashed\") +   labs(x = \"Ac\", y = \"dE[Y|A]/dA\") +   theme_bw()"},{"path":"https://ngreifer.github.io/WeightIt/articles/estimating-effects.html","id":"longitudinal-treatments","dir":"Articles","previous_headings":"Estimating Treatment Effects and Standard Errors After Weighting","what":"Longitudinal Treatments","title":"Estimating Effects After Weighting","text":"Coming soon! now see example vignette(\"WeightIt\").","code":""},{"path":"https://ngreifer.github.io/WeightIt/articles/estimating-effects.html","id":"using-bootstrapping-to-estimate-confidence-intervals","dir":"Articles","previous_headings":"Estimating Treatment Effects and Standard Errors After Weighting","what":"Using Bootstrapping to Estimate Confidence Intervals","title":"Estimating Effects After Weighting","text":"bootstrap alternative delta method estimating confidence intervals estimated effects. See section Bootstrapping details. , ’ll demonstrate standard bootstrap, involves resampling units estimating weights treatment effect within bootstrap sample. , use functionality boot package. critical set seed using set.seed() prior performing bootstrap order results replicable. standard bootstrap, need function takes original dataset vector sampled unit indices returns estimated quantity interest. function estimate weights bootstrap sample, fit outcome model, estimate treatment effect using g-computation. ’ll consider marginal RR ATT binary outcome Y_B. first step write estimation function, call boot_fun. function returns marginal RR. , estimate weights effect return estimate interest. Next, call boot::boot() function original dataset supplied perform bootstrapping. ’ll request 199 bootstrap replications , practice use many , upwards 999. always better. Using also allows use bias-corrected accelerated (BCa) bootstrap confidence intervals (can request setting type = \"bca\" call boot.ci()), known accurate. See ?boot.ci details. , ’ll just use percentile confidence interval. find RR 1.477 confidence interval (1.274, 1.693). wanted risk difference, adjusted arguments avg_comparisons() (.e., removing comparison transform arguments). estimate bootstrap confidence HR, can except boot_fun look like following: (Remember need load survival package .)","code":"boot_fun <- function(data, i) {   boot_data <- data[i,]      #PS weighting for the ATE   W <- weightit(A ~ X1 + X2 + X3 + X4 + X5 +                   X6 + X7 + X8 + X9,                data = boot_data,                method = \"glm\", estimand = \"ATT\")      #Bring weights into the dataset   boot_data$weights <- W$weights      #Fit outcome model   fit <- glm(Y_B ~ A * (X1 + X2 + X3 + X4 + X5 +                   X6 + X7 + X8 + X9),              data = boot_data, weights = weights,              family = quasibinomial)      #G-computation   comp <- avg_comparisons(fit,                           variables = \"A\",                           vcov = FALSE,                           newdata = subset(boot_data, A == 1),                           wts = \"weights\",                           comparison = \"lnratioavg\",                           transform = \"exp\")      comp$estimate } library(\"boot\") set.seed(54321) boot_out <- boot(d, boot_fun, R = 199)  boot_out ##  ## ORDINARY NONPARAMETRIC BOOTSTRAP ##  ##  ## Call: ## boot(data = d, statistic = boot_fun, R = 199) ##  ##  ## Bootstrap Statistics : ##     original    bias    std. error ## t1*    1.477 -0.008648      0.1108 boot.ci(boot_out, type = \"perc\") ## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS ## Based on 199 bootstrap replicates ##  ## CALL :  ## boot.ci(boot.out = boot_out, type = \"perc\") ##  ## Intervals :  ## Level     Percentile      ## 95%   ( 1.274,  1.693 )   ## Calculations and Intervals on Original Scale ## Some percentile intervals may be unstable boot_fun <- function(data, i) {   boot_data <- data[i,]      #PS weighting for the ATE   W <- weightit(A ~ X1 + X2 + X3 + X4 + X5 +                   X6 + X7 + X8 + X9,                data = boot_data,                method = \"glm\", estimand = \"ATT\")      #Bring weights into the dataset   boot_data$weights <- W$weights      #Fit outcome model   fit <- coxph(Surv(Y_S) ~ A, data = boot_data,                weights = weights)      #Return the coefficient on treatment   coef(fit)[\"A\"] }"},{"path":"https://ngreifer.github.io/WeightIt/articles/estimating-effects.html","id":"reporting-results","dir":"Articles","previous_headings":"","what":"Reporting Results","title":"Estimating Effects After Weighting","text":"important thorough complete possible describing methods estimating treatment effect results analysis. improves transparency replicability analysis. Results least include following: description outcome model used (e.g., logistic regression, linear model treatment-covariate interactions covariates, Cox proportional hazards model propensity score weights applied) way effect estimated (e.g., using g-computation coefficient outcome model) way SEs confidence intervals estimated (e.g., using robust SEs, using BCa bootstrap 4999 bootstrap replications entire process weighting effect estimation included replication) R packages functions used estimating effect SE (e.g., glm() base R, avg_comparisons() marginaleffects, boot() boot.ci() boot) effect SE confidence interval addition information weighting method, estimand, propensity score estimation procedure (used), balance assessment, etc.","code":""},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/articles/estimating-effects.html","id":"code-to-generate-data-used-in-examples","dir":"Articles","previous_headings":"","what":"Code to Generate Data used in Examples","title":"Estimating Effects After Weighting","text":"","code":"#Generating data similar to Austin (2009) for demonstrating treatment effect estimation gen_X <- function(n) {   X <- matrix(rnorm(9 * n), nrow = n, ncol = 9)   X[,5] <- as.numeric(X[,5] < .5)   X }  gen_Ac <- function(X) {   LP_A <- -1.2 + log(2)*X[,1] - log(1.5)*X[,2] + log(2)*X[,4] - log(2.4)*X[,5] + log(2)*X[,7] - log(1.5)*X[,8]   LP_A + rlogis(nrow(X)) }  #~20% treated gen_A <- function(Ac) {   1 * (Ac > 0) }  gen_Am <- function(A) {   factor(ifelse(A == 1, \"T\", sample(c(\"C1\", \"C2\"), length(A), TRUE))) }  # Continuous outcome gen_Y_C <- function(A, X) {   2*A + 2*X[,1] + 2*X[,2] + 2*X[,3] + 1*X[,4] + 2*X[,5] + 1*X[,6] + rnorm(length(A), 0, 5) } #Conditional: #  MD: 2 #Marginal: #  MD: 2  # Binary outcome gen_Y_B <- function(A, X) {   LP_B <- -2 + log(2.4)*A + log(2)*X[,1] + log(2)*X[,2] + log(2)*X[,3] + log(1.5)*X[,4] + log(2.4)*X[,5] + log(1.5)*X[,6]   P_B <- plogis(LP_B)   rbinom(length(A), 1, P_B) } #Conditional: #  OR:   2.4 #  logOR: .875 #Marginal: #  RD:    .144 #  RR:   1.54 #  logRR: .433 #  OR:   1.92 #  logOR  .655  # Survival outcome gen_Y_S <- function(A, X) {   LP_S <- -2 + log(2.4)*A + log(2)*X[,1] + log(2)*X[,2] + log(2)*X[,3] + log(1.5)*X[,4] + log(2.4)*X[,5] + log(1.5)*X[,6]   sqrt(-log(runif(length(A)))*2e4*exp(-LP_S)) } #Conditional: #  HR:   2.4 #  logHR: .875 #Marginal: #  HR:   1.57 #  logHR: .452  set.seed(19599)  n <- 2000 X <- gen_X(n) Ac <- gen_Ac(X) A <- gen_A(Ac) Am <- gen_Am(A)  Y_C <- gen_Y_C(A, X) Y_B <- gen_Y_B(A, X) Y_S <- gen_Y_S(A, X)  d <- data.frame(A, Am, Ac, X, Y_C, Y_B, Y_S)"},{"path":"https://ngreifer.github.io/WeightIt/articles/installing-packages.html","id":"propensity-score-weighting-using-glms-method-glm","dir":"Articles","previous_headings":"","what":"Propensity score weighting using GLMs (method = \"glm\")","title":"Installing Supporting Packages","text":"Several options available estimating propensity score weights using GLMs depending treatment type features desired model. binary treatments, weightit() uses stats::glm() default, continuous treatments, weightit() uses stats::lm() default, additional packages required. multi-category treatments use.mlogit = FALSE, weightit() uses stats::glm().","code":""},{"path":"https://ngreifer.github.io/WeightIt/articles/installing-packages.html","id":"binary-and-continuous-treatments-with-missing-saem","dir":"Articles","previous_headings":"Propensity score weighting using GLMs (method = \"glm\")","what":"Binary and continuous treatments with missing = \"saem\"","title":"Installing Supporting Packages","text":"binary continuous treatments, missing data present missing = \"saem\" supplied, misaem package required. install misaem CRAN, run misaem CRAN, want install development version source, can developer’s GitHub repo using following code:","code":"install.packages(\"misaem\") remotes::install_github(\"julierennes/misaem\")"},{"path":"https://ngreifer.github.io/WeightIt/articles/installing-packages.html","id":"binary-and-multicategory-treatments-with-link-br-logit","dir":"Articles","previous_headings":"Propensity score weighting using GLMs (method = \"glm\")","what":"Binary and multicategory treatments with link = \"br.logit\"","title":"Installing Supporting Packages","text":"binary multicategory treatments, link supplied \"br.logit\" another link beginning \"br.\", brglm2 package required. install brglm2 CRAN, run brglm2 CRAN, want install development version source, can developer, Ioannis Kosmidis’s, GitHub repo using following code: brglm2 requires compilation, means may need additional software installed computer install source.","code":"install.packages(\"brglm2\") remotes::install_github(\"ikosmidis/brglm2\")"},{"path":"https://ngreifer.github.io/WeightIt/articles/installing-packages.html","id":"multicategory-treatments-with-use-mlogit-true","dir":"Articles","previous_headings":"Propensity score weighting using GLMs (method = \"glm\")","what":"Multicategory treatments with use.mlogit = TRUE","title":"Installing Supporting Packages","text":"multicategory treatments, use.mlogit = TRUE (default), mlogit package required multinomial logistic regression. install mlogit CRAN, run mlogit CRAN, want install development version source, can developer, Yves Croissant’s, R-Forge repo using following code:","code":"install.packages(\"mlogit\") install.packages(\"mlogit\", repos = \"http://R-Forge.R-project.org\")"},{"path":"https://ngreifer.github.io/WeightIt/articles/installing-packages.html","id":"multicategory-treatments-with-use-mclogit-true","dir":"Articles","previous_headings":"Propensity score weighting using GLMs (method = \"glm\")","what":"Multicategory treatments with use.mclogit = TRUE","title":"Installing Supporting Packages","text":"multicategory treatments, use.mclogit = TRUE, mclogit package required multinomial logistic regression. install mclogit CRAN, run mclogit CRAN, want install development version source, can developer, Martin Elff’s, GitHub repo using following code:","code":"install.packages(\"mclogit\") remotes::install_github(\"melff/mclogit\")"},{"path":"https://ngreifer.github.io/WeightIt/articles/installing-packages.html","id":"multicategory-treatments-with-link-bayes-probit","dir":"Articles","previous_headings":"Propensity score weighting using GLMs (method = \"glm\")","what":"Multicategory treatments with link = \"bayes.probit\"","title":"Installing Supporting Packages","text":"multicategory treatments, link = \"bayes.probit\", MNP package required Bayesian multinomial probit regression. install MNP CRAN, run MNP CRAN, want install development version source, can developer, Kosuke Imai’s, GitHub repo using following code: MNP requires compilation, means may need additional software installed computer install source.","code":"install.packages(\"MNP\") remotes::install_github(\"kosukeimai/MNP\")"},{"path":"https://ngreifer.github.io/WeightIt/articles/installing-packages.html","id":"ordinal-multicategory-treatments","dir":"Articles","previous_headings":"Propensity score weighting using GLMs (method = \"glm\")","what":"Ordinal multicategory treatments","title":"Installing Supporting Packages","text":"multicategory ordinal treatments (.e., treatment variable ordered factor), MASS package required unless link = \"br.logit\" (case brglm2 package needed; see ). MASS core package can always installed CRAN using code :","code":"install.packages(\"MASS\")"},{"path":"https://ngreifer.github.io/WeightIt/articles/installing-packages.html","id":"propensity-score-weighting-using-gbm-method-gbm","dir":"Articles","previous_headings":"","what":"Propensity Score weighting using GBM (method = \"gbm\")","title":"Installing Supporting Packages","text":"WeightIt uses R package gbm estimate propensity score weights using GBM. rely twang package . install gbm CRAN, run gbm CRAN, want install development version source, can developer’s GitHub repo using following code: gbm requires compilation, means may need additional software installed computer install source.","code":"install.packages(\"gbm\") remotes::install_github(\"gbm-developers/gbm\")"},{"path":"https://ngreifer.github.io/WeightIt/articles/installing-packages.html","id":"covariate-balancing-propensity-score-weighting-method-cbps-and-method-npcbps","dir":"Articles","previous_headings":"","what":"Covariate Balancing Propensity Score weighting (method = \"cbps\" and method = \"npcbps\")","title":"Installing Supporting Packages","text":"WeightIt uses R package CBPS perform (nonparametric) covariate balancing propensity score weighting. install CBPS CRAN, run CBPS CRAN, want install development version source, can developer, Kosuke Imai’s, GitHub repo using following code:","code":"install.packages(\"CBPS\") remotes::install_github(\"kosukeimai/CBPS\")"},{"path":"https://ngreifer.github.io/WeightIt/articles/installing-packages.html","id":"entropy-balancing-method-ebal","dir":"Articles","previous_headings":"","what":"Entropy balancing (method = \"ebal\")","title":"Installing Supporting Packages","text":"WeightIt uses code written WeightIt, additional packages need installed use entropy balancing.","code":""},{"path":"https://ngreifer.github.io/WeightIt/articles/installing-packages.html","id":"empirical-balancing-calibration-weighting-method-ebcw","dir":"Articles","previous_headings":"","what":"Empirical balancing calibration weighting (method = \"ebcw\")","title":"Installing Supporting Packages","text":"Empirical balancing calibration weighting longer available WeightIt. Use entropy balancing, cases mathematically identical.","code":""},{"path":"https://ngreifer.github.io/WeightIt/articles/installing-packages.html","id":"optimization-based-weighting-method-optweight","dir":"Articles","previous_headings":"","what":"Optimization-based weighting (method = \"optweight\")","title":"Installing Supporting Packages","text":"WeightIt uses R package optweight perform optimization-based weighting. install optweight CRAN, run optweight CRAN, want install development version source, can developer, Noah Greifer’s (), GitHub repo using following code: optweight depends osqp package, requires compilation, means may need additional software installed computer install source.","code":"install.packages(\"optweight\") remotes::install_github(\"ngreifer/optweight\")"},{"path":"https://ngreifer.github.io/WeightIt/articles/installing-packages.html","id":"propensity-score-weighting-using-superlearner-method-super","dir":"Articles","previous_headings":"","what":"Propensity score weighting using SuperLearner (method = \"super\")","title":"Installing Supporting Packages","text":"WeightIt uses R package SuperLearner estimate propensity score weights using SuperLearner. install SuperLearner CRAN, run SuperLearner CRAN, want install development version source, can developer, Eric Polley’s, GitHub repo using following code: SuperLearner wrapper many packages. whole point using SuperLearner include many different machine learning algorithms combine well-fitting stacked model. algorithms exist many different R packages, need installed use . See Suggested packages SuperLearner CRAN page see packages might used SuperLearner. additional functions use SuperLearner SuperLearnerExtra repository. read R session used method = \"super\", use source() raw text file URL. example, read code SL.dbarts, run","code":"install.packages(\"SuperLearner\") remotes::install_github(\"ecpolley/SuperLearner\") source(\"https://raw.githubusercontent.com/ecpolley/SuperLearnerExtra/master/SL/SL.dbarts.R\")"},{"path":"https://ngreifer.github.io/WeightIt/articles/installing-packages.html","id":"propensity-score-weighting-using-bart-method-bart","dir":"Articles","previous_headings":"","what":"Propensity score weighting using BART (method = \"bart\")","title":"Installing Supporting Packages","text":"WeightIt uses R package dbarts estimate propensity score weights using BART. install dbarts CRAN, run dbarts CRAN, want install development version source, can developer, Vincent Dorie’s, GitHub repo using following code: dbarts requires compilation, means may need additional software installed computer install source.","code":"install.packages(\"dbarts\") remotes::install_github(\"vdorie/dbarts\")"},{"path":"https://ngreifer.github.io/WeightIt/articles/installing-packages.html","id":"energy-balancing-method-energy","dir":"Articles","previous_headings":"","what":"Energy Balancing (method = \"energy\")","title":"Installing Supporting Packages","text":"WeightIt uses R package osqp perform optimization required energy balancing. install osqp CRAN, run osqp CRAN, want install development version source, can developer’s site using instructions given , though bit involved installations source.","code":"install.packages(\"osqp\")"},{"path":"https://ngreifer.github.io/WeightIt/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Noah Greifer. Author, maintainer.","code":""},{"path":"https://ngreifer.github.io/WeightIt/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Greifer N (2023). WeightIt: Weighting Covariate Balance Observational Studies. https://ngreifer.github.io/WeightIt/, https://github.com/ngreifer/WeightIt.","code":"@Manual{,   title = {WeightIt: Weighting for Covariate Balance in Observational Studies},   author = {Noah Greifer},   year = {2023},   note = {https://ngreifer.github.io/WeightIt/, https://github.com/ngreifer/WeightIt}, }"},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/index.html","id":"overview","dir":"","previous_headings":"","what":"Overview","title":"Weighting for Covariate Balance in Observational Studies","text":"WeightIt one-stop package generate balancing weights point longitudinal treatments observational studies. Contained within WeightIt methods call R packages estimate weights. value WeightIt unified familiar syntax used generate weights, packages , often challenging navigate, syntax. WeightIt extends capabilities packages generate weights used estimate ATE, ATT, ATC, estimands binary multinomial treatments, treatment effects continuous treatments available. ways, WeightIt weighting MatchIt done matching, MatchIt users find syntax familiar. complete vignette, see website WeightIt vignette(\"WeightIt\"). install load WeightIt, use code : workhorse function WeightIt weightit(), generates weights given formula data input according methods parameters specified user. example use weightit() generate propensity score weights estimating ATE: Evaluating weights two components: evaluating covariate balance produced weights, evaluating whether weights allow sufficient precision eventual effect estimate. first goal, functions cobalt package, fully compatible WeightIt, can used, demonstrated : second goal, qualities distributions weights can assessed using summary(), demonstrated . Desirable qualities include small coefficients variation close 0 large effective sample sizes. table contains available methods WeightIt estimating weights binary, multinomial, continuous treatments using various methods functions various packages. See vignette(\"installing-packages\") information install packages. addition, WeightIt implements subgroup balancing propensity score using function sbps(). Several tools utilities available. Please submit bug reports issues https://github.com/ngreifer/WeightIt/issues. like see package method integrated WeightIt, questions comments WeightIt, please contact author. Fan mail greatly appreciated.","code":"#CRAN version install.packages(\"WeightIt\")  #Development version remotes::install_github(\"ngreifer/WeightIt\")  library(\"WeightIt\") data(\"lalonde\", package = \"cobalt\")  W <- weightit(treat ~ age + educ + nodegree +                  married + race + re74 + re75,                data = lalonde, method = \"glm\",                estimand = \"ATE\") W #> A weightit object #>  - method: \"glm\" (propensity score weighting with GLM) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 2-category #>  - estimand: ATE #>  - covariates: age, educ, nodegree, married, race, re74, re75 library(\"cobalt\")  bal.tab(W, un = TRUE) #> Balance Measures #>                 Type Diff.Un Diff.Adj #> prop.score  Distance  1.7569   0.1360 #> age          Contin. -0.2419  -0.1676 #> educ         Contin.  0.0448   0.1296 #> nodegree      Binary  0.1114  -0.0547 #> married       Binary -0.3236  -0.0944 #> race_black    Binary  0.6404   0.0499 #> race_hispan   Binary -0.0827   0.0047 #> race_white    Binary -0.5577  -0.0546 #> re74         Contin. -0.5958  -0.2740 #> re75         Contin. -0.2870  -0.1579 #>  #> Effective sample sizes #>            Control Treated #> Unadjusted  429.    185.   #> Adjusted    329.01   58.33 summary(W) #>                  Summary of weights #>  #> - Weight ranges: #>  #>            Min                                   Max #> treated 1.1721 |---------------------------| 40.0773 #> control 1.0092 |-|                            4.7432 #>  #> - Units with the 5 most extreme weights by group: #>                                                  #>               68     116      10     137     124 #>  treated 13.5451 15.9884 23.2967 23.3891 40.0773 #>              597     573     381     411     303 #>  control  4.0301  4.0592  4.2397  4.5231  4.7432 #>  #> - Weight statistics: #>  #>         Coef of Var   MAD Entropy # Zeros #> treated       1.478 0.807   0.534       0 #> control       0.552 0.391   0.118       0 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted  429.    185.   #> Weighted    329.01   58.33"},{"path":"https://ngreifer.github.io/WeightIt/reference/ESS.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute effective sample size of weighted sample — ESS","title":"Compute effective sample size of weighted sample — ESS","text":"Computes effective sample size (ESS) weighted sample, represents size unweighted sample approximately amount precision weighted sample consideration.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/ESS.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute effective sample size of weighted sample — ESS","text":"","code":"ESS(w)"},{"path":"https://ngreifer.github.io/WeightIt/reference/ESS.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute effective sample size of weighted sample — ESS","text":"w vector weights","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/ESS.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Compute effective sample size of weighted sample — ESS","text":"ESS calculated \\((\\sum w)^2/\\sum w^2\\).","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/ESS.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Compute effective sample size of weighted sample — ESS","text":"McCaffrey, D. F., Ridgeway, G., & Morral, . R. (2004). Propensity Score Estimation Boosted Regression Evaluating Causal Effects Observational Studies. Psychological Methods, 9(4), 403–425. doi:10.1037/1082-989X.9.4.403 Shook‐Sa, B. E., & Hudgens, M. G. (2020). Power sample size observational studies point exposure effects. Biometrics, biom.13405. doi:doi.org/10.1111/biom.13405","code":""},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/reference/ESS.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute effective sample size of weighted sample — ESS","text":"","code":"library(\"cobalt\") #>  cobalt (Version 4.5.1, Build Date: 2023-04-27) data(\"lalonde\", package = \"cobalt\")  #Balancing covariates between treatment groups (binary) (W1 <- weightit(treat ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"glm\", estimand = \"ATE\")) #> A weightit object #>  - method: \"glm\" (propensity score weighting with GLM) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 2-category #>  - estimand: ATE #>  - covariates: age, educ, married, nodegree, re74 summary(W1) #>                  Summary of weights #>  #> - Weight ranges: #>  #>            Min                                   Max #> treated 1.5560  |--------------------------| 73.3315 #> control 1.0222 ||                             3.0438 #>  #> - Units with the 5 most extreme weights by group: #>                                                  #>              124     184     172     181     182 #>  treated 11.2281 11.3437 12.0848 26.1775 73.3315 #>              411     595     269     409     296 #>  control  2.3303  2.4365  2.5005  2.6369  3.0438 #>  #> - Weight statistics: #>  #>         Coef of Var   MAD Entropy # Zeros #> treated       1.609 0.555   0.403       0 #> control       0.247 0.211   0.029       0 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted  429.    185.   #> Weighted    404.35   51.73 ESS(W1$weights[W1$treat == 0]) #> [1] 404.3484 ESS(W1$weights[W1$treat == 1]) #> [1] 51.73462"},{"path":"https://ngreifer.github.io/WeightIt/reference/as.weightit.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a weightit object manually — as.weightit","title":"Create a weightit object manually — as.weightit","text":"function allows users get benefits weightit object using weights estimated weightit() weightitMSM(). benefits include diagnostics, plots, direct compatibility cobalt assessing balance.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/as.weightit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a weightit object manually — as.weightit","text":"","code":"as.weightit(...)  # S3 method for default as.weightit(weights,             treat,             covs = NULL,             estimand = NULL,             s.weights = NULL,             ps = NULL,             ...)  as.weightitMSM(...)  # S3 method for default as.weightitMSM(weights,                treat.list,                covs.list = NULL,                estimand = NULL,                s.weights = NULL,                ps.list = NULL,                ...)"},{"path":"https://ngreifer.github.io/WeightIt/reference/as.weightit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a weightit object manually — as.weightit","text":"weights required; numeric vector weights, one unit. treat required; vector treatment statuses, one unit. covs optional data.frame covariates. using WeightIt functions, necessary, use cobalt . estimand optional character length 1 giving estimand. text checked. s.weights optional numeric vector sampling weights, one unit. ps optional numeric vector propensity scores, one unit. treat.list list treatment statuses time point. covs.list optional list data.frames covariates covariates time point. using WeightIt functions, necessary, use cobalt . ps.list optional list numeric vectors propensity scores time point. ... additional arguments. must named. included output object.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/as.weightit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a weightit object manually — as.weightit","text":"object class weightit (.weightit()) weightitMSM (.weightitMSM()).","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/as.weightit.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Create a weightit object manually — as.weightit","text":"Noah Greifer","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/as.weightit.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a weightit object manually — as.weightit","text":"","code":"treat <- rbinom(500, 1, .3) weights <- rchisq(500, df = 2) W <- as.weightit(weights= weights, treat = treat,                  estimand = \"ATE\") summary(W) #>                  Summary of weights #>  #> - Weight ranges: #>  #>            Min                                   Max #> treated 0.0217 |--------------|               9.9952 #> control 0.0132 |---------------------------| 18.5466 #>  #> - Units with the 5 most extreme weights by group: #>                                               #>              74      6    165     397     281 #>  treated 5.9543 6.1237 6.4227  7.5772  9.9952 #>              27    222    373     435     206 #>  control 8.9055 9.0806 9.5184 10.4005 18.5466 #>  #> - Weight statistics: #>  #>         Coef of Var   MAD Entropy # Zeros #> treated       0.922 0.706   0.382       0 #> control       0.993 0.705   0.391       0 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted   344.   156.   #> Weighted     173.5   84.56"},{"path":"https://ngreifer.github.io/WeightIt/reference/get_w_from_ps.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute weights from propensity scores — get_w_from_ps","title":"Compute weights from propensity scores — get_w_from_ps","text":"Given vector matrix propensity scores, outputs vector weights target provided estimand.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/get_w_from_ps.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute weights from propensity scores — get_w_from_ps","text":"","code":"get_w_from_ps(ps,               treat,               estimand = \"ATE\",               focal = NULL,               treated = NULL,               subclass = NULL,               stabilize = FALSE)"},{"path":"https://ngreifer.github.io/WeightIt/reference/get_w_from_ps.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute weights from propensity scores — get_w_from_ps","text":"ps vector, matrix, data frame propensity scores. See Details. treat vector treatment status individual. See Details. estimand desired estimand weights target. Current options include \"ATE\" (average treatment effect), \"ATT\" (average treatment effect treated), \"ATC\" (average treatment effect control), \"ATO\" (average treatment effect overlap), \"ATM\" (average treatment effect matched sample), \"ATOS\" (average treatment effect optimal subset). focal estimand ATT ATC, group consider (focal) \"treated\" \"control\" group, respectively. NULL estimand \"ATT\" \"ATC\", estimand automatically set \"ATT\". treated treatment binary, value treat considered \"treated\" group (.e., group propensity scores probability ). NULL, get_w_from_ps() attempt figure using heuristics. really matters treat values 0 1 ps given vector unnamed single-column matrix data frame. subclass numeric; number subclasses use computing weights using marginal mean weighting stratification (also known fine stratification). NULL, standard inverse probability weights (extensions) computed; number greater 1, subclasses formed weights computed based subclass membership. estimand must ATE, ATT, ATC subclass non-NULL. See Details. stabilize logical; whether compute stabilized weights . simply involves multiplying unit's weight proportion units treatment group. saturated outcome models balance checking, make difference; otherwise, can improve performance.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/get_w_from_ps.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Compute weights from propensity scores — get_w_from_ps","text":"get_w_from_ps applies formula computing weights propensity scores desired estimand. See References section information estimands formulas. ps can entered variety ways. binary treatments, ps entered vector unnamed single-column matrix data frame, get_w_from_ps know value treat corresponds \"treated\" group. 0/1 variables, 1 considered treated. types variables, get_w_from_ps() try figure using heuristics, safer supply argument treated. estimand \"ATT\" \"ATC\", supplying value focal sufficient (ATT, focal treated group, ATC, focal control group). entered matrix data frame, columns must named levels treatment, assumed column corresponds probability treatment group. safest way supply ps unless treat 0/1 variable. multi-category treatments, ps can entered vector matrix data frame. entered vector, assumed value corresponds probability treatment actually received; possible estimand \"ATE\". Otherwise, ps must entered named matrix data frame described binary treatments. estimand \"ATT\" \"ATC\", value focal must specified. subclass NULL, marginal mean weighting stratification (MMWS) weights computed. implementation differs slightly described Hong (2010, 2012). First, subclasses formed finding quantiles propensity scores target group (ATE, units; ATT ATC, just units focal group). subclasses lacking members treatment group filled neighboring subclasses subclass always least one member treatment group. new subclass-propensity score matrix formed, unit's subclass-propensity score treatment value computed proportion units treatment value unit's subclass. example, subclass 10 treated units 90 control units , subclass-propensity score treated .1 subclass-propensity score control .9 units subclass. multi-category treatments, propensity scores treatment stratified separately described Hong (2012); binary treatments, one set propensity scores stratified subclass-propensity scores treatment computed complement propensity scores stratified treatment. subclass-propensity scores computed, standard propensity score weighting formulas used compute unstabilized MMWS weights. estimate MMWS weights equivalent described Hong (2010, 2012), stabilize must set TRUE, , standard propensity score weights, optional. Note MMWS weights also known fine stratification weights described Desai et al. (2017). get_w_from_ps() compatible continuous treatments.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/get_w_from_ps.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute weights from propensity scores — get_w_from_ps","text":"vector weights. subclass NULL, subclasses returned \"subclass\" attribute. estimand = \"ATOS\", chosen value alpha (smallest propensity score allowed remain sample) returned \"alpha\" attribute.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/get_w_from_ps.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Compute weights from propensity scores — get_w_from_ps","text":"Noah Greifer","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/get_w_from_ps.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Compute weights from propensity scores — get_w_from_ps","text":"Binary treatments - estimand = \"ATO\" Li, F., Morgan, K. L., & Zaslavsky, . M. (2018). Balancing covariates via propensity score weighting. Journal American Statistical Association, 113(521), 390–400. doi:10.1080/01621459.2016.1260466 - estimand = \"ATM\" Li, L., & Greene, T. (2013). Weighting Analogue Pair Matching Propensity Score Analysis. International Journal Biostatistics, 9(2). doi:10.1515/ijb-2012-0030 - estimand = \"ATOS\" Crump, R. K., Hotz, V. J., Imbens, G. W., & Mitnik, O. . (2009). Dealing limited overlap estimation average treatment effects. Biometrika, 96(1), 187–199. doi:10.1093/biomet/asn055 - estimands Austin, P. C. (2011). Introduction Propensity Score Methods Reducing Effects Confounding Observational Studies. Multivariate Behavioral Research, 46(3), 399–424. doi:10.1080/00273171.2011.568786 - Marginal mean weighting stratification (MMWS) Hong, G. (2010). Marginal mean weighting stratification: Adjustment selection bias multilevel data. Journal Educational Behavioral Statistics, 35(5), 499–531. doi:10.3102/1076998609359785 Desai, R. J., Rothman, K. J., Bateman, B. . T., Hernandez-Diaz, S., & Huybrechts, K. F. (2017). Propensity-score-based Fine Stratification Approach Confounding Adjustment Exposure Infrequent: Epidemiology, 28(2), 249–257. doi:10.1097/EDE.0000000000000595 Multinomial Treatments - estimand = \"ATO\" Li, F., & Li, F. (2019). Propensity score weighting causal inference multiple treatments. Annals Applied Statistics, 13(4), 2389–2415. doi:10.1214/19-AOAS1282 - estimand = \"ATM\" Yoshida, K., Hernández-Díaz, S., Solomon, D. H., Jackson, J. W., Gagne, J. J., Glynn, R. J., & Franklin, J. M. (2017). Matching weights simultaneously compare three treatment groups: Comparison three-way matching. Epidemiology (Cambridge, Mass.), 28(3), 387–395. doi:10.1097/EDE.0000000000000627 - estimands McCaffrey, D. F., Griffin, B. ., Almirall, D., Slaughter, M. E., Ramchand, R., & Burgette, L. F. (2013). Tutorial Propensity Score Estimation Multiple Treatments Using Generalized Boosted Models. Statistics Medicine, 32(19), 3388–3414. doi:10.1002/sim.5753 - Marginal mean weighting stratification Hong, G. (2012). Marginal mean weighting stratification: generalized method evaluating multivalued multiple treatments nonexperimental data. Psychological Methods, 17(1), 44–60. doi:10.1037/a0024918","code":""},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/reference/get_w_from_ps.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute weights from propensity scores — get_w_from_ps","text":"","code":"library(\"cobalt\") data(\"lalonde\", package = \"cobalt\")  ps.fit <- glm(treat ~ age + educ + race + married +                 nodegree + re74 + re75, data = lalonde,               family = binomial) ps <- ps.fit$fitted.values  w1 <- get_w_from_ps(ps, treat = lalonde$treat,                     estimand = \"ATT\")  treatAB <- factor(ifelse(lalonde$treat == 1, \"A\", \"B\")) w2 <- get_w_from_ps(ps, treat = treatAB,                     estimand = \"ATT\", focal = \"A\") all.equal(w1, w2) #> [1] TRUE w3 <- get_w_from_ps(ps, treat = treatAB,                     estimand = \"ATT\", treated = \"A\") all.equal(w1, w3) #> [1] TRUE  #Using MMWS w4 <- get_w_from_ps(ps, treat = lalonde$treat,                     estimand = \"ATE\", subclass = 20,                     stabilize = TRUE) #A multi-category example using GBM predicted probabilities library(gbm) #> Loaded gbm 2.1.8.1 T3 <- factor(sample(c(\"A\", \"B\", \"C\"), nrow(lalonde), replace = TRUE))  gbm.fit <- gbm(T3 ~ age + educ + race + married +                  nodegree + re74 + re75, data = lalonde,                distribution = \"multinomial\", n.trees = 200,                interaction.depth = 3) #> Warning: Setting `distribution = \"multinomial\"` is ill-advised as it is currently broken. It exists only for backwards compatibility. Use at your own risk. ps.multi <- drop(predict(gbm.fit, type = \"response\",                          n.trees = 200)) w <- get_w_from_ps(ps.multi, T3, estimand = \"ATE\")"},{"path":"https://ngreifer.github.io/WeightIt/reference/make_full_rank.html","id":null,"dir":"Reference","previous_headings":"","what":"Make a design matrix full rank — make_full_rank","title":"Make a design matrix full rank — make_full_rank","text":"writing user-defined methods use weightit(), may necessary take potentially non-full rank covs data frame make full rank use downstream function. function performs operation.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/make_full_rank.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Make a design matrix full rank — make_full_rank","text":"","code":"make_full_rank(mat,                with.intercept = TRUE)"},{"path":"https://ngreifer.github.io/WeightIt/reference/make_full_rank.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Make a design matrix full rank — make_full_rank","text":"mat numeric matrix data frame transformed. Typically contains covariates. NAs allowed. .intercept whether intercept (.e., vector 1s) added mat making full rank. TRUE, intercept used determining whether column linearly dependent others. Regardless, intercept included output.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/make_full_rank.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Make a design matrix full rank — make_full_rank","text":"make_full_rank() calls qr() find rank linearly independent columns mat, retained others dropped. .intercept set TRUE, intercept column added matrix calling qr(). Note dependent columns appear later mat dropped first. See example method_user.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/make_full_rank.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Make a design matrix full rank — make_full_rank","text":"Older versions drop columns one value. .intercept = FALSE, one column one value, removed, function though intercept present; column one value, first one remain.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/make_full_rank.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Make a design matrix full rank — make_full_rank","text":"object type mat containing linearly independent columns.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/make_full_rank.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Make a design matrix full rank — make_full_rank","text":"Noah Greifer","code":""},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/reference/make_full_rank.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Make a design matrix full rank — make_full_rank","text":"","code":"set.seed(1000) c1 <- rbinom(10, 1, .4) c2 <- 1-c1 c3 <- rnorm(10) c4 <- 10*c3 mat <- data.frame(c1, c2, c3, c4)  make_full_rank(mat) #leaves c2 and c4 #>    c1          c3 #> 1   0 -0.38548930 #> 2   1 -0.47586788 #> 3   0  0.71975069 #> 4   1 -0.01850562 #> 5   0 -1.37311776 #> 6   0 -0.98242783 #> 7   1 -0.55448870 #> 8   0  0.12138119 #> 9   0 -0.12087232 #> 10  0 -1.33604105  make_full_rank(mat, with.intercept = FALSE) #leaves c1, c2, and c4 #>    c1 c2          c3 #> 1   0  1 -0.38548930 #> 2   1  0 -0.47586788 #> 3   0  1  0.71975069 #> 4   1  0 -0.01850562 #> 5   0  1 -1.37311776 #> 6   0  1 -0.98242783 #> 7   1  0 -0.55448870 #> 8   0  1  0.12138119 #> 9   0  1 -0.12087232 #> 10  0  1 -1.33604105"},{"path":"https://ngreifer.github.io/WeightIt/reference/method_bart.html","id":null,"dir":"Reference","previous_headings":"","what":"Propensity Score Weighting Using BART — method_bart","title":"Propensity Score Weighting Using BART — method_bart","text":"page explains details estimating weights Bayesian additive regression trees (BART)-based propensity scores setting method = \"bart\" call weightit() weightitMSM(). method can used binary, multinomial, continuous treatments. general, method relies estimating propensity scores using BART converting propensity scores weights using formula depends desired estimand. method relies dbarts::bart2() dbarts package.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_bart.html","id":"binary-treatments","dir":"Reference","previous_headings":"","what":"Binary Treatments","title":"Propensity Score Weighting Using BART — method_bart","text":"binary treatments, method estimates propensity scores using dbarts::bart2(). following estimands allowed: ATE, ATT, ATC, ATO, ATM, ATOS. Weights can also computed using marginal mean weighting stratification ATE, ATT, ATC. See get_w_from_ps() details.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_bart.html","id":"multinomial-treatments","dir":"Reference","previous_headings":"","what":"Multinomial Treatments","title":"Propensity Score Weighting Using BART — method_bart","text":"multinomial treatments, propensity scores estimated using several calls dbarts::bart2(), one treatment group; treatment probabilities normalized sum 1. following estimands allowed: ATE, ATT, ATC, ATO, ATM. weights estimand computed using standard formulas mentioned . Weights can also computed using marginal mean weighting stratification ATE, ATT, ATC. See get_w_from_ps() details.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_bart.html","id":"continuous-treatments","dir":"Reference","previous_headings":"","what":"Continuous Treatments","title":"Propensity Score Weighting Using BART — method_bart","text":"continuous treatments, generalized propensity score estimated using dbarts::bart2(). addition, kernel density estimation can used instead assuming normal density numerator denominator generalized propensity score setting use.kernel = TRUE. arguments density() can specified refine density estimation parameters. plot = TRUE can specified plot density numerator denominator, can helpful diagnosing extreme weights.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_bart.html","id":"longitudinal-treatments","dir":"Reference","previous_headings":"","what":"Longitudinal Treatments","title":"Propensity Score Weighting Using BART — method_bart","text":"longitudinal treatments, weights product weights estimated time point.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_bart.html","id":"sampling-weights","dir":"Reference","previous_headings":"","what":"Sampling Weights","title":"Propensity Score Weighting Using BART — method_bart","text":"Sampling weights supported.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_bart.html","id":"missing-data","dir":"Reference","previous_headings":"","what":"Missing Data","title":"Propensity Score Weighting Using BART — method_bart","text":"presence missing data, following value(s) missing allowed: \"ind\" (default) First, variable missingness, new missingness indicator variable created takes value 1 original covariate NA 0 otherwise. missingness indicators added model formula main effects. missing values covariates replaced covariate medians. weight estimation proceeds new formula set covariates. covariates output resulting weightit object original covariates NAs.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_bart.html","id":"additional-arguments","dir":"Reference","previous_headings":"","what":"Additional Arguments","title":"Propensity Score Weighting Using BART — method_bart","text":"arguments dbarts::bart2() can passed weightit() weightitMSM(), following exceptions: test, weights, subset, offset.test ignored combine.chains always set TRUE sampleronly always set FALSE continuous treatments , following arguments may supplied: density function corresponding conditional density treatment. standardized residuals treatment model fed function produce numerator denominator generalized propensity score weights. blank, dnorm() used recommended Robins et al. (2000). can also supplied string containing name function called. string contains underscores, call split underscores latter splits supplied arguments second argument beyond. example, density = \"dt_2\" specified, density used t-distribution 2 degrees freedom. Using t-distribution can useful extreme outcome values observed (Naimi et al., 2014). Ignored use.kernel = TRUE (described ). use.kernel TRUE, uses kernel density estimation density() estimate numerator denominator densities weights. FALSE, argument density parameter used instead. bw, adjust, kernel, n use.kernel = TRUE, arguments density() function. defaults density except n 10 times number units sample. plot use.kernel = TRUE, whether plot estimated density.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_bart.html","id":"additional-outputs","dir":"Reference","previous_headings":"","what":"Additional Outputs","title":"Propensity Score Weighting Using BART — method_bart","text":"obj include.obj = TRUE, bart2 fit(s) used generate predicted values. multinomial treatments, list fits; otherwise, single fit. predicted probabilities used compute propensity scores can extracted using fitted().","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_bart.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Propensity Score Weighting Using BART — method_bart","text":"BART works fitting sum--trees model treatment probability treatment. number trees determined n.trees argument. Bayesian priors used hyperparameters, result posterior distribution predicted values unit. mean unit taken use computing (generalized) propensity score. Although hyperparameters governing priors can modified supplying arguments weightit() passed BART fitting function, default values tend work well require little modification (though defaults differ continuous categorical treatments; see dbarts::bart2() documentation details). Unlike many machine learning methods, loss function optimized hyperparameters need tuned (e.g., using cross-validation), though performance can benefit tuning. BART tends balance sparseness flexibility using weak learners trees, makes suitable capturing complex functions without specifying particular functional form without overfitting.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_bart.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Propensity Score Weighting Using BART — method_bart","text":"version 0.9-19 dbarts, special care taken ensure reproducibility using method = \"bart\". Setting seed (either set.seed() supplying argument rngSeed) work one thread requested. default use four threads. request one thread used, necessary reproducible results, set n.threads = 1 call weightit() set seed. Note fewer threads used, slower estimation . One can set n.chains lower number (default 4) speed estimation possible expense statistical performance. version 0.9-20 , setting seed set.seed() works correctly results reproducible.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_bart.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Propensity Score Weighting Using BART — method_bart","text":"Hill, J., Weiss, C., & Zhai, F. (2011). Challenges Propensity Score Strategies High-Dimensional Setting Potential Alternative. Multivariate Behavioral Research, 46(3), 477–513. doi:10.1080/00273171.2011.570161 Chipman, H. ., George, E. ., & McCulloch, R. E. (2010). BART: Bayesian additive regression trees. Annals Applied Statistics, 4(1), 266–298. doi:10.1214/09-AOAS285 Note many references deal BART causal inference focus estimating potential outcomes BART, propensity scores, directly relevant using BART estimate propensity scores weights. See method_ps additional references propensity score weighting generally.","code":""},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/reference/method_bart.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Propensity Score Weighting Using BART — method_bart","text":"","code":"library(\"cobalt\") data(\"lalonde\", package = \"cobalt\")  #Balancing covariates between treatment groups (binary) (W1 <- weightit(treat ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"bart\", estimand = \"ATT\")) #> A weightit object #>  - method: \"bart\" (propensity score weighting with BART) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 2-category #>  - estimand: ATT (focal: 1) #>  - covariates: age, educ, married, nodegree, re74 summary(W1) #>                  Summary of weights #>  #> - Weight ranges: #>  #>            Min                                   Max #> treated 1.0000    ||                          1.0000 #> control 0.0027 |---------------------------| 11.3222 #>  #> - Units with the 5 most extreme weights by group: #>                                             #>               5      4      3     2       1 #>  treated      1      1      1     1       1 #>             454    569    592   374     608 #>  control 2.2547 3.0131 3.0676 3.524 11.3222 #>  #> - Weight statistics: #>  #>         Coef of Var   MAD Entropy # Zeros #> treated       0.000 0.000  -0.000       0 #> control       2.007 0.931   0.766       0 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted  429.       185 #> Weighted     85.51     185 bal.tab(W1) #> Balance Measures #>                Type Diff.Adj #> prop.score Distance   0.4864 #> age         Contin.   0.0489 #> educ        Contin.  -0.0236 #> married      Binary  -0.0348 #> nodegree     Binary   0.0374 #> re74        Contin.  -0.0474 #>  #> Effective sample sizes #>            Control Treated #> Unadjusted  429.       185 #> Adjusted     85.51     185 # \\donttest{ #Balancing covariates with respect to race (multinomial) (W2 <- weightit(race ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"bart\", estimand = \"ATE\")) #> A weightit object #>  - method: \"bart\" (propensity score weighting with BART) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 3-category (black, hispan, white) #>  - estimand: ATE #>  - covariates: age, educ, married, nodegree, re74 summary(W2) #>                  Summary of weights #>  #> - Weight ranges: #>  #>           Min                                   Max #> black  1.2396 |-----------------|            8.8929 #> hispan 2.6466     |-----------------------| 12.7762 #> white  1.0596 |---------------|              8.0632 #>  #> - Units with the 5 most extreme weights by group: #>                                                #>             226     181     244    231     423 #>   black  6.9935  7.3696   8.181 8.5277  8.8929 #>             345     426     564    570     512 #>  hispan 11.7316 12.1743 12.5039 12.568 12.7762 #>              68      23      60     76     140 #>   white  4.4361  5.2498  5.5611 7.9036  8.0632 #>  #> - Weight statistics: #>  #>        Coef of Var   MAD Entropy # Zeros #> black        0.577 0.372   0.125       0 #> hispan       0.369 0.304   0.068       0 #> white        0.462 0.320   0.084       0 #>  #> - Effective Sample Sizes: #>  #>             black hispan  white #> Unweighted 243.    72.   299.   #> Weighted   182.53  63.49 246.64 bal.tab(W2) #> Balance summary across all treatment pairs #>             Type Max.Diff.Adj #> age      Contin.       0.1934 #> educ     Contin.       0.1713 #> married   Binary       0.0539 #> nodegree  Binary       0.0322 #> re74     Contin.       0.1130 #>  #> Effective sample sizes #>             black hispan  white #> Unadjusted 243.    72.   299.   #> Adjusted   182.53  63.49 246.64  #Balancing covariates with respect to re75 (continuous) #assuming t(3) conditional density for treatment (W3 <- weightit(re75 ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"bart\", density = \"dt_3\")) #> A weightit object #>  - method: \"bart\" (propensity score weighting with BART) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: continuous #>  - covariates: age, educ, married, nodegree, re74 summary(W3) #>                  Summary of weights #>  #> - Weight ranges: #>  #>        Min                                   Max #> all 0.0784 |---------------------------| 24.3946 #>  #> - Units with the 5 most extreme weights: #>                                           #>         308    486    469     484     487 #>  all 7.5032 7.6586 9.4619 18.2688 24.3946 #>  #> - Weight statistics: #>  #>     Coef of Var   MAD Entropy # Zeros #> all       1.204 0.482    0.29       0 #>  #> - Effective Sample Sizes: #>  #>             Total #> Unweighted 614.   #> Weighted   250.91 bal.tab(W3) #> Balance Measures #>             Type Corr.Adj #> age      Contin.   0.0293 #> educ     Contin.   0.0553 #> married   Binary   0.0721 #> nodegree  Binary  -0.0837 #> re74     Contin.   0.1082 #>  #> Effective sample sizes #>             Total #> Unadjusted 614.   #> Adjusted   250.91 # }"},{"path":"https://ngreifer.github.io/WeightIt/reference/method_cbps.html","id":null,"dir":"Reference","previous_headings":"","what":"Covariate Balancing Propensity Score Weighting — method_cbps","title":"Covariate Balancing Propensity Score Weighting — method_cbps","text":"page explains details estimating weights covariate balancing propensity scores setting method = \"cbps\" call weightit() weightitMSM(). method can used binary, multinomial, continuous treatments. general, method relies estimating propensity scores using generalized method moments converting propensity scores weights using formula depends desired estimand. method relies CBPS::CBPS() CBPS package.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_cbps.html","id":"binary-treatments","dir":"Reference","previous_headings":"","what":"Binary Treatments","title":"Covariate Balancing Propensity Score Weighting — method_cbps","text":"binary treatments, method estimates propensity scores weights using CBPS::CBPS(). following estimands allowed: ATE, ATT, ATC. weights taken output CBPS fit object. estimand ATE, return propensity score probability \"second\" treatment group, .e., levels(factor(treat))[2]; estimand ATC, returned propensity score probability control (.e., non-focal) group.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_cbps.html","id":"multinomial-treatments","dir":"Reference","previous_headings":"","what":"Multinomial Treatments","title":"Covariate Balancing Propensity Score Weighting — method_cbps","text":"multinomial treatments three four categories estimand ATE, method estimates propensity scores weights using one call CBPS::CBPS(). multinomial treatments three four categories estimand ATT, method estimates propensity scores weights using multiple calls CBPS::CBPS(). following estimands allowed: ATE ATT. weights taken output CBPS fit objects.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_cbps.html","id":"continuous-treatments","dir":"Reference","previous_headings":"","what":"Continuous Treatments","title":"Covariate Balancing Propensity Score Weighting — method_cbps","text":"continuous treatments, generalized propensity score weights estimated using CBPS::CBPS().","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_cbps.html","id":"longitudinal-treatments","dir":"Reference","previous_headings":"","what":"Longitudinal Treatments","title":"Covariate Balancing Propensity Score Weighting — method_cbps","text":"longitudinal treatments, weights product weights estimated time point. CBPS::CBMSM() CBPS package estimates weights longitudinal treatments.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_cbps.html","id":"sampling-weights","dir":"Reference","previous_headings":"","what":"Sampling Weights","title":"Covariate Balancing Propensity Score Weighting — method_cbps","text":"Sampling weights supported s.weights scenarios. See Note sampling weights.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_cbps.html","id":"missing-data","dir":"Reference","previous_headings":"","what":"Missing Data","title":"Covariate Balancing Propensity Score Weighting — method_cbps","text":"presence missing data, following value(s) missing allowed: \"ind\" (default) First, variable missingness, new missingness indicator variable created takes value 1 original covariate NA 0 otherwise. missingness indicators added model formula main effects. missing values covariates replaced covariate medians (value arbitrary affect estimation). weight estimation proceeds new formula set covariates. covariates output resulting weightit object original covariates NAs.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_cbps.html","id":"additional-arguments","dir":"Reference","previous_headings":"","what":"Additional Arguments","title":"Covariate Balancing Propensity Score Weighting — method_cbps","text":"arguments CBPS() can passed weightit() weightitMSM(), following exceptions: method CBPS() replaced argument weightit(). Setting = FALSE weightit() equivalent setting method = \"exact\" CBPS(). sample.weights ignored sampling weights passed using s.weights. standardize ignored. arguments take defaults CBPS(). may useful many cases set = FALSE, especially continuous treatments.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_cbps.html","id":"additional-outputs","dir":"Reference","previous_headings":"","what":"Additional Outputs","title":"Covariate Balancing Propensity Score Weighting — method_cbps","text":"obj include.obj = TRUE, CB(G)PS model fit. binary treatments, multinomial treatments estimand = \"ATE\" four fewer treatment levels, continuous treatments, output call CBPS::CBPS(). multinomial treatments estimand = \"ATT\" four treatment levels, list CBPS fit objects.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_cbps.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Covariate Balancing Propensity Score Weighting — method_cbps","text":"CBPS estimates coefficients logistic regression model (binary treatments), multinomial logistic regression model (form multinomial treatments), linear regression model (continuous treatments) used compute (generalized) propensity scores, weights computed. involves augmenting standard regression score equations balance constraints -identified generalized method moments estimation. idea nudge estimation coefficients toward produce balance weighted sample. just-identified version (exact = FALSE) away score equations coefficients balance constraints (score equation variance error continuous treatment) used. just-identified version therefore produce superior balance means (.e., corresponding balance constraints) binary multinomial treatments linear terms continuous treatments -identified version. Note WeightIt provides less functionality CBPS package terms versions CBPS available; extensions CBPS, CBPS package may preferred.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_cbps.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Covariate Balancing Propensity Score Weighting — method_cbps","text":"Binary treatments Imai, K., & Ratkovic, M. (2014). Covariate balancing propensity score. Journal Royal Statistical Society: Series B (Statistical Methodology), 76(1), 243–263. Multinomial Treatments Imai, K., & Ratkovic, M. (2014). Covariate balancing propensity score. Journal Royal Statistical Society: Series B (Statistical Methodology), 76(1), 243–263. Continuous treatments Fong, C., Hazlett, C., & Imai, K. (2018). Covariate balancing propensity score continuous treatment: Application efficacy political advertisements. Annals Applied Statistics, 12(1), 156–177. doi:10.1214/17-AOAS1101","code":""},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/reference/method_cbps.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Covariate Balancing Propensity Score Weighting — method_cbps","text":"sampling weights used CBPS::CBPS(), estimated weights already incorporate sampling weights. weightit() used method = \"cbps\", estimated weights separated sampling weights, methods.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_cbps.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Covariate Balancing Propensity Score Weighting — method_cbps","text":"","code":"library(\"cobalt\") data(\"lalonde\", package = \"cobalt\")  #Balancing covariates between treatment groups (binary) (W1 <- weightit(treat ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"cbps\", estimand = \"ATT\")) #> A weightit object #>  - method: \"cbps\" (covariate balancing propensity score weighting) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 2-category #>  - estimand: ATT (focal: 1) #>  - covariates: age, educ, married, nodegree, re74 summary(W1) #>                  Summary of weights #>  #> - Weight ranges: #>  #>           Min                                  Max #> treated 1.000              ||               1.0000 #> control 0.017 |---------------------------| 2.2742 #>  #> - Units with the 5 most extreme weights by group: #>                                             #>               5      4      3      2      1 #>  treated      1      1      1      1      1 #>             589    595    269    409    296 #>  control 1.4755 1.4873 1.5799 1.7484 2.2742 #>  #> - Weight statistics: #>  #>         Coef of Var   MAD Entropy # Zeros #> treated       0.000 0.000  -0.000       0 #> control       0.839 0.707   0.341       0 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted  429.       185 #> Weighted    251.99     185 bal.tab(W1) #> Balance Measures #>                Type Diff.Adj #> prop.score Distance   0.0163 #> age         Contin.  -0.0032 #> educ        Contin.   0.0017 #> married      Binary  -0.0003 #> nodegree     Binary  -0.0003 #> re74        Contin.   0.0005 #>  #> Effective sample sizes #>            Control Treated #> Unadjusted  429.       185 #> Adjusted    251.99     185  if (FALSE) { #Balancing covariates with respect to race (multinomial) (W2 <- weightit(race ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"cbps\", estimand = \"ATE\")) summary(W2) bal.tab(W2) }  #Balancing covariates with respect to re75 (continuous) (W3 <- weightit(re75 ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"cbps\", over = FALSE)) #> A weightit object #>  - method: \"cbps\" (covariate balancing propensity score weighting) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: continuous #>  - covariates: age, educ, married, nodegree, re74 summary(W3) #>                  Summary of weights #>  #> - Weight ranges: #>  #>        Min                                  Max #> all 0.0154 |---------------------------| 13.157 #>  #> - Units with the 5 most extreme weights: #>                                           #>        484     482     180     481    483 #>  all 9.215 10.8401 11.1959 11.9913 13.157 #>  #> - Weight statistics: #>  #>     Coef of Var  MAD Entropy # Zeros #> all       1.153 0.45   0.289       0 #>  #> - Effective Sample Sizes: #>  #>             Total #> Unweighted 614.   #> Weighted   263.72 bal.tab(W3) #> Balance Measures #>             Type Corr.Adj #> age      Contin.  -0.0004 #> educ     Contin.  -0.0001 #> married   Binary  -0.0004 #> nodegree  Binary   0.0000 #> re74     Contin.  -0.0008 #>  #> Effective sample sizes #>             Total #> Unadjusted 614.   #> Adjusted   263.72"},{"path":"https://ngreifer.github.io/WeightIt/reference/method_ebal.html","id":null,"dir":"Reference","previous_headings":"","what":"Entropy Balancing — method_ebal","title":"Entropy Balancing — method_ebal","text":"page explains details estimating weights using entropy balancing setting method = \"ebal\" call weightit() weightitMSM(). method can used binary, multinomial, continuous treatments. general, method relies estimating weights minimizing negative entropy weights subject exact moment balancing constraints. method relies code written WeightIt using optim().","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_ebal.html","id":"binary-treatments","dir":"Reference","previous_headings":"","what":"Binary Treatments","title":"Entropy Balancing — method_ebal","text":"binary treatments, method estimates weights using optim() using formulas described Hainmueller (2012). following estimands allowed: ATE, ATT, ATC. ATE requested, optimization run twice, treatment group.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_ebal.html","id":"multinomial-treatments","dir":"Reference","previous_headings":"","what":"Multinomial Treatments","title":"Entropy Balancing — method_ebal","text":"multinomial treatments, method estimates weights using optim(). following estimands allowed: ATE ATT. ATE requested, optim() run treatment group. ATT requested, optim() run non-focal (.e., control) group.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_ebal.html","id":"continuous-treatments","dir":"Reference","previous_headings":"","what":"Continuous Treatments","title":"Entropy Balancing — method_ebal","text":"continuous treatments, method estimates weights using optim() using formulas described Tübbicke (2022) Vegetabile et al. (2021).","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_ebal.html","id":"longitudinal-treatments","dir":"Reference","previous_headings":"","what":"Longitudinal Treatments","title":"Entropy Balancing — method_ebal","text":"longitudinal treatments, weights product weights estimated time point. method guaranteed yield exact balance time point. NOTE: use entropy balancing longitudinal treatments validated!","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_ebal.html","id":"sampling-weights","dir":"Reference","previous_headings":"","what":"Sampling Weights","title":"Entropy Balancing — method_ebal","text":"Sampling weights supported s.weights scenarios.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_ebal.html","id":"missing-data","dir":"Reference","previous_headings":"","what":"Missing Data","title":"Entropy Balancing — method_ebal","text":"presence missing data, following value(s) missing allowed: \"ind\" (default) First, variable missingness, new missingness indicator variable created takes value 1 original covariate NA 0 otherwise. missingness indicators added model formula main effects. missing values covariates replaced covariate medians (value arbitrary affect estimation). weight estimation proceeds new formula set covariates. covariates output resulting weightit object original covariates NAs.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_ebal.html","id":"additional-arguments","dir":"Reference","previous_headings":"","what":"Additional Arguments","title":"Entropy Balancing — method_ebal","text":"moments int accepted. See weightit() details. base.weights vector base weights, one unit. works continuous treatments well. correspond base weights \\(q\\) Hainmueller (2012). estimated weights minimize Kullback entropy divergence base weights, defined \\(\\sum w \\log(w/q)\\), subject exact balance constraints. can used supply previously estimated weights newly estimated weights retain properties original weights ensuring balance constraints met. Sampling weights passed base.weights can included weightit() call includes s.weights. d.moments continuous treatments, number moments treatment covariate distributions constrained weighted sample original sample. example, setting d.moments = 3 ensures mean, variance, skew treatment covariates weighted sample unweighted sample. d.moments greater equal moments automatically set accordingly (specified). Vegetabile et al. (2021) recommend setting d.moments = 3, even moments less 3. argument corresponds tuning parameters \\(r\\) \\(s\\) Vegetabile et al. (2021) (set equal). Ignored binary multi-category treatments. arguments maxit reltol can supplied passed control argument optim(). \"BFGS\" method used, defaults correspond . stabilize argument ignored; past reduce variability weights iterative process. want minimize variance weights subject balance constraints, use method = \"optweight\".","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_ebal.html","id":"additional-outputs","dir":"Reference","previous_headings":"","what":"Additional Outputs","title":"Entropy Balancing — method_ebal","text":"obj include.obj = TRUE, output call optim(), contains dual variables convergence information. ATE fits multinomial treatments, list optim() outputs, one weighted group.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_ebal.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Entropy Balancing — method_ebal","text":"Entropy balancing involves specification optimization problem, solution used compute weights. constraints primal optimization problem correspond covariate balance means (binary multinomial treatments) treatment-covariate covariances (continuous treatments), positivity weights, weights sum certain value. turns dual optimization problem much easier solve many variables balance constraints rather weights unit unconstrained. Zhao Percival (2017) found entropy balancing ATT binary treatment actually involves estimation coefficients logistic regression propensity score model using specialized loss function different optimized maximum likelihood. Entropy balancing doubly robust (ATT) sense consistent either true propensity score model logistic regression treatment covariates true outcome model control units linear regression outcome covariates, attains semi-parametric efficiency bound true. Entropy balancing always yield exact mean balance included terms.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_ebal.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Entropy Balancing — method_ebal","text":"Binary Treatments Hainmueller, J. (2012). Entropy Balancing Causal Effects: Multivariate Reweighting Method Produce Balanced Samples Observational Studies. Political Analysis, 20(1), 25–46. doi:10.1093/pan/mpr025 Källberg, D., & Waernbaum, . (2022). Large Sample Properties Entropy Balancing Estimators Average Causal Effects. ArXiv:2204.10623 [Stat]. https://arxiv.org/abs/2204.10623 Zhao, Q., & Percival, D. (2017). Entropy balancing doubly robust. Journal Causal Inference, 5(1). doi:10.1515/jci-2016-0010 Continuous Treatments Tübbicke, S. (2022). Entropy Balancing Continuous Treatments. Journal Econometric Methods, 11(1), 71–89. doi:10.1515/jem-2021-0002 Vegetabile, B. G., Griffin, B. ., Coffman, D. L., Cefalu, M., Robbins, M. W., & McCaffrey, D. F. (2021). Nonparametric estimation population average dose-response curves using entropy balancing weights continuous exposures. Health Services Outcomes Research Methodology, 21(1), 69–110. doi:10.1007/s10742-020-00236-2","code":""},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/reference/method_ebal.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Entropy Balancing — method_ebal","text":"","code":"library(\"cobalt\") data(\"lalonde\", package = \"cobalt\")  #Balancing covariates between treatment groups (binary) (W1 <- weightit(treat ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"ebal\", estimand = \"ATT\")) #> A weightit object #>  - method: \"ebal\" (entropy balancing) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 2-category #>  - estimand: ATT (focal: 1) #>  - covariates: age, educ, married, nodegree, re74 summary(W1) #>                  Summary of weights #>  #> - Weight ranges: #>  #>            Min                                  Max #> treated 1.0000       ||                      1.0000 #> control 0.0398 |---------------------------| 5.2466 #>  #> - Units with the 5 most extreme weights by group: #>                                             #>               5      4      3      2      1 #>  treated      1      1      1      1      1 #>             589    595    269    409    296 #>  control 3.3964 3.4432 3.6553 4.0427 5.2466 #>  #> - Weight statistics: #>  #>         Coef of Var   MAD Entropy # Zeros #> treated       0.000 0.000   0.000       0 #> control       0.839 0.707   0.341       0 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted   429.      185 #> Weighted     252.1     185 bal.tab(W1) #> Balance Measures #>             Type Diff.Adj #> age      Contin.   0.0000 #> educ     Contin.   0.0000 #> married   Binary   0.0001 #> nodegree  Binary   0.0000 #> re74     Contin.  -0.0000 #>  #> Effective sample sizes #>            Control Treated #> Unadjusted   429.      185 #> Adjusted     252.1     185  #Balancing covariates with respect to race (multinomial) (W2 <- weightit(race ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"ebal\", estimand = \"ATE\")) #> A weightit object #>  - method: \"ebal\" (entropy balancing) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 3-category (black, hispan, white) #>  - estimand: ATE #>  - covariates: age, educ, married, nodegree, re74 summary(W2) #>                  Summary of weights #>  #> - Weight ranges: #>  #>           Min                                  Max #> black  0.5530   |-------------------------| 5.3496 #> hispan 0.1409 |----------------|            3.3322 #> white  0.3979  |-------|                    1.9224 #>  #> - Units with the 5 most extreme weights by group: #>                                            #>            226    244    485    181    182 #>   black 2.5215 2.5491 2.8059 3.5551 5.3496 #>            392    564    269    345    371 #>  hispan 2.0467   2.53 2.6322 2.7049 3.3322 #>             68    457    599    589    531 #>   white 1.7106 1.7226 1.7426 1.7743 1.9224 #>  #> - Weight statistics: #>  #>        Coef of Var   MAD Entropy # Zeros #> black        0.590 0.413   0.131       0 #> hispan       0.609 0.440   0.163       0 #> white        0.371 0.306   0.068       0 #>  #> - Effective Sample Sizes: #>  #>             black hispan  white #> Unweighted 243.    72.   299.   #> Weighted   180.47  52.71 262.93 bal.tab(W2) #> Balance summary across all treatment pairs #>             Type Max.Diff.Adj #> age      Contin.       0.0001 #> educ     Contin.       0.0000 #> married   Binary       0.0001 #> nodegree  Binary       0.0001 #> re74     Contin.       0.0001 #>  #> Effective sample sizes #>             black hispan  white #> Unadjusted 243.    72.   299.   #> Adjusted   180.47  52.71 262.93  #Balancing covariates and squares with respect to #re75 (continuous), maintaining 3 moments of the #covariate and treatment distributions (W3 <- weightit(re75 ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"ebal\", moments = 2,                 d.moments = 3)) #> A weightit object #>  - method: \"ebal\" (entropy balancing) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: continuous #>  - covariates: age, educ, married, nodegree, re74 summary(W3) #>                  Summary of weights #>  #> - Weight ranges: #>  #>        Min                                   Max #> all 0.0001 |---------------------------| 17.6716 #>  #> - Units with the 5 most extreme weights: #>                                          #>         484   200    166     171     180 #>  all 6.7602 7.609 8.5222 10.2613 17.6716 #>  #> - Weight statistics: #>  #>     Coef of Var   MAD Entropy # Zeros #> all       1.254 0.615   0.424       0 #>  #> - Effective Sample Sizes: #>  #>             Total #> Unweighted 614.   #> Weighted   239.02 bal.tab(W3) #> Balance Measures #>             Type Corr.Adj #> age      Contin.   0.0001 #> educ     Contin.  -0.0002 #> married   Binary  -0.0001 #> nodegree  Binary   0.0002 #> re74     Contin.  -0.0005 #>  #> Effective sample sizes #>             Total #> Unadjusted 614.   #> Adjusted   239.02"},{"path":"https://ngreifer.github.io/WeightIt/reference/method_energy.html","id":null,"dir":"Reference","previous_headings":"","what":"Energy Balancing — method_energy","title":"Energy Balancing — method_energy","text":"page explains details estimating weights using energy balancing setting method = \"energy\" call weightit() weightitMSM(). method can used binary, multinomial, continuous treatments. general, method relies estimating weights minimizing energy statistic related covariate balance. binary multinomial treatments, energy distance, multivariate distance distributions, treatment groups. continuous treatments, sum distance covariance treatment variable covariates energy distances treatment covariates weighted sample distributions original sample. method relies code written WeightIt using osqp::osqp() osqp package perform optimization. method may slow memory-intensive large datasets.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_energy.html","id":"binary-treatments","dir":"Reference","previous_headings":"","what":"Binary Treatments","title":"Energy Balancing — method_energy","text":"binary treatments, method estimates weights using osqp() using formulas described Huling Mak (2022). following estimands allowed: ATE, ATT, ATC.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_energy.html","id":"multinomial-treatments","dir":"Reference","previous_headings":"","what":"Multinomial Treatments","title":"Energy Balancing — method_energy","text":"multinomial treatments, method estimates weights using osqp() using formulas described Huling Mak (2022). following estimands allowed: ATE ATT.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_energy.html","id":"continuous-treatments","dir":"Reference","previous_headings":"","what":"Continuous Treatments","title":"Energy Balancing — method_energy","text":"continuous treatments, method estimates weights using osqp() using formulas described Huling, Greifer, Chen (2021).","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_energy.html","id":"longitudinal-treatments","dir":"Reference","previous_headings":"","what":"Longitudinal Treatments","title":"Energy Balancing — method_energy","text":"longitudinal treatments, weights product weights estimated time point. method guaranteed yield optimal balance time point. NOTE: use energy balancing longitudinal treatments validated!","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_energy.html","id":"sampling-weights","dir":"Reference","previous_headings":"","what":"Sampling Weights","title":"Energy Balancing — method_energy","text":"Sampling weights supported s.weights scenarios. cases, sampling weights cause optimization fail due lack convexity infeasible constraints.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_energy.html","id":"missing-data","dir":"Reference","previous_headings":"","what":"Missing Data","title":"Energy Balancing — method_energy","text":"presence missing data, following value(s) missing allowed: \"ind\" (default) First, variable missingness, new missingness indicator variable created takes value 1 original covariate NA 0 otherwise. missingness indicators added model formula main effects. missing values covariates replaced covariate medians (value arbitrary affect estimation). weight estimation proceeds new formula set covariates. covariates output resulting weightit object original covariates NAs.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_energy.html","id":"additional-arguments","dir":"Reference","previous_headings":"","what":"Additional Arguments","title":"Energy Balancing — method_energy","text":"following following additional arguments can specified: dist.mat name method used compute distance matrix covariates numeric distance matrix . Allowable options include \"scaled_euclidean\" Euclidean (L2) distance scaled covariates (default), \"mahalanobis\" Mahalanobis distance, \"euclidean\" raw Euclidean distance. Abbreviations allowed. Note user-supplied distance matrices can cause R session abort due bug within osqp, argument used caution. distance matrix must square, symmetric, numeric matrix zeros along diagonal row column unit. Can also supplied output call dist(). lambda positive numeric scalar used penalize square weights. value divided square total sample size added diagonal quadratic part loss function. Higher values favor weights less variability. Note distinct lambda value described Huling Mak (2022), penalizes complexity individual treatment rules rather weights, correspond lambda Huling et al. (2021). Default .0001, essentially 0. binary multinomial treatments, following additional argument can specified: improved logical; whether use improved energy balancing weights described Huling Mak (2022) estimand = \"ATE\". involves optimizing balance treatment group overall sample, also pair treatment groups. Huling Mak (2022) found improved energy balancing weights generally outperformed standard energy balancing. Default TRUE; set FALSE use standard energy balancing weights instead (recommended). continuous treatments, following additional arguments can specified: d.moments number moments treatment covariate distributions constrained weighted sample original sample. example, setting d.moments = 3 ensures mean, variance, skew treatment covariates weighted sample unweighted sample. d.moments greater equal moments automatically set accordingly (specified). dimension.adj logical; whether include dimensionality adjustment described Huling et al. (2021). TRUE, default, energy distance covariates weighted \\(\\sqrt{p}\\) times much energy distance treatment, \\(p\\) number covariates. FALSE, two energy distances given equal weights. Default TRUE. moments argument functions differently method = \"energy\" methods. unspecified set zero, energy balancing weights estimated described Huling Mak (2022) binary multi-category treatments Huling et al. (2021) continuous treatments. moments set integer larger 0, additional balance constraints requested moments covariates also included, guaranteeing exact moment balance covariates minimizing energy distance weighted sample. binary multinomial treatments, involves exact balance means entered covariates; continuous treatments, involves exact balance treatment-covariate correlations entered covariates.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_energy.html","id":"additional-outputs","dir":"Reference","previous_headings":"","what":"Additional Outputs","title":"Energy Balancing — method_energy","text":"obj include.obj = TRUE, output call osqp::solve_osqp(), contains dual variables convergence information.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_energy.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Energy Balancing — method_energy","text":"Energy balancing method estimating weights using optimization without propensity score. weights solution constrain quadratic optimization problem objective function concerns covariate balance measured energy distance (continuous treatments) distance covariance. Energy balancing binary multi-category treatments involves minimizing energy distance treatment groups treatment group target group (e.g., full sample ATE). energy distance scalar measure difference two multivariate distributions equal 0 two distributions identical. Energy balancing continuous treatments involves minimizing distance covariance treatment covariates; distance covariance scalar measure association two (possibly multivariate) distributions equal 0 two distributions independent. addition, energy distances treatment covariate distributions weighted sample treatment covariate distributions original sample minimized. primary benefit energy balancing features covariate distribution balanced, just means, optimization-based methods like entropy balancing. Still, possible add additional balance constraints require balance individual terms using moments argument, just like entropy balancing. Energy balancing can sometimes yield weights high variability; lambda argument can supplied penalize highly variable weights increase effective sample size expense balance.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_energy.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Energy Balancing — method_energy","text":"Sometimes optimization can fail converge problem convex. warning displayed . cases, try simply re-fitting weights without changing anything. method repeatedly fails, try another method change supplied parameters (though uncommon). Increasing max_iter might help. seems like weights balancing covariates still get failure converge, usually indicates iterations needs find optimal solutions. can occur moments int specified. max_iter increased, setting verbose = TRUE allows monitor process examine optimization approaching convergence.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_energy.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Energy Balancing — method_energy","text":"Noah Greifer, using code Jared Huling's independenceWeights package continuous treatments.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_energy.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Energy Balancing — method_energy","text":"Binary Multinomial treatments Huling, J. D., & Mak, S. (2022). Energy Balancing Covariate Distributions (arXiv:2004.13962). arXiv. doi:10.48550/arXiv.2004.13962 Continuous treatments Huling, J. D., Greifer, N., & Chen, G. (2021). Independence weights causal inference continuous exposures (arXiv:2107.07086). arXiv. doi:10.48550/arXiv.2107.07086","code":""},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/reference/method_energy.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Energy Balancing — method_energy","text":"","code":"library(\"cobalt\") data(\"lalonde\", package = \"cobalt\")  #Examples may not converge, but may after several runs if (FALSE) { #Balancing covariates between treatment groups (binary) (W1 <- weightit(treat ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"energy\", estimand = \"ATE\")) summary(W1) bal.tab(W1)  #Balancing covariates with respect to race (multinomial) (W2 <- weightit(race ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"energy\", estimand = \"ATT\",                 focal = \"black\")) summary(W2) bal.tab(W2)   #Balancing covariates with respect to re75 (continuous) (W3 <- weightit(re75 ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"energy\", moments = 1)) summary(W3) bal.tab(W3, poly = 2) }"},{"path":"https://ngreifer.github.io/WeightIt/reference/method_gbm.html","id":null,"dir":"Reference","previous_headings":"","what":"Propensity Score Weighting Using Generalized Boosted Models — method_gbm","title":"Propensity Score Weighting Using Generalized Boosted Models — method_gbm","text":"page explains details estimating weights generalized boosted model-based propensity scores setting method = \"gbm\" call weightit() weightitMSM(). method can used binary, multinomial, continuous treatments. general, method relies estimating propensity scores using generalized boosted modeling converting propensity scores weights using formula depends desired estimand. algorithm involves using balance-based prediction-based criterion optimize choosing value tuning parameters (number trees possibly others). method relies gbm package. method mimics functionality functions twang package, improved performance flexible options. See Details section details.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_gbm.html","id":"binary-treatments","dir":"Reference","previous_headings":"","what":"Binary Treatments","title":"Propensity Score Weighting Using Generalized Boosted Models — method_gbm","text":"binary treatments, method estimates propensity scores using gbm::gbm.fit() selects optimal tuning parameter values using method specified criterion argument. following estimands allowed: ATE, ATT, ATC, ATO, ATM. weights computed estimated propensity scores using get_w_from_ps(), implements standard formulas. Weights can also computed using marginal mean weighting stratification ATE, ATT, ATC. See get_w_from_ps() details.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_gbm.html","id":"multinomial-treatments","dir":"Reference","previous_headings":"","what":"Multinomial Treatments","title":"Propensity Score Weighting Using Generalized Boosted Models — method_gbm","text":"multinomial treatments, method estimates propensity scores using gbm::gbm.fit() distribution = \"multinomial\" selects optimal tuning parameter values using method specified criterion argument. following estimands allowed: ATE, ATT, ATC, ATO, ATM. weights computed estimated propensity scores using get_w_from_ps(), implements standard formulas. Weights can also computed using marginal mean weighting stratification ATE, ATT, ATC. See get_w_from_ps() details.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_gbm.html","id":"continuous-treatments","dir":"Reference","previous_headings":"","what":"Continuous Treatments","title":"Propensity Score Weighting Using Generalized Boosted Models — method_gbm","text":"continuous treatments, method estimates generalized propensity score using gbm::gbm.fit() selects optimal tuning parameter values using method specified criterion argument.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_gbm.html","id":"longitudinal-treatments","dir":"Reference","previous_headings":"","what":"Longitudinal Treatments","title":"Propensity Score Weighting Using Generalized Boosted Models — method_gbm","text":"longitudinal treatments, weights product weights estimated time point.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_gbm.html","id":"sampling-weights","dir":"Reference","previous_headings":"","what":"Sampling Weights","title":"Propensity Score Weighting Using Generalized Boosted Models — method_gbm","text":"Sampling weights supported s.weights scenarios.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_gbm.html","id":"missing-data","dir":"Reference","previous_headings":"","what":"Missing Data","title":"Propensity Score Weighting Using Generalized Boosted Models — method_gbm","text":"presence missing data, following value(s) missing allowed: \"ind\" (default) First, variable missingness, new missingness indicator variable created takes value 1 original covariate NA 0 otherwise. missingness indicators added model formula main effects. weight estimation proceeds new formula set covariates using surrogate splitting described . covariates output resulting weightit object original covariates NAs. \"surr\" Surrogate splitting used process NAs. missingness indicators created. Nodes split using non-missing values variable. generate predicted values unit, non-missing variable operates similarly variable missingness used surrogate. Missing values ignored calculating balance statistics choose optimal tree.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_gbm.html","id":"additional-arguments","dir":"Reference","previous_headings":"","what":"Additional Arguments","title":"Propensity Score Weighting Using Generalized Boosted Models — method_gbm","text":"following additional arguments can specified: criterion string describing balance criterion used select best weights. See cobalt::bal.compute() allowable options treatment type. addition, optimize cross-validation error instead balance, criterion can set \"cv{#}\", {#} replaced number representing number cross-validation folds used (e.g., \"cv5\" 5-fold cross-validation). binary multinomial treatments, default \"smd.mean\", minimizes average absolute standard mean difference among covariates treatment groups. continuous treatments, default \"p.mean\", minimizes average absolute Pearson correlation treatment covariates. trim.number supplied trim() trims weights trees choosing best tree. can valuable weights extreme, occurs especially continuous treatments. default 0 (.e., trimming). distribution string distribution used loss function boosted model. supplied distribution argument gbm::gbm.fit(). binary treatments, \"bernoulli\" \"adaboost\" available, \"bernoulli\" default. multinomial treatments, \"multinomial\" allowed. continuous treatments \"gaussian\", \"laplace\", \"tdist\" available, \"gaussian\" default. argument tunable. n.trees maximum number trees used. passed onto n.trees argument gbm.fit(). default 10000 binary multinomial treatments 20000 continuous treatments. start.tree tree start balance checking. know best balance first 100 trees, example, can set start.tree = 101 balance statistics computed first 100 trees. can save time since balance checking takes bulk run time balance-based stopping methods, especially useful running model adding trees. default 1, .e., start first tree assessing balance. interaction.depth depth trees. passed onto interaction.depth argument gbm.fit(). Higher values indicate better ability capture nonlinear nonadditive relationships. default 3 binary multinomial treatments 4 continuous treatments. argument tunable. shrinkage shrinkage parameter applied trees. passed onto shrinkage argument gbm.fit(). default .01 binary multinomial treatments .0005 continuous treatments. lower value , trees one may include reach optimum. argument tunable. bag.fraction fraction units randomly selected propose next tree expansion. passed onto bag.fraction argument gbm.fit(). default 1, smaller values tried. values less 1, subsequent runs parameters yield different results due random sampling; sure seed seed using set.seed() ensure replicability results. arguments take defaults gbm::gbm.fit(), used . w argument gbm.fit() ignored sampling weights passed using s.weights. continuous treatments , following arguments may supplied: density function corresponding conditional density treatment. standardized residuals treatment model fed function produce numerator denominator generalized propensity score weights. blank, dnorm() used recommended Robins et al. (2000). can also supplied string containing name function called. string contains underscores, call split underscores latter splits supplied arguments second argument beyond. example, density = \"dt_2\" specified, density used t-distribution 2 degrees freedom. Using t-distribution can useful extreme outcome values observed (Naimi et al., 2014). Ignored use.kernel = TRUE (described ). use.kernel TRUE, uses kernel density estimation density() function estimate numerator denominator densities weights. FALSE (default), argument density parameter used instead. bw, adjust, kernel, n use.kernel = TRUE, arguments density(). defaults density except n 10 times number units sample. plot use.kernel = TRUE continuous treatments, whether plot estimated density. tunable arguments, multiple entries may supplied, weightit() choose best value optimizing criterion specified criterion. See additional outputs included arguments supplied tuned. See Examples example tuning.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_gbm.html","id":"additional-outputs","dir":"Reference","previous_headings":"","what":"Additional Outputs","title":"Propensity Score Weighting Using Generalized Boosted Models — method_gbm","text":"info list following entries: best.tree number trees optimum. close n.trees, weightit() rerun larger value n.trees, start.tree can set just best.tree. parameters tuned, best tree value best combination tuned parameters. See example. tree.val data frame two columns: first number trees second value criterion corresponding tree. Running plot() object plot criterion number trees good way see patterns relationship determine trees needed. parameters tuned, number trees criterion values best combination tuned parameters. See example. arguments tuned (.e., supplied one value), following two additional components included info: tune data frame column argument tuned, best value balance criterion given combination parameters, number trees best value reached. best.tune one-row data frame containing values arguments tuned ultimately selected estimate returned weights. obj include.obj = TRUE, gbm fit used generate predicted values.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_gbm.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Propensity Score Weighting Using Generalized Boosted Models — method_gbm","text":"Generalized boosted modeling (GBM, also known gradient boosting machines) machine learning method generates predicted values flexible regression treatment covariates, treated propensity scores used compute weights. building series regression trees, fit residuals last, minimizing loss function depends distribution chosen. optimal number trees tuning parameter must chosen; McCaffrey et al. (2004) innovative using covariate balance select value rather traditional machine learning performance metrics cross-validation accuracy. GBM particularly effective fitting nonlinear treatment models characterized curves interactions, performs worse simpler treatment models. unclear balance measure used select number trees, though research indicated balance measures tend perform better cross-validation accuracy estimating effective propensity score weights. WeightIt offers almost identical functionality twang, first package implement method. Compared current version twang, WeightIt offers options measure balance used select number trees, improved performance, tuning hyperparameters, estimands, support continuous treatments. WeightIt computes weights multinomial treatments differently twang ; rather fitting separate binary GBM pair treatments, WeightIt fits single multi-class GBM model uses balance measures appropriate multinomial treatments.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_gbm.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Propensity Score Weighting Using Generalized Boosted Models — method_gbm","text":"criterion argument used called stop.method, name twang. stop.method still works backward compatibility. Additionally, criteria formerly named es.mean, es.max, es.rms renamed smd.mean, smd.max, smd.rms. former used twang still work weightit() backward compatibility.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_gbm.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Propensity Score Weighting Using Generalized Boosted Models — method_gbm","text":"Binary treatments McCaffrey, D. F., Ridgeway, G., & Morral, . R. (2004). Propensity Score Estimation Boosted Regression Evaluating Causal Effects Observational Studies. Psychological Methods, 9(4), 403–425. doi:10.1037/1082-989X.9.4.403 Multinomial Treatments McCaffrey, D. F., Griffin, B. ., Almirall, D., Slaughter, M. E., Ramchand, R., & Burgette, L. F. (2013). Tutorial Propensity Score Estimation Multiple Treatments Using Generalized Boosted Models. Statistics Medicine, 32(19), 3388–3414. doi:10.1002/sim.5753 Continuous treatments Zhu, Y., Coffman, D. L., & Ghosh, D. (2015). Boosting Algorithm Estimating Generalized Propensity Scores Continuous Treatments. Journal Causal Inference, 3(1). doi:10.1515/jci-2014-0022","code":""},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/reference/method_gbm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Propensity Score Weighting Using Generalized Boosted Models — method_gbm","text":"","code":"library(\"cobalt\") data(\"lalonde\", package = \"cobalt\")  #Balancing covariates between treatment groups (binary) (W1 <- weightit(treat ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"gbm\", estimand = \"ATE\",                 criterion = \"smd.max\")) #> A weightit object #>  - method: \"gbm\" (propensity score weighting with GBM) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 2-category #>  - estimand: ATE #>  - covariates: age, educ, married, nodegree, re74 summary(W1) #>                  Summary of weights #>  #> - Weight ranges: #>  #>            Min                                   Max #> treated 1.1137 |---------------------------| 12.6196 #> control 1.0147 |----------------|             8.4841 #>  #> - Units with the 5 most extreme weights by group: #>                                               #>             115     177    184    183     182 #>  treated 9.4968 10.7796 11.052 11.052 12.6196 #>             585     569    592    374     608 #>  control 4.2564  5.3573 5.5494 5.5494  8.4841 #>  #> - Weight statistics: #>  #>         Coef of Var   MAD Entropy # Zeros #> treated       0.848 0.514   0.244       0 #> control       0.460 0.233   0.066       0 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted  429.    185.   #> Weighted    354.26  107.84 bal.tab(W1) #> Balance Measures #>                Type Diff.Adj #> prop.score Distance   0.7940 #> age         Contin.  -0.1999 #> educ        Contin.   0.0829 #> married      Binary  -0.0867 #> nodegree     Binary   0.0473 #> re74        Contin.  -0.0599 #>  #> Effective sample sizes #>            Control Treated #> Unadjusted  429.    185.   #> Adjusted    354.26  107.84  if (FALSE) { #Balancing covariates with respect to race (multinomial) (W2 <- weightit(race ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"gbm\", estimand = \"ATT\",                 focal = \"hispan\", criterion = \"ks.mean\")) summary(W2) bal.tab(W2, stats = c(\"m\", \"ks\"))  #Balancing covariates with respect to re75 (continuous) (W3 <- weightit(re75 ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"gbm\", use.kernel = TRUE,                 criterion = \"p.rms\", trim.at = .97)) summary(W3) bal.tab(W3)  #Using a t(3) density and illustrating the search for #more trees. W4a <- weightit(re75 ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"gbm\", density = \"dt_3\",                 criterion = \"p.max\",                 n.trees = 10000)  W4a$info$best.tree #10000; optimum hasn't been found plot(W4a$info$tree.val, type = \"l\") #decreasing at right edge  W4b <- weightit(re75 ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"gbm\", density = \"dt_3\",                 criterion = \"p.max\",                 start.tree = 10000,                 n.trees = 20000)  W4b$info$best.tree #13417; optimum has been found plot(W4b$info$tree.val, type = \"l\") #increasing at right edge  bal.tab(W4b)  #Tuning hyperparameters (W5 <- weightit(treat ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"gbm\", estimand = \"ATT\",                 criterion = \"ks.max\",                 interaction.depth = 2:4,                 distribution = c(\"bernoulli\", \"adaboost\")))  W5$info$tune  W5$info$best.tune #Best values of tuned parameters  bal.tab(W5, stats = c(\"m\", \"ks\")) }"},{"path":"https://ngreifer.github.io/WeightIt/reference/method_glm.html","id":null,"dir":"Reference","previous_headings":"","what":"Propensity Score Weighting Using Generalized Linear Models — method_glm","title":"Propensity Score Weighting Using Generalized Linear Models — method_glm","text":"page explains details estimating weights generalized linear model-based propensity scores setting method = \"glm\" call weightit() weightitMSM(). method can used binary, multinomial, continuous treatments. (method used requested method = \"ps\", still works.) general, method relies estimating propensity scores parametric generalized linear model converting propensity scores weights using formula depends desired estimand. binary multinomial treatments, binomial multinomial regression model used estimate propensity scores predicted probability treatment given covariates. ordinal treatments, ordinal regression model used estimate generalized propensity scores. continuous treatments, generalized linear model used estimate generalized propensity scores conditional density treatment given covariates.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_glm.html","id":"binary-treatments","dir":"Reference","previous_headings":"","what":"Binary Treatments","title":"Propensity Score Weighting Using Generalized Linear Models — method_glm","text":"binary treatments, method estimates propensity scores using glm(). additional argument link, uses options link family(). default link \"logit\", others, including \"probit\", allowed. following estimands allowed: ATE, ATT, ATC, ATO, ATM, ATOS. Weights can also computed using marginal mean weighting stratification ATE, ATT, ATC. See get_w_from_ps() details.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_glm.html","id":"multinomial-treatments","dir":"Reference","previous_headings":"","what":"Multinomial Treatments","title":"Propensity Score Weighting Using Generalized Linear Models — method_glm","text":"multinomial treatments, propensity scores estimated using multinomial regression one functions depending requested link: logit (\"logit\") probit (\"probit\") links, mlogit::mlogit() mlogit package used; Bayesian probit (\"bayes.probit\") link, MNP::mnp() MNP package used; biased-reduced multinomial logistic regression (\"br.logit\"), brglm2::brmultinom() brglm2 package used. treatment variable ordered factor, MASS::polr() MASS package used fit ordinal regression unless link = \"br.logit\", case brglm2::bracl() brglm2 used. methods allowed method argument polr() can supplied link. following estimands allowed: ATE, ATT, ATC, ATO, ATM. weights estimand computed using standard formulas mentioned . Weights can also computed using marginal mean weighting stratification ATE, ATT, ATC. See get_w_from_ps() details.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_glm.html","id":"continuous-treatments","dir":"Reference","previous_headings":"","what":"Continuous Treatments","title":"Propensity Score Weighting Using Generalized Linear Models — method_glm","text":"continuous treatments, generalized propensity score estimated using linear regression. conditional density can specified normal another distribution. addition, kernel density estimation can used instead assuming specific density numerator denominator generalized propensity score setting use.kernel = TRUE. arguments density() can specified refine density estimation parameters. plot = TRUE can specified plot density numerator denominator, can helpful diagnosing extreme weights.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_glm.html","id":"longitudinal-treatments","dir":"Reference","previous_headings":"","what":"Longitudinal Treatments","title":"Propensity Score Weighting Using Generalized Linear Models — method_glm","text":"longitudinal treatments, weights product weights estimated time point.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_glm.html","id":"sampling-weights","dir":"Reference","previous_headings":"","what":"Sampling Weights","title":"Propensity Score Weighting Using Generalized Linear Models — method_glm","text":"Sampling weights supported s.weights scenarios except multinomial treatments link = \"bayes.probit\" binary continuous treatments missing = \"saem\" (see ). Warning messages may appear otherwise non-integer successes, can ignored.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_glm.html","id":"missing-data","dir":"Reference","previous_headings":"","what":"Missing Data","title":"Propensity Score Weighting Using Generalized Linear Models — method_glm","text":"presence missing data, following value(s) missing allowed: \"ind\" (default) First, variable missingness, new missingness indicator variable created takes value 1 original covariate NA 0 otherwise. missingness indicators added model formula main effects. missing values covariates replaced covariate medians (value arbitrary affect estimation). weight estimation proceeds new formula set covariates. covariates output resulting weightit object original covariates NAs. \"saem\" binary treatments link = \"logit\" continuous treatments, stochastic approximation version EM algorithm (SAEM) used via misaem package. additional covariates created. See Jiang et al. (2019) information method. cases, suitable alternative multiple imputation.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_glm.html","id":"additional-arguments","dir":"Reference","previous_headings":"","what":"Additional Arguments","title":"Propensity Score Weighting Using Generalized Linear Models — method_glm","text":"following additional arguments can specified: link link used generalized linear model propensity scores. binary treatments, link can allowed binomial(). br. prefix can added (e.g., \"br.logit\"); changes fitting method bias-corrected generalized linear models implemented brglm2 package. multicategory treatments, link can \"logit\", \"probit\", \"bayes.probit\", \"br.logit\". ordered treatments, link can allowed method argument MASS::polr() \"br.logit\". continuous treatments, link can allowed gaussian(). continuous treatments , following arguments may supplied: density function corresponding conditional density treatment. standardized residuals treatment model fed function produce numerator denominator generalized propensity score weights. blank, dnorm() used recommended Robins et al. (2000). can also supplied string containing name function called. string contains underscores, call split underscores latter splits supplied arguments second argument beyond. example, density = \"dt_2\" specified, density used t-distribution 2 degrees freedom. Using t-distribution can useful extreme outcome values observed (Naimi et al., 2014). Ignored use.kernel = TRUE (described ). use.kernel TRUE, uses kernel density estimation density() function estimate numerator denominator densities weights. FALSE, argument density parameter used instead. bw, adjust, kernel, n use.kernel = TRUE, arguments density() function. defaults density except n 10 times number units sample. plot use.kernel = TRUE continuous treatments, whether plot estimated density. binary treatments, additional arguments glm() can specified well. method argument glm() renamed glm.method. can used supply alternative fitting functions, implemented glm2 package. arguments weightit() passed ... glm(). presence missing data link = \"logit\" missing = \"saem\", additional arguments passed miss.glm predict.miss.glm, except method argument predict.miss.glm replaced saem.method. multi-category treatments link = \"logit\" \"probit\", default use multinomial logistic probit regression using mlogit package. request separate binary logistic probit regressions run instead, set use.mlogit = FALSE. can helpful mlogit slow fails converge. link = \"logit\", option use.mclogit = TRUE can specified request mclogit::mblogit() mclogit package used instead, can faster recommended. continuous treatments presence missing data missing = \"saem\", additional arguments passed miss.lm predict.miss.lm.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_glm.html","id":"additional-outputs","dir":"Reference","previous_headings":"","what":"Additional Outputs","title":"Propensity Score Weighting Using Generalized Linear Models — method_glm","text":"obj include.obj = TRUE, (generalized) propensity score model fit. binary treatments, output call glm(). ordinal treatments, output call MASS::polr(). multinomial treatments link = \"logit\" \"probit\" use.mlogit = TRUE, output call mlogit::mlogit(). multinomial treatments use.mlogit = FALSE, list glm() fits. multinomial treatments link = \"br.logit\", output call brglm2::brmultinom(). multinomial treatments link = \"bayes.probit\", output call MNP::mnp(). continuous treatments, output call glm() predicted values denominator density.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_glm.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Propensity Score Weighting Using Generalized Linear Models — method_glm","text":"Binary treatments - estimand = \"ATO\" Li, F., Morgan, K. L., & Zaslavsky, . M. (2018). Balancing covariates via propensity score weighting. Journal American Statistical Association, 113(521), 390–400. doi:10.1080/01621459.2016.1260466 - estimand = \"ATM\" Li, L., & Greene, T. (2013). Weighting Analogue Pair Matching Propensity Score Analysis. International Journal Biostatistics, 9(2). doi:10.1515/ijb-2012-0030 - estimand = \"ATOS\" Crump, R. K., Hotz, V. J., Imbens, G. W., & Mitnik, O. . (2009). Dealing limited overlap estimation average treatment effects. Biometrika, 96(1), 187–199. doi:10.1093/biomet/asn055 - estimands Austin, P. C. (2011). Introduction Propensity Score Methods Reducing Effects Confounding Observational Studies. Multivariate Behavioral Research, 46(3), 399–424. doi:10.1080/00273171.2011.568786 - Marginal mean weighting stratification Hong, G. (2010). Marginal mean weighting stratification: Adjustment selection bias multilevel data. Journal Educational Behavioral Statistics, 35(5), 499–531. doi:10.3102/1076998609359785 - Bias-reduced logistic regression See references brglm2 package. - SAEM logistic regression missing data Jiang, W., Josse, J., & Lavielle, M. (2019). Logistic regression missing covariates — Parameter estimation, model selection prediction within joint-modeling framework. Computational Statistics & Data Analysis, 106907. doi:10.1016/j.csda.2019.106907 Multinomial Treatments - estimand = \"ATO\" Li, F., & Li, F. (2019). Propensity score weighting causal inference multiple treatments. Annals Applied Statistics, 13(4), 2389–2415. doi:10.1214/19-AOAS1282 - estimand = \"ATM\" Yoshida, K., Hernández-Díaz, S., Solomon, D. H., Jackson, J. W., Gagne, J. J., Glynn, R. J., & Franklin, J. M. (2017). Matching weights simultaneously compare three treatment groups: Comparison three-way matching. Epidemiology (Cambridge, Mass.), 28(3), 387–395. doi:10.1097/EDE.0000000000000627 - estimands McCaffrey, D. F., Griffin, B. ., Almirall, D., Slaughter, M. E., Ramchand, R., & Burgette, L. F. (2013). Tutorial Propensity Score Estimation Multiple Treatments Using Generalized Boosted Models. Statistics Medicine, 32(19), 3388–3414. doi:10.1002/sim.5753 - Marginal mean weighting stratification Hong, G. (2012). Marginal mean weighting stratification: generalized method evaluating multivalued multiple treatments nonexperimental data. Psychological Methods, 17(1), 44–60. doi:10.1037/a0024918 Continuous treatments Robins, J. M., Hernán, M. Á., & Brumback, B. (2000). Marginal Structural Models Causal Inference Epidemiology. Epidemiology, 11(5), 550–560. - Using non-normal conditional densities Naimi, . ., Moodie, E. E. M., Auger, N., & Kaufman, J. S. (2014). Constructing Inverse Probability Weights Continuous Exposures: Comparison Methods. Epidemiology, 25(2), 292–299. doi:10.1097/EDE.0000000000000053 - SAEM linear regression missing data Jiang, W., Josse, J., & Lavielle, M. (2019). Logistic regression missing covariates — Parameter estimation, model selection prediction within joint-modeling framework. Computational Statistics & Data Analysis, 106907. doi:10.1016/j.csda.2019.106907","code":""},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/reference/method_glm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Propensity Score Weighting Using Generalized Linear Models — method_glm","text":"","code":"library(\"cobalt\") data(\"lalonde\", package = \"cobalt\")  #Balancing covariates between treatment groups (binary) (W1 <- weightit(treat ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"glm\", estimand = \"ATT\",                 link = \"probit\")) #> A weightit object #>  - method: \"glm\" (propensity score weighting with GLM) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 2-category #>  - estimand: ATT (focal: 1) #>  - covariates: age, educ, married, nodegree, re74 summary(W1) #>                  Summary of weights #>  #> - Weight ranges: #>  #>            Min                                  Max #> treated 1.0000                 ||            1.0000 #> control 0.0176 |---------------------------| 1.8338 #>  #> - Units with the 5 most extreme weights by group: #>                                          #>              10     8     5     4      1 #>  treated      1     1     1     1      1 #>             612   595   269   409    296 #>  control 1.2781 1.351 1.412 1.518 1.8338 #>  #> - Weight statistics: #>  #>         Coef of Var   MAD Entropy # Zeros #> treated       0.000 0.000  -0.000       0 #> control       0.804 0.691   0.322       0 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted  429.       185 #> Weighted    260.83     185 bal.tab(W1) #> Balance Measures #>                Type Diff.Adj #> prop.score Distance   0.0252 #> age         Contin.   0.0716 #> educ        Contin.  -0.0565 #> married      Binary   0.0058 #> nodegree     Binary   0.0128 #> re74        Contin.  -0.0507 #>  #> Effective sample sizes #>            Control Treated #> Unadjusted  429.       185 #> Adjusted    260.83     185  #Balancing covariates with respect to race (multinomial) (W2 <- weightit(race ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"glm\", estimand = \"ATE\",                 use.mlogit = FALSE)) #> A weightit object #>  - method: \"glm\" (propensity score weighting with GLM) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 3-category (black, hispan, white) #>  - estimand: ATE #>  - covariates: age, educ, married, nodegree, re74 summary(W2) #>                  Summary of weights #>  #> - Weight ranges: #>  #>           Min                                   Max #> black  1.4486 |---------------------------| 29.1514 #> hispan 1.8148  |------------------------|   26.9274 #> white  1.1180 |-|                            4.1126 #>  #> - Units with the 5 most extreme weights by group: #>                                                 #>             226     231     485     181     182 #>   black  7.6815  7.8028   8.566 12.2152 29.1514 #>             346     392     345     269     371 #>  hispan 17.5087 18.3157 18.4663 21.5593 26.9274 #>             432      68     404     599     531 #>   white  3.8059  3.8309  3.8835  4.0487  4.1126 #>  #> - Weight statistics: #>  #>        Coef of Var   MAD Entropy # Zeros #> black        0.861 0.426   0.188       0 #> hispan       0.546 0.407   0.134       0 #> white        0.385 0.313   0.069       0 #>  #> - Effective Sample Sizes: #>  #>             black hispan  white #> Unweighted 243.    72.   299.   #> Weighted   139.74  55.66 260.44 bal.tab(W2) #> Balance summary across all treatment pairs #>             Type Max.Diff.Adj #> age      Contin.       0.0374 #> educ     Contin.       0.1187 #> married   Binary       0.0540 #> nodegree  Binary       0.0510 #> re74     Contin.       0.1682 #>  #> Effective sample sizes #>             black hispan  white #> Unadjusted 243.    72.   299.   #> Adjusted   139.74  55.66 260.44  #Balancing covariates with respect to re75 (continuous) (W3 <- weightit(re75 ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"glm\", use.kernel = TRUE)) #> A weightit object #>  - method: \"glm\" (propensity score weighting with GLM) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: continuous #>  - covariates: age, educ, married, nodegree, re74 summary(W3) #>                  Summary of weights #>  #> - Weight ranges: #>  #>       Min                                   Max #> all 0.088 |---------------------------| 83.1017 #>  #> - Units with the 5 most extreme weights: #>                                            #>         482     481    484     483     485 #>  all 56.933 56.9865 70.237 77.1173 83.1017 #>  #> - Weight statistics: #>  #>     Coef of Var   MAD Entropy # Zeros #> all       2.908 1.049   1.156       0 #>  #> - Effective Sample Sizes: #>  #>             Total #> Unweighted 614.   #> Weighted    65.03 bal.tab(W3) #> Balance Measures #>             Type Corr.Adj #> age      Contin.  -0.0501 #> educ     Contin.   0.0016 #> married   Binary  -0.0427 #> nodegree  Binary  -0.0196 #> re74     Contin.  -0.0773 #>  #> Effective sample sizes #>             Total #> Unadjusted 614.   #> Adjusted    65.03"},{"path":"https://ngreifer.github.io/WeightIt/reference/method_npcbps.html","id":null,"dir":"Reference","previous_headings":"","what":"Nonparametric Covariate Balancing Propensity Score Weighting — method_npcbps","title":"Nonparametric Covariate Balancing Propensity Score Weighting — method_npcbps","text":"page explains details estimating weights nonparametric covariate balancing propensity scores setting method = \"npcbps\" call weightit() weightitMSM(). method can used binary, multinomial, continuous treatments. general, method relies estimating weights maximizing empirical likelihood data subject balance constraints. method relies CBPS::npCBPS() CBPS package.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_npcbps.html","id":"binary-treatments","dir":"Reference","previous_headings":"","what":"Binary Treatments","title":"Nonparametric Covariate Balancing Propensity Score Weighting — method_npcbps","text":"binary treatments, method estimates weights using CBPS::npCBPS(). ATE estimand allowed. weights taken output npCBPS fit object.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_npcbps.html","id":"multinomial-treatments","dir":"Reference","previous_headings":"","what":"Multinomial Treatments","title":"Nonparametric Covariate Balancing Propensity Score Weighting — method_npcbps","text":"multinomial treatments, method estimates weights using CBPS::npCBPS(). ATE estimand allowed. weights taken output npCBPS fit object.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_npcbps.html","id":"continuous-treatments","dir":"Reference","previous_headings":"","what":"Continuous Treatments","title":"Nonparametric Covariate Balancing Propensity Score Weighting — method_npcbps","text":"continuous treatments, method estimates weights using CBPS::npCBPS(). weights taken output npCBPS fit object.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_npcbps.html","id":"longitudinal-treatments","dir":"Reference","previous_headings":"","what":"Longitudinal Treatments","title":"Nonparametric Covariate Balancing Propensity Score Weighting — method_npcbps","text":"longitudinal treatments, weights product weights estimated time point. CBMSM CBPS package estimates weights longitudinal treatments.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_npcbps.html","id":"sampling-weights","dir":"Reference","previous_headings":"","what":"Sampling Weights","title":"Nonparametric Covariate Balancing Propensity Score Weighting — method_npcbps","text":"Sampling weights supported method = \"npcbps\".","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_npcbps.html","id":"missing-data","dir":"Reference","previous_headings":"","what":"Missing Data","title":"Nonparametric Covariate Balancing Propensity Score Weighting — method_npcbps","text":"presence missing data, following value(s) missing allowed: \"ind\" (default) First, variable missingness, new missingness indicator variable created takes value 1 original covariate NA 0 otherwise. missingness indicators added model formula main effects. missing values covariates replaced covariate medians (value arbitrary affect estimation). weight estimation proceeds new formula set covariates. covariates output resulting weightit object original covariates NAs.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_npcbps.html","id":"additional-arguments","dir":"Reference","previous_headings":"","what":"Additional Arguments","title":"Nonparametric Covariate Balancing Propensity Score Weighting — method_npcbps","text":"arguments npCBPS() can passed weightit() weightitMSM(). arguments take defaults npCBPS().","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_npcbps.html","id":"additional-outputs","dir":"Reference","previous_headings":"","what":"Additional Outputs","title":"Nonparametric Covariate Balancing Propensity Score Weighting — method_npcbps","text":"obj include.obj = TRUE, nonparametric CB(G)PS model fit. output call CBPS::npCBPS().","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_npcbps.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Nonparametric Covariate Balancing Propensity Score Weighting — method_npcbps","text":"Nonparametric CBPS involves specification constrained optimization problem weights. constraints correspond covariate balance, loss function empirical likelihood data given weights. npCBPS similar entropy balancing generally produce similar results. optimization problem npCBPS convex can slow converge converge , approximate balance allowed instead using cor.prior argument, controls average deviation zero correlation treatment covariates allowed.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_npcbps.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Nonparametric Covariate Balancing Propensity Score Weighting — method_npcbps","text":"Fong, C., Hazlett, C., & Imai, K. (2018). Covariate balancing propensity score continuous treatment: Application efficacy political advertisements. Annals Applied Statistics, 12(1), 156–177. doi:10.1214/17-AOAS1101","code":""},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/reference/method_npcbps.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Nonparametric Covariate Balancing Propensity Score Weighting — method_npcbps","text":"","code":"# Examples take a long time to run if (FALSE) { library(\"cobalt\") data(\"lalonde\", package = \"cobalt\")  #Balancing covariates between treatment groups (binary) (W1 <- weightit(treat ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"npcbps\", estimand = \"ATE\")) summary(W1) bal.tab(W1)  #Balancing covariates with respect to race (multinomial) (W2 <- weightit(race ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"npcbps\", estimand = \"ATE\")) summary(W2) bal.tab(W2) }"},{"path":"https://ngreifer.github.io/WeightIt/reference/method_optweight.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimization-Based Weighting — method_optweight","title":"Optimization-Based Weighting — method_optweight","text":"page explains details estimating optimization-based weights 9also known stable balancing weights) setting method = \"optweight\" call weightit() weightitMSM(). method can used binary, multinomial, continuous treatments. general, method relies estimating weights solving quadratic programming problem subject approximate exact balance constraints. method relies optweight::optweight() optweight package. optweight() offers finer control uses syntax weightit(), recommended optweight::optweight() used instead weightit method = \"optweight\".","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_optweight.html","id":"binary-treatments","dir":"Reference","previous_headings":"","what":"Binary Treatments","title":"Optimization-Based Weighting — method_optweight","text":"binary treatments, method estimates weights using optweight::optweight(). following estimands allowed: ATE, ATT, ATC. weights taken output optweight fit object.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_optweight.html","id":"multinomial-treatments","dir":"Reference","previous_headings":"","what":"Multinomial Treatments","title":"Optimization-Based Weighting — method_optweight","text":"multinomial treatments, method estimates weights using optweight::optweight(). following estimands allowed: ATE ATT. weights taken output optweight fit object.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_optweight.html","id":"continuous-treatments","dir":"Reference","previous_headings":"","what":"Continuous Treatments","title":"Optimization-Based Weighting — method_optweight","text":"binary treatments, method estimates weights using optweight::optweight(). weights taken output optweight fit object.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_optweight.html","id":"longitudinal-treatments","dir":"Reference","previous_headings":"","what":"Longitudinal Treatments","title":"Optimization-Based Weighting — method_optweight","text":"longitudinal treatments, optweight() estimates weights simultaneously satisfy balance constraints time points, one model fit obtain weights. Using method = \"optweight\" weightitMSM() causes .MSM.method set TRUE default. Setting FALSE run one model time point multiply weights together, method recommended. NOTE: neither use optimization-based weights longitudinal treatments validated!","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_optweight.html","id":"sampling-weights","dir":"Reference","previous_headings":"","what":"Sampling Weights","title":"Optimization-Based Weighting — method_optweight","text":"Sampling weights supported s.weights scenarios.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_optweight.html","id":"missing-data","dir":"Reference","previous_headings":"","what":"Missing Data","title":"Optimization-Based Weighting — method_optweight","text":"presence missing data, following value(s) missing allowed: \"ind\" (default) First, variable missingness, new missingness indicator variable created takes value 1 original covariate NA 0 otherwise. missingness indicators added model formula main effects. missing values covariates replaced covariate medians (value arbitrary affect estimation). weight estimation proceeds new formula set covariates. covariates output resulting weightit object original covariates NAs.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_optweight.html","id":"additional-arguments","dir":"Reference","previous_headings":"","what":"Additional Arguments","title":"Optimization-Based Weighting — method_optweight","text":"arguments optweight() can passed weightit() weightitMSM(), following exception: targets used ignored. arguments take defaults optweight().","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_optweight.html","id":"additional-outputs","dir":"Reference","previous_headings":"","what":"Additional Outputs","title":"Optimization-Based Weighting — method_optweight","text":"info list one entry: duals data frame dual variables balance constraint. obj include.obj = TRUE, output call optweight::optweight().","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_optweight.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Optimization-Based Weighting — method_optweight","text":"specification tols differs weightit() optweight(). weightit(), one tolerance value included per level factor variable, whereas optweight(), levels factor given tolerance, one value needs supplied factor variable. potential confusion ambiguity, recommended supply one value tols weightit() applies variables. finer control, use optweight() directly. Seriously, just use optweight::optweight(). syntax almost identical compatible cobalt, .","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_optweight.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimization-Based Weighting — method_optweight","text":"Stable balancing weights weights solve constrained optimization problem, constraints correspond covariate balance loss function variance (norm) weights. weights maximize effective sample size weighted sample subject user-supplied balance constraints. advantage method entropy balancing ability allow approximate, rather exact, balance tols argument, can increase precision even slight relaxations constraints.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_optweight.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimization-Based Weighting — method_optweight","text":"Binary Treatments Wang, Y., & Zubizarreta, J. R. (2020). Minimal dispersion approximately balancing weights: Asymptotic properties practical considerations. Biometrika, 107(1), 93–105. doi:10.1093/biomet/asz050 Zubizarreta, J. R. (2015). Stable Weights Balance Covariates Estimation Incomplete Outcome Data. Journal American Statistical Association, 110(511), 910–922. doi:10.1080/01621459.2015.1023805 Multinomial Treatments de los Angeles Resa, M., & Zubizarreta, J. R. (2020). Direct stable weight adjustment non-experimental studies multivalued treatments: Analysis effect earthquake post-traumatic stress. Journal Royal Statistical Society: Series (Statistics Society), n/(n/). doi:10.1111/rssa.12561 Continuous Treatments Greifer, N. (2020). Estimating Balancing Weights Continuous Treatments Using Constrained Optimization. doi:10.17615/DYSS-B342","code":""},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/reference/method_optweight.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimization-Based Weighting — method_optweight","text":"","code":"library(\"cobalt\") data(\"lalonde\", package = \"cobalt\")  #Balancing covariates between treatment groups (binary) (W1 <- weightit(treat ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"optweight\", estimand = \"ATT\",                 tols = 0)) #> A weightit object #>  - method: \"optweight\" (targeted stable balancing weights) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 2-category #>  - estimand: ATT (focal: 1) #>  - covariates: age, educ, married, nodegree, re74 summary(W1) #>                  Summary of weights #>  #> - Weight ranges: #>  #>         Min                                  Max #> treated   1           ||                  1.0000 #> control   0 |---------------------------| 3.0426 #>  #> - Units with the 5 most extreme weights by group: #>                                             #>               5      4      3      2      1 #>  treated      1      1      1      1      1 #>             411    589    269    409    296 #>  control 2.5261 2.5415 2.6434 2.7396 3.0426 #>  #> - Weight statistics: #>  #>         Coef of Var   MAD Entropy # Zeros #> treated       0.000 0.000   0.000       0 #> control       0.788 0.697   0.393      83 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted  429.       185 #> Weighted    264.88     185 bal.tab(W1) #> Balance Measures #>             Type Diff.Adj #> age      Contin.       -0 #> educ     Contin.       -0 #> married   Binary       -0 #> nodegree  Binary        0 #> re74     Contin.       -0 #>  #> Effective sample sizes #>            Control Treated #> Unadjusted  429.       185 #> Adjusted    264.88     185  #Balancing covariates with respect to race (multinomial) (W2 <- weightit(race ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"optweight\", estimand = \"ATE\",                 tols = .01)) #> A weightit object #>  - method: \"optweight\" (targeted stable balancing weights) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 3-category (black, hispan, white) #>  - estimand: ATE #>  - covariates: age, educ, married, nodegree, re74 summary(W2) #>                  Summary of weights #>  #> - Weight ranges: #>  #>           Min                                  Max #> black  0.4429     |-----------------------| 3.5741 #> hispan 0.0000 |-------------------|         2.5848 #> white  0.2574   |---------|                 1.6593 #>  #> - Units with the 5 most extreme weights by group: #>                                            #>            184    190    485    181    182 #>   black 2.3351 2.3723 2.5586 2.8367 3.5741 #>            392    345    269    564    371 #>  hispan 2.0459 2.0984 2.1887 2.1982 2.5848 #>             68    589    324    599    531 #>   white 1.5706 1.5706 1.5725 1.5968 1.6593 #>  #> - Weight statistics: #>  #>        Coef of Var   MAD Entropy # Zeros #> black        0.550 0.443   0.130       0 #> hispan       0.566 0.449   0.176       2 #> white        0.353 0.295   0.065       0 #>  #> - Effective Sample Sizes: #>  #>             black hispan  white #> Unweighted 243.     72.  299.   #> Weighted   186.76   54.7 266.01 bal.tab(W2) #> Balance summary across all treatment pairs #>             Type Max.Diff.Adj #> age      Contin.         0.01 #> educ     Contin.         0.01 #> married   Binary         0.01 #> nodegree  Binary         0.01 #> re74     Contin.         0.01 #>  #> Effective sample sizes #>             black hispan  white #> Unadjusted 243.     72.  299.   #> Adjusted   186.76   54.7 266.01  #Balancing covariates with respect to re75 (continuous) (W3 <- weightit(re75 ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"optweight\", tols = .05)) #> A weightit object #>  - method: \"optweight\" (targeted stable balancing weights) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: continuous #>  - covariates: age, educ, married, nodegree, re74 summary(W3) #>                  Summary of weights #>  #> - Weight ranges: #>  #>     Min                                 Max #> all   0 |---------------------------| 3.966 #>  #> - Units with the 5 most extreme weights: #>                                       #>         482    483   200    178   180 #>  all 2.9566 3.0117 3.048 3.5934 3.966 #>  #> - Weight statistics: #>  #>     Coef of Var   MAD Entropy # Zeros #> all       0.574 0.445   0.173      34 #>  #> - Effective Sample Sizes: #>  #>             Total #> Unweighted 614.   #> Weighted   462.02 bal.tab(W3) #> Balance Measures #>             Type Corr.Adj #> age      Contin.  -0.0087 #> educ     Contin.  -0.0106 #> married   Binary   0.0499 #> nodegree  Binary  -0.0451 #> re74     Contin.   0.0499 #>  #> Effective sample sizes #>             Total #> Unadjusted 614.   #> Adjusted   462.02"},{"path":"https://ngreifer.github.io/WeightIt/reference/method_super.html","id":null,"dir":"Reference","previous_headings":"","what":"Propensity Score Weighting Using SuperLearner — method_super","title":"Propensity Score Weighting Using SuperLearner — method_super","text":"page explains details estimating weights SuperLearner-based propensity scores setting method = \"super\" call weightit() weightitMSM(). method can used binary, multinomial, continuous treatments. general, method relies estimating propensity scores using SuperLearner algorithm stacking predictions converting propensity scores weights using formula depends desired estimand. binary multinomial treatments, one binary classification algorithms used estimate propensity scores predicted probability treatment given covariates. continuous treatments, regression algorithms used estimate generalized propensity scores conditional density treatment given covariates. method relies SuperLearner::SuperLearner() SuperLearner package.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_super.html","id":"binary-treatments","dir":"Reference","previous_headings":"","what":"Binary Treatments","title":"Propensity Score Weighting Using SuperLearner — method_super","text":"binary treatments, method estimates propensity scores using SuperLearner::SuperLearner(). following estimands allowed: ATE, ATT, ATC, ATO, ATM, ATOS. Weights can also computed using marginal mean weighting stratification ATE, ATT, ATC. See get_w_from_ps() details.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_super.html","id":"multinomial-treatments","dir":"Reference","previous_headings":"","what":"Multinomial Treatments","title":"Propensity Score Weighting Using SuperLearner — method_super","text":"multinomial treatments, propensity scores estimated using several calls SuperLearner::SuperLearner(), one treatment group; treatment probabilities normalized sum 1. following estimands allowed: ATE, ATT, ATC, ATO, ATM. weights estimand computed using standard formulas mentioned . Weights can also computed using marginal mean weighting stratification ATE, ATT, ATC. See get_w_from_ps() details.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_super.html","id":"continuous-treatments","dir":"Reference","previous_headings":"","what":"Continuous Treatments","title":"Propensity Score Weighting Using SuperLearner — method_super","text":"continuous treatments, generalized propensity score estimated using SuperLearner::SuperLearner(). addition, kernel density estimation can used instead assuming normal density numerator denominator generalized propensity score setting use.kernel = TRUE. arguments density() can specified refine density estimation parameters. plot = TRUE can specified plot density numerator denominator, can helpful diagnosing extreme weights.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_super.html","id":"longitudinal-treatments","dir":"Reference","previous_headings":"","what":"Longitudinal Treatments","title":"Propensity Score Weighting Using SuperLearner — method_super","text":"longitudinal treatments, weights product weights estimated time point.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_super.html","id":"sampling-weights","dir":"Reference","previous_headings":"","what":"Sampling Weights","title":"Propensity Score Weighting Using SuperLearner — method_super","text":"Sampling weights supported s.weights scenarios.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_super.html","id":"missing-data","dir":"Reference","previous_headings":"","what":"Missing Data","title":"Propensity Score Weighting Using SuperLearner — method_super","text":"presence missing data, following value(s) missing allowed: \"ind\" (default) First, variable missingness, new missingness indicator variable created takes value 1 original covariate NA 0 otherwise. missingness indicators added model formula main effects. missing values covariates replaced covariate medians. weight estimation proceeds new formula set covariates. covariates output resulting weightit object original covariates NAs.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_super.html","id":"additional-arguments","dir":"Reference","previous_headings":"","what":"Additional Arguments","title":"Propensity Score Weighting Using SuperLearner — method_super","text":"discrete TRUE, uses discrete SuperLearner, simply selects best performing method. Default FALSE, finds optimal combination predictions libraries using SL.method. argument SL.library must supplied. see list available entries, use SuperLearner::listWrappers(). arguments SuperLearner::SuperLearner() can passed weightit() weightitMSM(), following exceptions: obsWeights ignored sampling weights passed using s.weights. method SuperLearner() replaced argument SL.method weightit(). continuous treatments , following arguments may supplied: density function corresponding conditional density treatment. standardized residuals treatment model fed function produce numerator denominator generalized propensity score weights. blank, dnorm() used recommended Robins et al. (2000). can also supplied string containing name function called. string contains underscores, call split underscores latter splits supplied arguments second argument beyond. example, density = \"dt_2\" specified, density used t-distribution 2 degrees freedom. Using t-distribution can useful extreme outcome values observed (Naimi et al., 2014). Ignored use.kernel = TRUE (described ). use.kernel TRUE, uses kernel density estimation density() function estimate numerator denominator densities weights. FALSE, argument density parameter used instead. bw, adjust, kernel, n use.kernel = TRUE, arguments density() function. defaults density except n 10 times number units sample. plot use.kernel = TRUE, whether plot estimated density.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_super.html","id":"balance-superlearner","dir":"Reference","previous_headings":"","what":"Balance SuperLearner","title":"Propensity Score Weighting Using SuperLearner — method_super","text":"addition methods allowed SuperLearner(), one can specify SL.method = \"method.balance\" use \"Balance SuperLearner\" described Pirracchio Carone (2018), wherein covariate balance used choose optimal combination predictions methods specified SL.library. Coefficients chosen (one prediction method) weights generated weighted combination predictions optimize balance criterion, must set criterion argument, described . criterion string describing balance criterion used select best weights. See cobalt::bal.compute() allowable options treatment type. binary multinomial treatments, default \"smd.mean\", minimizes average absolute standard mean difference among covariates treatment groups. continuous treatments, default \"p.mean\", minimizes average absolute Pearson correlation treatment covariates. Note implementation differs Pirracchio Carone (2018) , balance measured terms included model formula (.e., interactions unless specifically included), balance results sample weighted using estimated predicted values propensity scores, sample matched using propensity score matching predicted values. Binary continuous treatments supported, currently multinomial treatments .","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_super.html","id":"additional-outputs","dir":"Reference","previous_headings":"","what":"Additional Outputs","title":"Propensity Score Weighting Using SuperLearner — method_super","text":"info binary continuous treatments, list two entries, coef cvRisk. multinomial treatments, list lists two entries, one treatment level. coef coefficients linear combination predictions method SL.library. Higher values indicate corresponding method plays larger role determining resulting predicted value, values close zero indicate method plays little role determining predicted value. discrete = TRUE, correspond coefficients estimated discrete FALSE. cvRisk cross-validation risk method SL.library. Higher values indicate method worse cross-validation accuracy. SL.method = \"method.balance\", sample weighted balance statistic requested criterion. Higher values indicate worse balance. obj include.obj = TRUE, SuperLearner fit(s) used generate predicted values. binary continuous treatments, output call SuperLearner::SuperLearner(). multinomial treatments, list outputs calls SuperLearner::SuperLearner().","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_super.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Propensity Score Weighting Using SuperLearner — method_super","text":"SuperLearner works fitting several machine learning models treatment covariates taking weighted combination generated predicted values use propensity scores, used construct weights. machine learning models used supplied using SL.library argument; models supplied, higher chance correctly modeling propensity score. predicted values combined using method supplied SL.method argument (nonnegative least squares default). benefit SuperLearner , asymptotically, guaranteed perform well better best-performing method included library. Using Balance SuperLearner setting SL.method = \"method.balance\" works selecting combination predicted values minimizes imbalance measure.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_super.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Propensity Score Weighting Using SuperLearner — method_super","text":"Binary treatments Pirracchio, R., Petersen, M. L., & van der Laan, M. (2015). Improving Propensity Score Estimators’ Robustness Model Misspecification Using Super Learner. American Journal Epidemiology, 181(2), 108–119. doi:10.1093/aje/kwu253 Continuous treatments Kreif, N., Grieve, R., Díaz, ., & Harrison, D. (2015). Evaluation Effect Continuous Treatment: Machine Learning Approach Application Treatment Traumatic Brain Injury. Health Economics, 24(9), 1213–1228. doi:10.1002/hec.3189 - Balance SuperLearner (SL.method = \"method.balance\") Pirracchio, R., & Carone, M. (2018). Balance Super Learner: robust adaptation Super Learner improve estimation average treatment effect treated based propensity score matching. Statistical Methods Medical Research, 27(8), 2504–2518. doi:10.1177/0962280216682055 See method_glm additional references.","code":""},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/reference/method_super.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Propensity Score Weighting Using SuperLearner — method_super","text":"methods formerly available SuperLearner now SuperLearnerExtra, can found GitHub https://github.com/ecpolley/SuperLearnerExtra. criterion argument used called stop.method, name twang. stop.method still works backward compatibility. Additionally, criteria formerly named es.mean, es.max, es.rms renamed smd.mean, smd.max, smd.rms. former used twang still work weightit() backward compatibility.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_super.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Propensity Score Weighting Using SuperLearner — method_super","text":"","code":"library(\"cobalt\") data(\"lalonde\", package = \"cobalt\")  #Balancing covariates between treatment groups (binary) (W1 <- weightit(treat ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"super\", estimand = \"ATT\",                 SL.library = c(\"SL.glm\", \"SL.stepAIC\",                                \"SL.glm.interaction\"))) #> Loading required package: nnls #> A weightit object #>  - method: \"super\" (propensity score weighting with SuperLearner) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 2-category #>  - estimand: ATT (focal: 1) #>  - covariates: age, educ, married, nodegree, re74 summary(W1) #>                  Summary of weights #>  #> - Weight ranges: #>  #>            Min                                  Max #> treated 1.0000      ||                       1.0000 #> control 0.0062 |---------------------------| 5.6573 #>  #> - Units with the 5 most extreme weights by group: #>                                             #>               7      5      4      3      1 #>  treated      1      1      1      1      1 #>             411    589    269    409    296 #>  control 2.4116 2.5004 2.7322 3.3402 5.6573 #>  #> - Weight statistics: #>  #>         Coef of Var   MAD Entropy # Zeros #> treated       0.000 0.000  -0.000       0 #> control       1.116 0.738   0.442       0 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted  429.       185 #> Weighted    191.27     185 bal.tab(W1) #> Balance Measures #>                Type Diff.Adj #> prop.score Distance   0.0599 #> age         Contin.  -0.1408 #> educ        Contin.   0.0167 #> married      Binary  -0.0049 #> nodegree     Binary   0.0123 #> re74        Contin.  -0.0251 #>  #> Effective sample sizes #>            Control Treated #> Unadjusted  429.       185 #> Adjusted    191.27     185 # \\donttest{ #Balancing covariates with respect to race (multinomial) (W2 <- weightit(race ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"super\", estimand = \"ATE\",                 SL.library = c(\"SL.glm\", \"SL.stepAIC\",                                \"SL.glm.interaction\"))) #> A weightit object #>  - method: \"super\" (propensity score weighting with SuperLearner) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 3-category (black, hispan, white) #>  - estimand: ATE #>  - covariates: age, educ, married, nodegree, re74 summary(W2) #>                  Summary of weights #>  #> - Weight ranges: #>  #>           Min                                   Max #> black  1.3125 |-------------------|         14.0815 #> hispan 1.7509  |--------------------------| 19.0788 #> white  1.0815 |----|                         4.9344 #>  #> - Units with the 5 most extreme weights by group: #>                                                 #>             184     244     485     182     181 #>   black  7.5917  7.7461 10.1133 11.6337 14.0815 #>             346     392     371     269     345 #>  hispan 16.6957 17.1862 17.2636 17.8517 19.0788 #>             457      23     409     589     296 #>   white  4.0945  4.1846  4.3505  4.6072  4.9344 #>  #> - Weight statistics: #>  #>        Coef of Var   MAD Entropy # Zeros #> black        0.640 0.391   0.138       0 #> hispan       0.476 0.372   0.110       0 #> white        0.388 0.317   0.069       0 #>  #> - Effective Sample Sizes: #>  #>             black hispan  white #> Unweighted 243.    72.   299.   #> Weighted   172.57  58.85 260.06 bal.tab(W2) #> Balance summary across all treatment pairs #>             Type Max.Diff.Adj #> age      Contin.       0.0968 #> educ     Contin.       0.0794 #> married   Binary       0.0450 #> nodegree  Binary       0.0246 #> re74     Contin.       0.0328 #>  #> Effective sample sizes #>             black hispan  white #> Unadjusted 243.    72.   299.   #> Adjusted   172.57  58.85 260.06  #Balancing covariates with respect to re75 (continuous) #assuming t(8) conditional density for treatment (W3 <- weightit(re75 ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"super\", density = \"dt_8\",                 SL.library = c(\"SL.glm\", \"SL.ridge\",                                \"SL.glm.interaction\"))) #> A weightit object #>  - method: \"super\" (propensity score weighting with SuperLearner) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: continuous #>  - covariates: age, educ, married, nodegree, re74 summary(W3) #>                  Summary of weights #>  #> - Weight ranges: #>  #>        Min                                   Max #> all 0.0439 |---------------------------| 21.0528 #>  #> - Units with the 5 most extreme weights: #>                                            #>         431    483     484     485     354 #>  all 9.4162 16.527 18.1028 19.0692 21.0528 #>  #> - Weight statistics: #>  #>     Coef of Var   MAD Entropy # Zeros #> all       1.334 0.504   0.341       0 #>  #> - Effective Sample Sizes: #>  #>             Total #> Unweighted 614.   #> Weighted   221.04 bal.tab(W3) #> Balance Measures #>             Type Corr.Adj #> age      Contin.   0.0311 #> educ     Contin.   0.0348 #> married   Binary   0.0613 #> nodegree  Binary  -0.0599 #> re74     Contin.   0.0405 #>  #> Effective sample sizes #>             Total #> Unadjusted 614.   #> Adjusted   221.04 # } #Balancing covariates between treatment groups (binary) # using balance SuperLearner to minimize the maximum # KS statistic (W4 <- weightit(treat ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"super\", estimand = \"ATT\",                 SL.library = c(\"SL.glm\", \"SL.stepAIC\",                                \"SL.lda\"),                 SL.method = \"method.balance\",                 criterion = \"ks.max\")) #> A weightit object #>  - method: \"super\" (propensity score weighting with SuperLearner) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 2-category #>  - estimand: ATT (focal: 1) #>  - covariates: age, educ, married, nodegree, re74 summary(W4) #>                  Summary of weights #>  #> - Weight ranges: #>  #>            Min                                  Max #> treated 1.0000                 ||            1.0000 #> control 0.0339 |---------------------------| 1.7088 #>  #> - Units with the 5 most extreme weights by group: #>                                             #>               6      5      4      2      1 #>  treated      1      1      1      1      1 #>             411    595    269    409    296 #>  control 1.2058 1.2729 1.3372 1.4342 1.7088 #>  #> - Weight statistics: #>  #>         Coef of Var   MAD Entropy # Zeros #> treated       0.000 0.000   -0.00       0 #> control       0.763 0.666    0.29       0 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted  429.       185 #> Weighted    271.31     185 bal.tab(W4, stats = c(\"m\", \"ks\")) #> Balance Measures #>                Type Diff.Adj KS.Adj #> prop.score Distance   0.0694 0.1030 #> age         Contin.   0.0567 0.2727 #> educ        Contin.  -0.0305 0.0587 #> married      Binary  -0.0042 0.0042 #> nodegree     Binary   0.0193 0.0193 #> re74        Contin.  -0.0789 0.2993 #>  #> Effective sample sizes #>            Control Treated #> Unadjusted  429.       185 #> Adjusted    271.31     185"},{"path":"https://ngreifer.github.io/WeightIt/reference/method_user.html","id":null,"dir":"Reference","previous_headings":"","what":"User-Defined Functions for Estimating Weights — method_user","title":"User-Defined Functions for Estimating Weights — method_user","text":"page explains details estimating weights using user-defined function. function must take arguments passed weightit() weightitMSM() return vector weights list containing weights. supply  user-defined function, function object entered directly method; example, function fun, method = fun.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_user.html","id":"point-treatments","dir":"Reference","previous_headings":"","what":"Point Treatments","title":"User-Defined Functions for Estimating Weights — method_user","text":"following arguments automatically passed user-defined function, named parameters corresponding : treat: vector treatment status unit. comes directly left hand side formula passed weightit type (e.g., numeric, factor, etc.), may need converted. covs: data frame covariate values unit. comes directly right hand side formula passed weightit. covariates processed columns numeric; factor variables split dummies interactions evaluated. levels factor variables given dummies, matrix covariates full rank. Users can use make_full_rank, accepts numeric matrix data frame removes columns make full rank, full rank covariate matrix desired. s.weights: numeric vector sampling weights, one unit. ps: numeric vector propensity scores. subset: logical vector length treat TRUE units included estimation FALSE otherwise. used subset input objects exact used. treat, covs, s.weights, ps, supplied, already subsetted subset. estimand: character vector length 1 containing desired estimand. characters converted uppercase. \"ATC\" supplied estimand, weightit sets focal control level (usually 0 lowest level treat) sets estimand \"ATT\". focal: character vector length 1 containing focal level treatment estimand ATT (ATC detailed ). weightit() ensures value focal level treat. stabilize: logical vector length 1. processed weightit() reaches fitting function. moments: numeric vector length 1. processed weightit() reaches fitting function except .integer applied . used methods determine whether polynomials entered covariates used weight estimation. int: logical vector length 1. processed weightit() reaches fitting function. used methods determine whether interactions entered covariates used weight estimation. None parameters required fitting function. simply automatically available. addition, additional arguments supplied weightit() passed fitting function. weightit() ensures arguments correspond parameters fitting function throws error incorrectly named argument supplied fitting function include ... parameter. fitting function must output either numeric vector weights list (list-like object) entry named wither \"w\" \"weights\". list, list can contain named entries, entries named \"w\", \"weights\", \"ps\", \"fit.obj\" processed. \"ps\" vector propensity scores \"fit.obj\" object used fitting process user may want examine included weightit output object \"obj\" include.obj = TRUE. \"ps\" \"fit.obj\" components optional, \"weights\" \"w\" required.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_user.html","id":"longitudinal-treatments","dir":"Reference","previous_headings":"","what":"Longitudinal Treatments","title":"User-Defined Functions for Estimating Weights — method_user","text":"Longitudinal treatments can handled either running fitting function point treatments time point multiplying resulting weights together running method accommodates multiple time points outputs single set weights. former, weightitMSM() can used user-defined function just weightit(). latter method yet accommodated weightitMSM(), someday, maybe.","code":""},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/reference/method_user.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"User-Defined Functions for Estimating Weights — method_user","text":"","code":"library(\"cobalt\") data(\"lalonde\", package = \"cobalt\")  #A user-defined version of method = \"ps\" my.ps <- function(treat, covs, estimand, focal = NULL) {   covs <- make_full_rank(covs)   d <- data.frame(treat, covs)   f <- formula(d)   ps <- glm(f, data = d, family = \"binomial\")$fitted   w <- get_w_from_ps(ps, treat = treat, estimand = estimand,                      focal = focal)    return(list(w = w, ps = ps)) }  #Balancing covariates between treatment groups (binary) (W1 <- weightit(treat ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = my.ps, estimand = \"ATT\")) #> A weightit object #>  - method: \"my.ps\" (a user-defined method) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 2-category #>  - estimand: ATT (focal: 1) #>  - covariates: age, educ, married, nodegree, re74 summary(W1) #>                  Summary of weights #>  #> - Weight ranges: #>  #>            Min                                  Max #> treated 1.0000               ||              1.0000 #> control 0.0222 |---------------------------| 2.0438 #>  #> - Units with the 5 most extreme weights by group: #>                                             #>               7      5      4      3      1 #>  treated      1      1      1      1      1 #>             411    595    269    409    296 #>  control 1.3303 1.4365 1.5005 1.6369 2.0438 #>  #> - Weight statistics: #>  #>         Coef of Var   MAD Entropy # Zeros #> treated       0.000 0.000   -0.00       0 #> control       0.823 0.701    0.33       0 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted  429.       185 #> Weighted    255.99     185 bal.tab(W1) #> Balance Measures #>                Type Diff.Adj #> prop.score Distance   0.0199 #> age         Contin.   0.0459 #> educ        Contin.  -0.0360 #> married      Binary   0.0044 #> nodegree     Binary   0.0080 #> re74        Contin.  -0.0275 #>  #> Effective sample sizes #>            Control Treated #> Unadjusted  429.       185 #> Adjusted    255.99     185  data(\"msmdata\") (W2 <- weightitMSM(list(A_1 ~ X1_0 + X2_0,                         A_2 ~ X1_1 + X2_1 +                           A_1 + X1_0 + X2_0,                         A_3 ~ X1_2 + X2_2 +                           A_2 + X1_1 + X2_1 +                           A_1 + X1_0 + X2_0),                    data = msmdata,                    method = my.ps)) #> A weightitMSM object #>  - method: \"my.ps\" (a user-defined method) #>  - number of obs.: 7500 #>  - sampling weights: none #>  - number of time points: 3 (A_1, A_2, A_3) #>  - treatment:  #>     + time 1: 2-category #>     + time 2: 2-category #>     + time 3: 2-category #>  - covariates:  #>     + baseline: X1_0, X2_0 #>     + after time 1: X1_1, X2_1, A_1, X1_0, X2_0 #>     + after time 2: X1_2, X2_2, A_2, X1_1, X2_1, A_1, X1_0, X2_0  summary(W2) #>                  Summary of weights #>  #>                        Time 1                        #>                  Summary of weights #>  #> - Weight ranges: #>  #>            Min                                    Max #> treated 1.0791 |---------------------------| 403.4833 #> control 1.2761 |-------------------|         284.7636 #>  #> - Units with the 5 most extreme weights by group: #>                                                       #>              5488     3440     3593     1286     5685 #>  treated  166.992 170.5549 196.4136 213.1934 403.4833 #>              2594     2932     5226     1875     2533 #>  control 155.6248  168.964 172.4195 245.8822 284.7636 #>  #> - Weight statistics: #>  #>         Coef of Var   MAD Entropy # Zeros #> treated       1.914 0.816   0.649       0 #> control       1.706 0.862   0.670       0 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted 3306.    4194.  #> Weighted    845.79   899.4 #>  #>                        Time 2                        #>                  Summary of weights #>  #> - Weight ranges: #>  #>            Min                                    Max #> treated 1.0791 |---------------------------| 403.4833 #> control 1.2761 |----------------|            245.8822 #>  #> - Units with the 5 most extreme weights by group: #>                                                       #>              2932     3440     3593     2533     5685 #>  treated  168.964 170.5549 196.4136 284.7636 403.4833 #>              2594     5488     5226     1286     1875 #>  control 155.6248  166.992 172.4195 213.1934 245.8822 #>  #> - Weight statistics: #>  #>         Coef of Var   MAD Entropy # Zeros #> treated       1.892 0.819   0.652       0 #> control       1.748 0.869   0.686       0 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted 3701.   3799.   #> Weighted    912.87  829.87 #>  #>                        Time 3                        #>                  Summary of weights #>  #> - Weight ranges: #>  #>            Min                                    Max #> treated 1.0791 |---------------------------| 403.4833 #> control 1.2761 |---------|                   148.1547 #>  #> - Units with the 5 most extreme weights by group: #>                                                       #>              3593     1286     1875     2533     5685 #>  treated 196.4136 213.1934 245.8822 284.7636 403.4833 #>              6862      168     3729     6158     3774 #>  control  88.0721  97.8273  104.623 121.8451 148.1547 #>  #> - Weight statistics: #>  #>         Coef of Var   MAD Entropy # Zeros #> treated       1.832 0.975   0.785       0 #> control       1.254 0.683   0.412       0 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted 4886.   2614.   #> Weighted   1900.26  600.12 #>  bal.tab(W2) #> Balance summary across all time points #>              Times     Type Max.Diff.Adj #> prop.score 1, 2, 3 Distance       0.0773 #> X1_0       1, 2, 3  Contin.       0.0342 #> X2_0       1, 2, 3   Binary       0.0299 #> X1_1          2, 3  Contin.       0.0657 #> X2_1          2, 3   Binary       0.0299 #> A_1           2, 3   Binary       0.0262 #> X1_2             3  Contin.       0.0643 #> X2_2             3   Binary       0.0096 #> A_2              3   Binary       0.0054 #>  #> Effective sample sizes #>  - Time 1 #>            Control Treated #> Unadjusted 3306.    4194.  #> Adjusted    845.79   899.4 #>  - Time 2 #>            Control Treated #> Unadjusted 3701.   3799.   #> Adjusted    912.87  829.87 #>  - Time 3 #>            Control Treated #> Unadjusted 4886.   2614.   #> Adjusted   1900.26  600.12  # Kernel balancing using the KBAL package, available # using devtools::install_github(\"chadhazlett/KBAL\"). # Only the ATT and ATC are available. Use 'kbal.method' # instead of 'method' in weightit() to choose between # \"ebal\" and \"el\".  if (FALSE) { kbal.fun <- function(treat, covs, estimand, focal, ...) {     args <- list(...)     if (is_not_null(focal))         treat <- as.numeric(treat == focal)     else if (estimand != \"ATT\")         stop(\"estimand must be 'ATT' or 'ATC'.\", call. = FALSE)     if (\"kbal.method\" %in% names(args)) {         names(args)[names(args) == \"kbal.method\"] <- \"method\"     }     args[!names(args) %in% setdiff(names(formals(KBAL::kbal)),         c(\"X\", \"D\"))] <- NULL     k.out <- do.call(KBAL::kbal, c(list(X = covs, D = treat),         args))     w <- k.out$w     return(list(w = w)) }  (Wk <- weightit(treat ~ age + educ + married +                 nodegree + re74, data = lalonde,                 method = kbal.fun, estimand = \"ATT\",                 kbal.method = \"ebal\")) summary(Wk) bal.tab(Wk, disp.ks = TRUE) }"},{"path":"https://ngreifer.github.io/WeightIt/reference/msmdata.html","id":null,"dir":"Reference","previous_headings":"","what":"Simulated data for a 3 time point sequential study — msmdata","title":"Simulated data for a 3 time point sequential study — msmdata","text":"simulated dataset 7500 units covariates treatment measured three times outcome measured end hypothetical observational study examining effect treatment delivered time point adverse event.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/msmdata.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Simulated data for a 3 time point sequential study — msmdata","text":"","code":"data(\"msmdata\")"},{"path":"https://ngreifer.github.io/WeightIt/reference/msmdata.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Simulated data for a 3 time point sequential study — msmdata","text":"data frame 7500 observations following 10 variables. X1_0 count covariate measured baseline X2_0 binary covariate measured baseline A_1 binary indicator treatment status first time point X1_1 count covariate measured first time point (first treatment) X2_1 binary covariate measured first time point (first treatment) A_2 binary indicator treatment status second time point X1_2 count covariate measured second time point (second treatment) X2_2 binary covariate measured first time point (first treatment) A_3 binary indicator treatment status third time point Y_B binary indicator outcome event (e.g., death)","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/msmdata.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Simulated data for a 3 time point sequential study — msmdata","text":"","code":"data(msmdata)"},{"path":"https://ngreifer.github.io/WeightIt/reference/sbps.html","id":null,"dir":"Reference","previous_headings":"","what":"Subgroup Balancing Propensity Score — sbps","title":"Subgroup Balancing Propensity Score — sbps","text":"Implements subgroup balancing propensity score (SBPS), algorithm attempts achieve balance subgroups sharing information overall sample subgroups (Dong, Zhang, Zeng, & Li, 2020; DZZL). subgroup can use either weights estimated using whole sample, weights estimated using just subgroup, combination two. optimal combination chosen minimizes imbalance criterion includes subgroup well overall balance.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/sbps.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Subgroup Balancing Propensity Score — sbps","text":"","code":"sbps(obj, obj2 = NULL,      moderator = NULL,      formula = NULL,      data = NULL,      smooth = FALSE,      full.search)  # S3 method for weightit.sbps print(x, ...)  # S3 method for weightit.sbps summary(object, top = 5,         ignore.s.weights = FALSE, ...)  # S3 method for summary.weightit.sbps print(x, ...)"},{"path":"https://ngreifer.github.io/WeightIt/reference/sbps.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Subgroup Balancing Propensity Score — sbps","text":"obj weightit object containing weights estimated overall sample. obj2 weightit object containing weights estimated subgroups. Typically estimated including call weightit(). Either obj2 moderator must specified. moderator optional; string containing name variable data weighting done within subgroups one-sided formula subgrouping variable right-hand side. argument analogous argument weightit(), fact passed . Either obj2 moderator must specified. formula optional formula covariates balance optimized. specified, formula obj$call used. data optional data set form data frame contains variables formula moderator. smooth logical; whether smooth version SBPS used. compatible weightit methods return propensity score. full.search logical; smooth = FALSE, whether every combination subgroup overall weights evaluated. FALSE, stochastic search described DZZL used instead. TRUE, \\(2^R\\) combinations checked, \\(R\\) number subgroups, can take long time many subgroups. unspecified, default TRUE \\(R <= 8\\) FALSE otherwise. x weightit.sbps summary.weightit.sbps object; output call sbps() summary.weightit.sbps(). object weightit.sbps object; output call sbps(). top many largest smallest weights display. Default 5. ignore.s.weights whether ignore sampling weights computing weight summary. FALSE, default, estimated weights multiplied sampling weights () values computed. ... print, arguments passed print(). Ignored otherwise.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/sbps.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Subgroup Balancing Propensity Score — sbps","text":"SBPS relies two sets weights: one estimated overall sample one estimated within subgroup. algorithm decides whether subgroup use weights estimated overall sample estimated subgroup. 2^R permutations overall subgroup weights, R number subgroups. optimal permutation chosen minimizes balance criterion described DZZL. balance criterion used , binary multinomial treatments, sum squared standardized mean differences within subgroups overall, computed using cobalt::col_w_smd() cobalt, continuous treatments, sum squared correlations covariate treatment within subgroups overall, computed using cobalt::col_w_corr() cobalt. smooth version estimates weights determine relative contribution overall subgroup propensity scores weighted average propensity score subgroup. P_O propensity scores estimated overall sample P_S propensity scores estimated subgroup, smooth SBPS finds R coefficients C subgroup, ultimate propensity score \\(C*P_S + (1-C)*P_O\\), weights computed propensity score. coefficients estimated using optim() method = \"L-BFGS-B\". C estimated 1 0 subgroup, smooth SBPS coincides standard SBPS. obj2 specified moderator , sbps() attempt refit model specified obj moderator argument. relies environment obj created intact can take time obj hard fit. safer estimate obj obj2 (latter simply including moderator argument) supply sbps().","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/sbps.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Subgroup Balancing Propensity Score — sbps","text":"weightit.sbps object, inherits weightit. contains information obj weights, propensity scores, call, possibly covariates updated sbps(). addition, prop.subgroup component contains values coefficients C subgroups (either 0 1 standard SBPS), moderator component contains data.frame moderator. object summary methods compatible cobalt functions. cluster argument used cobalt functions accurately reflect performance weights balancing subgroups.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/sbps.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Subgroup Balancing Propensity Score — sbps","text":"Dong, J., Zhang, J. L., Zeng, S., & Li, F. (2020). Subgroup balancing propensity score. Statistical Methods Medical Research, 29(3), 659–676. doi:10.1177/0962280219870836","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/sbps.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Subgroup Balancing Propensity Score — sbps","text":"Noah Greifer","code":""},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/reference/sbps.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Subgroup Balancing Propensity Score — sbps","text":"","code":"library(\"cobalt\") data(\"lalonde\", package = \"cobalt\")  #Balancing covariates between treatment groups within races (W1 <- weightit(treat ~ age + educ + married +                 nodegree + race + re74, data = lalonde,                 method = \"glm\", estimand = \"ATT\")) #> A weightit object #>  - method: \"glm\" (propensity score weighting with GLM) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 2-category #>  - estimand: ATT (focal: 1) #>  - covariates: age, educ, married, nodegree, race, re74  (W2 <- weightit(treat ~ age + educ + married +                 nodegree + race + re74, data = lalonde,                 method = \"glm\", estimand = \"ATT\",                 by = \"race\")) #> A weightit object #>  - method: \"glm\" (propensity score weighting with GLM) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 2-category #>  - estimand: ATT (focal: 1) #>  - covariates: age, educ, married, nodegree, race, re74 #>  - by: race S <- sbps(W1, W2) print(S) #> A weightit.sbps object #>  - method: \"glm\" (propensity score weighting with GLM) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 2-category #>  - estimand: ATT (focal: 1) #>  - covariates: age, educ, married, nodegree, race, re74 #>  - moderator: race (3 subgroups) summary(S) #> Summary of weights: #>  #>  - Overall vs. subgroup proportion contribution: #>          race = black race = hispan race = white #> Overall             0             0            0 #> Subgroup            1             1            1 #>  #>  - - - - - - - Subgroup race = black - - - - - - - #> - Weight ranges: #>           Min                                  Max #> treated 1.000      ||                       1.0000 #> control 0.466 |---------------------------| 3.5903 #>  #> - Units with 5 greatest weights by group: #>                                            #>               1      2     4      5      6 #>  treated      1      1     1      1      1 #>             221    228   188    185    174 #>  control 2.9494 2.9494 3.006 3.0637 3.5903 #>  #>          Ratio Coef of Var #> treated 1.0000      0.0000 #> control 7.7042      0.4250 #> overall 7.7042      0.4616 #>  #> - Effective Sample Sizes: #>            Control Treated #> Unweighted  87.000     156 #> Weighted    73.818     156 #>  #>  - - - - - - - Subgroup race = hispan - - - - - - - #> - Weight ranges: #>            Min                                    Max #> treated 1.0000                              || 1.0000 #> control 0.0209   |------------|                0.5046 #>  #> - Units with 5 greatest weights by group: #>                                             #>               1      2      3      4      6 #>  treated      1      1      1      1      1 #>              56     54     49     48     47 #>  control 0.4117 0.4767 0.4835 0.4968 0.5046 #>  #>           Ratio Coef of Var #> treated  1.0000      0.0000 #> control 24.1741      0.7143 #> overall 47.9120      1.0352 #>  #> - Effective Sample Sizes: #>            Control Treated #> Unweighted  61.000      11 #> Weighted    40.616      11 #>  #>  - - - - - - - Subgroup race = white - - - - - - - #> - Weight ranges: #>            Min                                   Max #> treated 1.0000                              || 1.000 #> control 0.0002   |---------|                   0.385 #>  #> - Units with 5 greatest weights by group: #>                                            #>               1      2      4      5     6 #>  treated      1      1      1      1     1 #>             289    287    285    280   267 #>  control 0.2393 0.2699 0.2937 0.2956 0.385 #>  #>            Ratio Coef of Var #> treated    1.000      0.0000 #> control 1825.568      1.1538 #> overall 4742.156      1.9499 #>  #> - Effective Sample Sizes: #>            Control Treated #> Unweighted 281.000      18 #> Weighted   120.777      18 bal.tab(S, cluster = \"race\") #> Balance by cluster #>  #>  - - - Cluster: black - - -  #> Balance Measures #>                 Type Diff.Adj #> prop.score  Distance   0.0016 #> age          Contin.   0.0126 #> educ         Contin.  -0.0332 #> married       Binary   0.0030 #> nodegree      Binary   0.0062 #> race_black    Binary   0.0000 #> race_hispan   Binary   0.0000 #> race_white    Binary   0.0000 #> re74         Contin.  -0.0826 #>  #> Effective sample sizes #>                0   1 #> Unadjusted 87.   156 #> Adjusted   73.82 156 #>  #>  - - - Cluster: hispan - - -  #> Balance Measures #>                 Type Diff.Adj #> prop.score  Distance  -0.2678 #> age          Contin.   0.1196 #> educ         Contin.  -0.0756 #> married       Binary   0.0217 #> nodegree      Binary   0.0018 #> race_black    Binary   0.0000 #> race_hispan   Binary   0.0000 #> race_white    Binary   0.0000 #> re74         Contin.   0.0114 #>  #> Effective sample sizes #>                0  1 #> Unadjusted 61.   11 #> Adjusted   40.62 11 #>  #>  - - - Cluster: white - - -  #> Balance Measures #>                 Type Diff.Adj #> prop.score  Distance   0.0652 #> age          Contin.   0.0191 #> educ         Contin.  -0.0185 #> married       Binary  -0.0015 #> nodegree      Binary   0.0039 #> race_black    Binary   0.0000 #> race_hispan   Binary   0.0000 #> race_white    Binary   0.0000 #> re74         Contin.  -0.0117 #>  #> Effective sample sizes #>                 0  1 #> Unadjusted 281.   18 #> Adjusted   120.78 18 #>  - - - - - - - - - - - - - -  #>   #Could also have run #  sbps(W1, moderator = \"race\")  S_ <- sbps(W1, W2, smooth = TRUE) print(S_) #> A weightit.sbps object #>  - method: \"glm\" (propensity score weighting with GLM) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 2-category #>  - estimand: ATT (focal: 1) #>  - covariates: age, educ, married, nodegree, race, re74 #>  - moderator: race (3 subgroups) summary(S_) #> Summary of weights: #>  #>  - Overall vs. subgroup proportion contribution: #>          race = black race = hispan race = white #> Overall          0.17          0.25            0 #> Subgroup         0.83          0.75            1 #>  #>  - - - - - - - Subgroup race = black - - - - - - - #> - Weight ranges: #>            Min                                  Max #> treated 1.0000      ||                       1.0000 #> control 0.4654 |---------------------------| 3.5703 #>  #> - Units with 5 greatest weights by group: #>                                             #>               1      2      3      4      7 #>  treated      1      1      1      1      1 #>             221    228    188    185    174 #>  control 2.9787 2.9787 3.0338 3.0899 3.5703 #>  #>          Ratio Coef of Var #> treated 1.0000      0.0000 #> control 7.6708      0.4264 #> overall 7.6708      0.4625 #>  #> - Effective Sample Sizes: #>            Control Treated #> Unweighted  87.000     156 #> Weighted    73.744     156 #>  #>  - - - - - - - Subgroup race = hispan - - - - - - - #> - Weight ranges: #>            Min                                    Max #> treated 1.0000                              || 1.0000 #> control 0.0254   |-----------|                 0.4743 #>  #> - Units with 5 greatest weights by group: #>                                             #>               1      2      3      4      6 #>  treated      1      1      1      1      1 #>              56     54     48     47     28 #>  control 0.3908 0.4496 0.4557 0.4704 0.4743 #>  #>           Ratio Coef of Var #> treated  1.0000      0.0000 #> control 18.6516      0.6795 #> overall 39.3245      1.0314 #>  #> - Effective Sample Sizes: #>            Control Treated #> Unweighted   61.00      11 #> Weighted     41.95      11 #>  #>  - - - - - - - Subgroup race = white - - - - - - - #> - Weight ranges: #>            Min                                   Max #> treated 1.0000                              || 1.000 #> control 0.0002   |---------|                   0.385 #>  #> - Units with 5 greatest weights by group: #>                                            #>               1      2      4      5     6 #>  treated      1      1      1      1     1 #>             289    287    285    280   267 #>  control 0.2393 0.2699 0.2937 0.2956 0.385 #>  #>            Ratio Coef of Var #> treated    1.000      0.0000 #> control 1825.568      1.1538 #> overall 4742.156      1.9499 #>  #> - Effective Sample Sizes: #>            Control Treated #> Unweighted 281.000      18 #> Weighted   120.777      18 bal.tab(S_, cluster = \"race\") #> Balance by cluster #>  #>  - - - Cluster: black - - -  #> Balance Measures #>                 Type Diff.Adj #> prop.score  Distance   0.0019 #> age          Contin.   0.0388 #> educ         Contin.  -0.0305 #> married       Binary   0.0096 #> nodegree      Binary   0.0086 #> race_black    Binary   0.0000 #> race_hispan   Binary   0.0000 #> race_white    Binary   0.0000 #> re74         Contin.  -0.0561 #>  #> Effective sample sizes #>                0   1 #> Unadjusted 87.   156 #> Adjusted   73.74 156 #>  #>  - - - Cluster: hispan - - -  #> Balance Measures #>                 Type Diff.Adj #> prop.score  Distance  -0.1909 #> age          Contin.   0.0167 #> educ         Contin.  -0.0654 #> married       Binary   0.0314 #> nodegree      Binary   0.0084 #> race_black    Binary   0.0000 #> race_hispan   Binary   0.0000 #> race_white    Binary   0.0000 #> re74         Contin.  -0.0175 #>  #> Effective sample sizes #>                0  1 #> Unadjusted 61.   11 #> Adjusted   41.95 11 #>  #>  - - - Cluster: white - - -  #> Balance Measures #>                 Type Diff.Adj #> prop.score  Distance   0.0652 #> age          Contin.   0.0191 #> educ         Contin.  -0.0185 #> married       Binary  -0.0015 #> nodegree      Binary   0.0039 #> race_black    Binary   0.0000 #> race_hispan   Binary   0.0000 #> race_white    Binary   0.0000 #> re74         Contin.  -0.0117 #>  #> Effective sample sizes #>                 0  1 #> Unadjusted 281.   18 #> Adjusted   120.78 18 #>  - - - - - - - - - - - - - -  #>"},{"path":"https://ngreifer.github.io/WeightIt/reference/summary.weightit.html","id":null,"dir":"Reference","previous_headings":"","what":"Print and Summarize Output — summary.weightit","title":"Print and Summarize Output — summary.weightit","text":"summary() generates summary weightit weightitMSM object evaluate properties estimated weights. plot() plots distribution weights.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/summary.weightit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print and Summarize Output — summary.weightit","text":"","code":"# S3 method for weightit summary(object, top = 5,         ignore.s.weights = FALSE, ...)  # S3 method for summary.weightit print(x, ...)  # S3 method for summary.weightit plot(x, binwidth = NULL, bins = NULL, ...)  # S3 method for weightitMSM summary(object, top = 5,         ignore.s.weights = FALSE, ...)  # S3 method for summary.weightitMSM print(x, ...)"},{"path":"https://ngreifer.github.io/WeightIt/reference/summary.weightit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print and Summarize Output — summary.weightit","text":"object weightit weightitMSM object; output call weightit() weightitMSM(). top many largest smallest weights display. Default 5. ignore.s.weights whether ignore sampling weights computing weight summary. FALSE, default, estimated weights multiplied sampling weights () values computed. binwidth, bins arguments passed ggplot2::geom_histogram() control size /number bins. x summary.weightit summary.weightitMSM object; output call summary.weightit() summary.weightitMSM(). ... print(), arguments passed print(). plot(), additional arguments passed graphics::hist() determine number bins, though ggplot2::geom_histogram() ggplot2 actually used create plot.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/summary.weightit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Print and Summarize Output — summary.weightit","text":"point treatments (.e., weightit objects), summary.weightit object following elements: weight.range range (minimum maximum) weight treatment group. weight.top units greatest weights treatment group; many included determined top. coef..var (Coef Var) coefficient variation (standard deviation divided mean) weights treatment group overall. scaled.mad (MAD) mean absolute deviation weights treatment group overall divided mean weights corresponding group. negative entropy (Entropy) negative entropy (\\(\\sum w log(w)\\)) weights treatment group overall divided mean weights corresponding group. num.zeros number weights equal zero. effective.sample.size effective sample size treatment group weighting. See ESS(). longitudinal treatments (.e., weightitMSM objects), list elements treatment period. plot() returns ggplot object histogram displaying distribution estimated weights. estimand ATT ATC, weights non-focal group(s) displayed (since weights focal group 1). dotted line displayed mean weights.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/summary.weightit.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Print and Summarize Output — summary.weightit","text":"Noah Greifer","code":""},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/reference/summary.weightit.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Print and Summarize Output — summary.weightit","text":"","code":"# See example at ?weightit or ?weightitMSM"},{"path":"https://ngreifer.github.io/WeightIt/reference/trim.html","id":null,"dir":"Reference","previous_headings":"","what":"Trim (Winsorize) Large Weights — trim","title":"Trim (Winsorize) Large Weights — trim","text":"Trims (.e., winsorizes) large weights setting weights higher given quantile weight quantile. can useful controlling extreme weights, can reduce effective sample size enlarging variability weights. Note observations fully discarded using trim(), may differ uses word \"trim\".","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/trim.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Trim (Winsorize) Large Weights — trim","text":"","code":"# S3 method for weightit trim(w, at = 0, lower = FALSE, ...)  # S3 method for numeric trim(w, at = 0, lower = FALSE, treat = NULL, ...)"},{"path":"https://ngreifer.github.io/WeightIt/reference/trim.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Trim (Winsorize) Large Weights — trim","text":"w weightit object vector weights. numeric; either quantile weights weights trimmed. single number .5 1, number weights trimmed (e.g., = 3 top 3 weights set 4th largest weight). lower logical; whether also trim lower quantile (e.g., = .9, trimming .1 .9, = 3, trimming top bottom 3 weights). treat vector treatment status unit. always included w numeric, can get away leaving treatment continuous estimand ATE binary multi-category treatments. ... used.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/trim.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Trim (Winsorize) Large Weights — trim","text":"trim() takes weightit object (output call weightit() weightitMSM()) numeric vector weights trims (winsorizes) specified quantile. weights quantile set weight quantile. lower = TRUE, weights 1 minus quantile set weight 1 minus quantile. general, trimming weights decreases balance also decreases variability weights, improving precision potential expense unbiasedness (Cole & Hernán, 2008). See Lee, Lessler, Stuart (2011) Thoemmes Ong (2015) discussions simulation results trimming weights various quantiles. Note trimming weights can also change target population therefore estimand. using trim() numeric vector weights, helpful include treatment vector well. helps determine type treatment estimand, used specify trimming performed. particular, estimand determined ATT ATC, weights target (.e., focal) group ignored, since equal 1. Otherwise, estimand ATE treatment continuous, weights considered trimming. general, weights group weights considered trimming.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/trim.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Trim (Winsorize) Large Weights — trim","text":"input weightit object, output weightit object weights replaced trimmed weights additional attribute, \"trim\", equal quantile trimming. input numeric vector weights, output numeric vector trimmed weights, aforementioned attribute.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/trim.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Trim (Winsorize) Large Weights — trim","text":"Cole, S. R., & Hernán, M. Á. (2008). Constructing Inverse Probability Weights Marginal Structural Models. American Journal Epidemiology, 168(6), 656–664. Lee, B. K., Lessler, J., & Stuart, E. . (2011). Weight Trimming Propensity Score Weighting. PLoS ONE, 6(3), e18174. Thoemmes, F., & Ong, . D. (2016). Primer Inverse Probability Treatment Weighting Marginal Structural Models. Emerging Adulthood, 4(1), 40–59.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/trim.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Trim (Winsorize) Large Weights — trim","text":"Noah Greifer","code":""},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/reference/trim.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Trim (Winsorize) Large Weights — trim","text":"","code":"library(\"cobalt\") data(\"lalonde\", package = \"cobalt\")  (W <- weightit(treat ~ age + educ + married +                  nodegree + re74, data = lalonde,                method = \"glm\", estimand = \"ATT\")) #> A weightit object #>  - method: \"glm\" (propensity score weighting with GLM) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 2-category #>  - estimand: ATT (focal: 1) #>  - covariates: age, educ, married, nodegree, re74 summary(W) #>                  Summary of weights #>  #> - Weight ranges: #>  #>            Min                                  Max #> treated 1.0000               ||              1.0000 #> control 0.0222 |---------------------------| 2.0438 #>  #> - Units with the 5 most extreme weights by group: #>                                             #>               7      5      4      3      1 #>  treated      1      1      1      1      1 #>             411    595    269    409    296 #>  control 1.3303 1.4365 1.5005 1.6369 2.0438 #>  #> - Weight statistics: #>  #>         Coef of Var   MAD Entropy # Zeros #> treated       0.000 0.000   -0.00       0 #> control       0.823 0.701    0.33       0 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted  429.       185 #> Weighted    255.99     185  #Trimming the top and bottom 5 weights trim(W, at = 5, lower = TRUE) #> Trimming the top and bottom 5 weights where treat is not 1. #> A weightit object #>  - method: \"glm\" (propensity score weighting with GLM) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 2-category #>  - estimand: ATT (focal: 1) #>  - covariates: age, educ, married, nodegree, re74 #>  - weights trimmed at the top and bottom 5   #Trimming at 90th percentile (W.trim <- trim(W, at = .9)) #> Trimming weights where treat is not 1 to 90%. #> A weightit object #>  - method: \"glm\" (propensity score weighting with GLM) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 2-category #>  - estimand: ATT (focal: 1) #>  - covariates: age, educ, married, nodegree, re74 #>  - weights trimmed at 90%   summary(W.trim) #>                  Summary of weights #>  #> - Weight ranges: #>  #>            Min                                    Max #> treated 1.0000                              || 1.0000 #> control 0.0222   |-------------------------|   0.9407 #>  #> - Units with the 5 most extreme weights by group: #>                                             #>               7      5      4      3      1 #>  treated      1      1      1      1      1 #>             303    296    285    269    264 #>  control 0.9407 0.9407 0.9407 0.9407 0.9407 #>  #> - Weight statistics: #>  #>         Coef of Var   MAD Entropy # Zeros #> treated       0.000 0.000  -0.000       0 #> control       0.766 0.682   0.303       0 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted  429.       185 #> Weighted    270.58     185 #Note that only the control weights were trimmed  #Trimming a numeric vector of weights all.equal(trim(W$weights, at = .9, treat = lalonde$treat),           W.trim$weights) #> Trimming weights where treat is not 1 to 90%. #> [1] TRUE  #Using made up data and as.weightit() treat <- rbinom(500, 1, .3) weights <- rchisq(500, df = 2) W <- as.weightit(weights = weights, treat = treat,                  estimand = \"ATE\") summary(W) #>                  Summary of weights #>  #> - Weight ranges: #>  #>            Min                                   Max #> treated 0.0782 |-----------------|            7.3680 #> control 0.0030 |---------------------------| 11.3178 #>  #> - Units with the 5 most extreme weights by group: #>                                              #>             203    333    436    335     103 #>  treated 5.5532 6.1825 6.2122 7.1462   7.368 #>             408    337    201    196      45 #>  control 8.8746 8.9234 9.7306 9.8349 11.3178 #>  #> - Weight statistics: #>  #>         Coef of Var   MAD Entropy # Zeros #> treated       0.808 0.640   0.309       0 #> control       0.929 0.714   0.391       0 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted  370.    130.   #> Weighted    198.76   78.93 summary(trim(W, at = .95)) #> Trimming weights to 95%. #>                  Summary of weights #>  #> - Weight ranges: #>  #>            Min                                  Max #> treated 0.0782 |---------------------------| 6.1935 #> control 0.0030 |---------------------------| 6.1935 #>  #> - Units with the 5 most extreme weights by group: #>                                             #>             203    333    436    335    103 #>  treated 5.5532 6.1825 6.1935 6.1935 6.1935 #>             152    131    115    114     45 #>  control 6.1935 6.1935 6.1935 6.1935 6.1935 #>  #> - Weight statistics: #>  #>         Coef of Var   MAD Entropy # Zeros #> treated       0.790 0.635   0.301       0 #> control       0.848 0.692   0.354       0 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted  370.    130.   #> Weighted    215.45   80.28"},{"path":"https://ngreifer.github.io/WeightIt/reference/weightit.fit.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Balancing Weights with Minimal Input Processing — weightit.fit","title":"Generate Balancing Weights with Minimal Input Processing — weightit.fit","text":"weightit.fit() dispatches one weight estimation methods determined method. internal function called weightit() probably used except special cases. Unlike weightit(), weightit.fit() accept formula data frame interface instead requires covariates treatment supplied numeric matrix atomic vector, respectively. way, weightit.fit() weightit() lm.fit() lm() - thinner, slightly faster interface performs minimal argument checking.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/weightit.fit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Balancing Weights with Minimal Input Processing — weightit.fit","text":"","code":"weightit.fit(covs,              treat,              method = \"glm\",              s.weights = NULL,              by.factor = NULL,              estimand = \"ATE\",              focal = NULL,              stabilize = FALSE,              ps = NULL,              moments = NULL,              int = FALSE,              subclass = NULL,              is.MSM.method = FALSE,              missing = NULL,              verbose = FALSE,              include.obj = FALSE,              ...)"},{"path":"https://ngreifer.github.io/WeightIt/reference/weightit.fit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Balancing Weights with Minimal Input Processing — weightit.fit","text":"covs numeric matrix covariates. treat vector treatment statuses. method string length 1 containing name method used estimate weights. See weightit() allowable options. default \"glm\" propensity score weighting using generalized linear model estimate propensity score. s.weights numeric vector sampling weights. See individual pages method information whether sampling weights can supplied. .factor factor variable weighting done within levels. Corresponds argument weightit(). estimand desired estimand. binary multi-category treatments, can \"ATE\", \"ATT\", \"ATC\", , methods, \"ATO\", \"ATM\", \"ATOS\". default \"ATE\". argument ignored continuous treatments. See individual pages method information estimands allowed method literature read interpret estimands. stabilize logical; whether stabilize weights. methods involve estimating propensity scores, involves multiplying unit's weight proportion units treatment group. Default FALSE. focal multi-category treatments used ATT weights requested, group consider \"treated\" focal group. group weighted, groups weighted like focal group. Must non-NULL estimand = \"ATT\" \"ATC\". ps vector propensity scores. specified, method ignored set \"glm\". moments, int, subclass arguments customize weight estimation. See weightit() details. .MSM.method see weightitMSM(). Typically can ignored. missing character; missing data handled. options depend method used. NULL, covs covs checked NA values, present, missing set \"ind\". \"\", covs covs checked NA values; can faster known none. verbose whether print additional information output fitting function. include.obj whether include output fit objects created process estimating weights. example, method = \"glm\", glm objects containing propensity score model included. See individual pages method information object included TRUE. ... arguments functions called weightit.fit() control aspects fitting covered arguments.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/weightit.fit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Balancing Weights with Minimal Input Processing — weightit.fit","text":"weightit.fit object following elements: weights estimated weights, one unit. ps estimated provided propensity scores. Estimated propensity scores returned binary treatments method \"glm\", \"gbm\", \"cbps\", \"super\", \"bart\". fit.obj include.obj = TRUE, fit object. info Additional information fitting. See individual methods pages included. weightit.fit object specialized print(), summary(), plot() methods. simply list containing components.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/weightit.fit.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Generate Balancing Weights with Minimal Input Processing — weightit.fit","text":"weightit.fit() called weightit() arguments weightit() checked processed. weightit.fit() dispatches function used actually estimate weights, passing supplied arguments directly. weightit.fit() meant used anyone experienced users specific use case mind. returned object contain information supplied arguments details estimation method; processed weightit. Less argument checking processing occurs weightit.fit() weightit(), means supplying incorrect arguments can result errors, crashes, invalid weights, error warning messages may helpful diagnosing problem. weightit.fit() check make sure weights actually estimated, though. weightit.fit() may useful speeding simulation simulation studies use weightit() covariates can supplied numeric matrix, often generated simulations, without go potentially slow process extracting covariates treatment formula data frame. user certain arguments valid (e.g., ensuring estimated weights consistent estimated weightit() arguments), less time needs spent processing arguments. Also, returned object much smaller weightit object covariates returned alongside weights.","code":""},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/reference/weightit.fit.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Generate Balancing Weights with Minimal Input Processing — weightit.fit","text":"Noah Greifer","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/weightit.fit.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate Balancing Weights with Minimal Input Processing — weightit.fit","text":"","code":"library(\"cobalt\") data(\"lalonde\", package = \"cobalt\")  #Balancing covariates between treatment groups (binary) covs_mat <- as.matrix(lalonde[c(\"age\", \"educ\", \"married\",                                 \"nodegree\", \"re74\", \"re75\")]) wf1 <- weightit.fit(covs_mat, treat = lalonde$treat,                     method = \"glm\", estimand = \"ATT\") str(wf1) #> List of 4 #>  $ weights: num [1:614] 1 1 1 1 1 1 1 1 1 1 ... #>  $ ps     : num [1:614] 0.296 0.462 0.391 0.558 0.483 ... #>  $ fit.obj: NULL #>  $ info   : Named list() #>  - attr(*, \"class\")= chr \"weightit.fit\""},{"path":"https://ngreifer.github.io/WeightIt/reference/weightit.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Balancing Weights — weightit","title":"Generate Balancing Weights — weightit","text":"weightit() allows easy generation balancing weights using variety available methods binary, continuous, multi-category treatments. Many methods exist packages, weightit() calls; packages must installed use desired method. Also included print() summary() methods examining output.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/weightit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Balancing Weights — weightit","text":"","code":"weightit(formula,          data = NULL,          method = \"glm\",          estimand = \"ATE\",          stabilize = FALSE,          focal = NULL,          by = NULL,          s.weights = NULL,          ps = NULL,          moments = NULL,          int = FALSE,          subclass = NULL,          missing = NULL,          verbose = FALSE,          include.obj = FALSE,          ...)  # S3 method for weightit print(x, ...)"},{"path":"https://ngreifer.github.io/WeightIt/reference/weightit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Balancing Weights — weightit","text":"formula formula treatment variable left hand side covariates balanced right hand side. See glm() details. Interactions functions covariates allowed. data optional data set form data frame contains variables formula. method string length 1 containing name method used estimate weights. See Details allowable options. default \"glm\" propensity score weighting using generalized linear model estimate propensity score. estimand desired estimand. binary multi-category treatments, can \"ATE\", \"ATT\", \"ATC\", , methods, \"ATO\", \"ATM\", \"ATOS\". default \"ATE\". argument ignored continuous treatments. See individual pages method information estimands allowed method literature read interpret estimands. stabilize logical; whether stabilize weights. methods involve estimating propensity scores, involves multiplying unit's weight proportion units treatment group. Default FALSE. focal multi-category treatments used ATT weights requested, group consider \"treated\" focal group. group weighted, groups weighted like focal group. specified, estimand automatically set \"ATT\". string containing name variable data weighting done within categories one-sided formula stratifying variable right-hand side. example, = \"gender\" = ~gender, weights generated separately within level variable \"gender\". (argument used called exact, still work message.) one variable allowed; stratify multiply variables simultaneously, create new variable full cross variables using interaction(). s.weights vector sampling weights name variable data contains sampling weights. can also matching weights weighting used matched data. See individual pages method information whether sampling weights can supplied. ps vector propensity scores name variable data containing propensity scores. NULL, method ignored, propensity scores used create weights. formula must include treatment variable data, listed covariates play role weight estimation. Using ps similar calling get_w_from_ps() directly, produces full weightit object rather just producing weights. moments numeric; methods, greatest power covariate balanced. example, moments = 3, non-categorical covariate, covariate, square, cube balanced. argument ignored methods; balance powers covariates, appropriate functions must entered formula. See individual pages method information whether accept moments. int logical; methods, whether first-order interactions covariates balanced. argument ignored methods; balance interactions variables, appropriate functions must entered formula. See individual pages method information whether accept int. subclass numeric; number subclasses use computing weights using marginal mean weighting subclasses (MMWS). NULL, standard inverse probability weights (extensions) computed; number greater 1, subclasses formed weights computed based subclass membership. Attempting set non-NULL value methods compute propensity score result error; see method's help page information whether MMWS weights compatible method. See get_w_from_ps() details references. missing character; missing data handled. options defaults depend method used. Ignored missing data present. noted multiple imputation outperforms available missingness methods available weightit() probably used instead. Consider MatchThem package use weightit() multiply imputed data. verbose logical; whether print additional information output fitting function. include.obj logical; whether include output fit objects created process estimating weights. example, method = \"glm\", glm objects containing propensity score model included. See individual pages method information object included TRUE. ... arguments functions called weightit() control aspects fitting covered arguments. See Details. x weightit object; output call weightit().","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/weightit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Balancing Weights — weightit","text":"weightit object following elements: weights estimated weights, one unit. treat values treatment variable. covs covariates used fitting. includes raw covariates, may altered fitting process. estimand estimand requested. method weight estimation method specified. ps estimated provided propensity scores. Estimated propensity scores returned binary treatments method \"glm\", \"gbm\", \"cbps\", \"super\", \"bart\". s.weights provided sampling weights. focal focal variable ATT requested multi-category treatment. data.frame containing variable specified. obj include.obj = TRUE, fit object. info Additional information fitting. See individual methods pages included.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/weightit.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Generate Balancing Weights — weightit","text":"primary purpose weightit() dispatcher functions perform estimation balancing weights using requested method. methods allowed links pages containing information , including additional arguments outputs (e.g., include.obj = TRUE), missing values treated, estimands allowed, whether sampling weights allowed. \"glm\" - Propensity score weighting using generalized linear models. \"gbm\" - Propensity score weighting using generalized boosted modeling. \"cbps\" - Covariate Balancing Propensity Score weighting. \"npcbps\" - Non-parametric Covariate Balancing Propensity Score weighting. \"ebal\" - Entropy balancing. \"optweight\" - Optimization-based weighting. \"super\" - Propensity score weighting using SuperLearner. \"bart\" - Propensity score weighting using Bayesian additive regression trees (BART). \"energy\" - Energy balancing. method can also supplied user-defined function; see method_user instructions examples. using weightit(), please cite WeightIt package (using citation(\"WeightIt\")) paper(s) references section method used.","code":""},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/reference/weightit.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Generate Balancing Weights — weightit","text":"Noah Greifer","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/weightit.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate Balancing Weights — weightit","text":"","code":"library(\"cobalt\") data(\"lalonde\", package = \"cobalt\")  #Balancing covariates between treatment groups (binary) (W1 <- weightit(treat ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"glm\", estimand = \"ATT\")) #> A weightit object #>  - method: \"glm\" (propensity score weighting with GLM) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 2-category #>  - estimand: ATT (focal: 1) #>  - covariates: age, educ, married, nodegree, re74 summary(W1) #>                  Summary of weights #>  #> - Weight ranges: #>  #>            Min                                  Max #> treated 1.0000               ||              1.0000 #> control 0.0222 |---------------------------| 2.0438 #>  #> - Units with the 5 most extreme weights by group: #>                                             #>               7      5      4      3      1 #>  treated      1      1      1      1      1 #>             411    595    269    409    296 #>  control 1.3303 1.4365 1.5005 1.6369 2.0438 #>  #> - Weight statistics: #>  #>         Coef of Var   MAD Entropy # Zeros #> treated       0.000 0.000   -0.00       0 #> control       0.823 0.701    0.33       0 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted  429.       185 #> Weighted    255.99     185 bal.tab(W1) #> Balance Measures #>                Type Diff.Adj #> prop.score Distance   0.0199 #> age         Contin.   0.0459 #> educ        Contin.  -0.0360 #> married      Binary   0.0044 #> nodegree     Binary   0.0080 #> re74        Contin.  -0.0275 #>  #> Effective sample sizes #>            Control Treated #> Unadjusted  429.       185 #> Adjusted    255.99     185 #Balancing covariates with respect to race (multi-category) (W2 <- weightit(race ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"ebal\", estimand = \"ATE\")) #> A weightit object #>  - method: \"ebal\" (entropy balancing) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 3-category (black, hispan, white) #>  - estimand: ATE #>  - covariates: age, educ, married, nodegree, re74 summary(W2) #>                  Summary of weights #>  #> - Weight ranges: #>  #>           Min                                  Max #> black  0.5530   |-------------------------| 5.3496 #> hispan 0.1409 |----------------|            3.3322 #> white  0.3979  |-------|                    1.9224 #>  #> - Units with the 5 most extreme weights by group: #>                                            #>            226    244    485    181    182 #>   black 2.5215 2.5491 2.8059 3.5551 5.3496 #>            392    564    269    345    371 #>  hispan 2.0467   2.53 2.6322 2.7049 3.3322 #>             68    457    599    589    531 #>   white 1.7106 1.7226 1.7426 1.7743 1.9224 #>  #> - Weight statistics: #>  #>        Coef of Var   MAD Entropy # Zeros #> black        0.590 0.413   0.131       0 #> hispan       0.609 0.440   0.163       0 #> white        0.371 0.306   0.068       0 #>  #> - Effective Sample Sizes: #>  #>             black hispan  white #> Unweighted 243.    72.   299.   #> Weighted   180.47  52.71 262.93 bal.tab(W2) #> Balance summary across all treatment pairs #>             Type Max.Diff.Adj #> age      Contin.       0.0001 #> educ     Contin.       0.0000 #> married   Binary       0.0001 #> nodegree  Binary       0.0001 #> re74     Contin.       0.0001 #>  #> Effective sample sizes #>             black hispan  white #> Unadjusted 243.    72.   299.   #> Adjusted   180.47  52.71 262.93 #Balancing covariates with respect to re75 (continuous) (W3 <- weightit(re75 ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"cbps\", over = FALSE)) #> A weightit object #>  - method: \"cbps\" (covariate balancing propensity score weighting) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: continuous #>  - covariates: age, educ, married, nodegree, re74 summary(W3) #>                  Summary of weights #>  #> - Weight ranges: #>  #>        Min                                  Max #> all 0.0154 |---------------------------| 13.157 #>  #> - Units with the 5 most extreme weights: #>                                           #>        484     482     180     481    483 #>  all 9.215 10.8401 11.1959 11.9913 13.157 #>  #> - Weight statistics: #>  #>     Coef of Var  MAD Entropy # Zeros #> all       1.153 0.45   0.289       0 #>  #> - Effective Sample Sizes: #>  #>             Total #> Unweighted 614.   #> Weighted   263.72 bal.tab(W3) #> Balance Measures #>             Type Corr.Adj #> age      Contin.  -0.0004 #> educ     Contin.  -0.0001 #> married   Binary  -0.0004 #> nodegree  Binary   0.0000 #> re74     Contin.  -0.0008 #>  #> Effective sample sizes #>             Total #> Unadjusted 614.   #> Adjusted   263.72"},{"path":"https://ngreifer.github.io/WeightIt/reference/weightitMSM.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Balancing Weights for Longitudinal Treatments — weightitMSM","title":"Generate Balancing Weights for Longitudinal Treatments — weightitMSM","text":"weightitMSM() allows easy generation balancing weights marginal structural models time-varying treatments using variety available methods binary, continuous, multinomial treatments. Many methods exist packages, weightit() calls; packages must installed use desired method. Also included print() summary() methods examining output. Currently \"wide\" data sets, row corresponds unit's entire variable history, supported. can use reshape() functions transform data format; see example .","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/weightitMSM.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Balancing Weights for Longitudinal Treatments — weightitMSM","text":"","code":"weightitMSM(formula.list,             data = NULL,             method = \"glm\",             stabilize = FALSE,             by = NULL,             s.weights = NULL,             num.formula = NULL,             moments = NULL,             int = FALSE,             missing = NULL,             verbose = FALSE,             include.obj = FALSE,             is.MSM.method,             weightit.force = FALSE,             ...)  # S3 method for weightitMSM print(x, ...)"},{"path":"https://ngreifer.github.io/WeightIt/reference/weightitMSM.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Balancing Weights for Longitudinal Treatments — weightitMSM","text":"formula.list list formulas corresponding time point time-specific treatment variable left hand side pre-treatment covariates balanced right hand side. formulas must temporal order, must contain covariates balanced time point (.e., treatments covariates featured early formulas appear later ones). Interactions functions covariates allowed. data optional data set form data frame contains variables formulas formula.list. must wide data set exactly one row per unit. method string length 1 containing name method used estimate weights. See weightit() allowable options. default \"glm\", estimates weights using generalized linear models. stabilize logical; whether stabilize weights. Stabilizing weights involves fitting model predicting treatment time point treatment status prior time points. TRUE, fully saturated model fit (.e., interactions treatments time point), essentially using observed treatment probabilities numerator (binary multinomial treatments). may yield error combinations observed. Default FALSE. manually specify stabilization model formulas, e.g., specify non-saturated models, use num.formula. many time points, saturated models may time-consuming impossible fit. num.formula optional; one-sided formula stabilization factors (previous treatments) right hand side, adds, time point, stabilization factors model saturated previous treatments. See Cole & Hernán (2008) discussion specify model; including stabilization factors can change estimand without proper adjustment, done caution. Can also list one-sided formulas, one time point. Unless know , recommend setting stabilize = TRUE ignoring num.formula. string containing name variable data weighting done within categories one-sided formula stratifying variable right-hand side. example, = \"gender\" = ~ gender, weights generated separately within level variable \"gender\". argument used called exact, still work message. one variable allowed. s.weights vector sampling weights name variable data contains sampling weights. ignored methods. moments numeric; methods, greatest power covariate balanced. example, moments = 3, non-categorical covariate, covariate, square, cube balanced. argument ignored methods; balance powers covariates, appropriate functions must entered formula. See specific methods help pages information whether accept moments. int logical; methods, whether first-order interactions covariates balanced. argument ignored methods; balance interactions variables, appropriate functions must entered formula. See specific methods help pages information whether accept int. missing character; missing data handled. options defaults depend method used. Ignored missing data present. noted multiple imputation outperforms available missingness methods available weightit() probably used instead. See MatchThem package use weightit() multiply imputed data. verbose whether print additional information output fitting function. include.obj whether include output list fit objects created process estimating weights time point. example, method = \"glm\", list glm objects containing propensity score models time point included. See help pages method information object included TRUE. .MSM.method whether method estimates weights multiple time points (TRUE) estimating weights time point multiplying together (FALSE). relevant method = \"optweight\"), estimates weights longitudinal treatments , user-specified functions. weightit.force several methods valid estimating weights longitudinal treatments, produce error message attempted. Set TRUE bypass error message. ... arguments functions called weightit() control aspects fitting covered arguments. See Details weightit(). x weightitMSM object; output call weightitMSM().","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/weightitMSM.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Balancing Weights for Longitudinal Treatments — weightitMSM","text":"weightitMSM object following elements: weights estimated weights, one unit. treat.list list values time-varying treatment variables. covs.list list covariates used fitting time point. includes raw covariates, may altered fitting process. data data.frame originally entered weightitMSM(). estimand \"ATE\", currently estimand MSMs binary multinomial treatments. method weight estimation method specified. ps.list list estimated propensity scores () time point. s.weights provided sampling weights. data.frame containing variable specified. stabilization stabilization factors, .","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/weightitMSM.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Generate Balancing Weights for Longitudinal Treatments — weightitMSM","text":"general, weightitMSM() works separating estimation weights separate procedures time period based formulas provided. formula, weightitMSM() simply calls weightit() formula, collects weights time period, multiplies together arrive longitudinal balancing weights. formula contain covariates balanced . example, formula corresponding second time period contain baseline covariates, treatment variable first time period, time-varying covariates took values first treatment second. Currently, wide data sets supported, unit represented exactly one row contains covariate treatment history encoded separate variables. \"cbps\" method, calls CBPS() CBPS, yield different results CBMSM() CBPS CBMSM() takes different approach generating weights simply estimating several time-specific models.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/weightitMSM.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Generate Balancing Weights for Longitudinal Treatments — weightitMSM","text":"Noah Greifer","code":""},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/reference/weightitMSM.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Generate Balancing Weights for Longitudinal Treatments — weightitMSM","text":"Cole, S. R., & Hernán, M. . (2008). Constructing Inverse Probability Weights Marginal Structural Models. American Journal Epidemiology, 168(6), 656–664. doi:10.1093/aje/kwn164","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/weightitMSM.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate Balancing Weights for Longitudinal Treatments — weightitMSM","text":"","code":"library(\"cobalt\")  data(\"msmdata\") (W1 <- weightitMSM(list(A_1 ~ X1_0 + X2_0,                         A_2 ~ X1_1 + X2_1 +                           A_1 + X1_0 + X2_0,                         A_3 ~ X1_2 + X2_2 +                           A_2 + X1_1 + X2_1 +                           A_1 + X1_0 + X2_0),                    data = msmdata,                    method = \"glm\")) #> A weightitMSM object #>  - method: \"glm\" (propensity score weighting with GLM) #>  - number of obs.: 7500 #>  - sampling weights: none #>  - number of time points: 3 (A_1, A_2, A_3) #>  - treatment:  #>     + time 1: 2-category #>     + time 2: 2-category #>     + time 3: 2-category #>  - covariates:  #>     + baseline: X1_0, X2_0 #>     + after time 1: X1_1, X2_1, A_1, X1_0, X2_0 #>     + after time 2: X1_2, X2_2, A_2, X1_1, X2_1, A_1, X1_0, X2_0 summary(W1) #>                  Summary of weights #>  #>                        Time 1                        #>                  Summary of weights #>  #> - Weight ranges: #>  #>            Min                                    Max #> treated 1.0791 |---------------------------| 403.4833 #> control 1.2761 |-------------------|         284.7636 #>  #> - Units with the 5 most extreme weights by group: #>                                                       #>              5488     3440     3593     1286     5685 #>  treated  166.992 170.5549 196.4136 213.1934 403.4833 #>              2594     2932     5226     1875     2533 #>  control 155.6248  168.964 172.4195 245.8822 284.7636 #>  #> - Weight statistics: #>  #>         Coef of Var   MAD Entropy # Zeros #> treated       1.914 0.816   0.649       0 #> control       1.706 0.862   0.670       0 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted 3306.    4194.  #> Weighted    845.79   899.4 #>  #>                        Time 2                        #>                  Summary of weights #>  #> - Weight ranges: #>  #>            Min                                    Max #> treated 1.0791 |---------------------------| 403.4833 #> control 1.2761 |----------------|            245.8822 #>  #> - Units with the 5 most extreme weights by group: #>                                                       #>              2932     3440     3593     2533     5685 #>  treated  168.964 170.5549 196.4136 284.7636 403.4833 #>              2594     5488     5226     1286     1875 #>  control 155.6248  166.992 172.4195 213.1934 245.8822 #>  #> - Weight statistics: #>  #>         Coef of Var   MAD Entropy # Zeros #> treated       1.892 0.819   0.652       0 #> control       1.748 0.869   0.686       0 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted 3701.   3799.   #> Weighted    912.87  829.87 #>  #>                        Time 3                        #>                  Summary of weights #>  #> - Weight ranges: #>  #>            Min                                    Max #> treated 1.0791 |---------------------------| 403.4833 #> control 1.2761 |---------|                   148.1547 #>  #> - Units with the 5 most extreme weights by group: #>                                                       #>              3593     1286     1875     2533     5685 #>  treated 196.4136 213.1934 245.8822 284.7636 403.4833 #>              6862      168     3729     6158     3774 #>  control  88.0721  97.8273  104.623 121.8451 148.1547 #>  #> - Weight statistics: #>  #>         Coef of Var   MAD Entropy # Zeros #> treated       1.832 0.975   0.785       0 #> control       1.254 0.683   0.412       0 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted 4886.   2614.   #> Weighted   1900.26  600.12 #>  bal.tab(W1) #> Balance summary across all time points #>              Times     Type Max.Diff.Adj #> prop.score 1, 2, 3 Distance       0.0773 #> X1_0       1, 2, 3  Contin.       0.0342 #> X2_0       1, 2, 3   Binary       0.0299 #> X1_1          2, 3  Contin.       0.0657 #> X2_1          2, 3   Binary       0.0299 #> A_1           2, 3   Binary       0.0262 #> X1_2             3  Contin.       0.0643 #> X2_2             3   Binary       0.0096 #> A_2              3   Binary       0.0054 #>  #> Effective sample sizes #>  - Time 1 #>            Control Treated #> Unadjusted 3306.    4194.  #> Adjusted    845.79   899.4 #>  - Time 2 #>            Control Treated #> Unadjusted 3701.   3799.   #> Adjusted    912.87  829.87 #>  - Time 3 #>            Control Treated #> Unadjusted 4886.   2614.   #> Adjusted   1900.26  600.12  #Using stabilization factors W2 <- weightitMSM(list(A_1 ~ X1_0 + X2_0,                         A_2 ~ X1_1 + X2_1 +                           A_1 + X1_0 + X2_0,                         A_3 ~ X1_2 + X2_2 +                           A_2 + X1_1 + X2_1 +                           A_1 + X1_0 + X2_0),                    data = msmdata,                    method = \"glm\",                    stabilize = TRUE,                    num.formula = list(~ 1,                                       ~ A_1,                                       ~ A_1 + A_2))  #Same as above but with fully saturated stabilization factors #(i.e., making the last entry in 'num.formula' A_1*A_2) W3 <- weightitMSM(list(A_1 ~ X1_0 + X2_0,                         A_2 ~ X1_1 + X2_1 +                           A_1 + X1_0 + X2_0,                         A_3 ~ X1_2 + X2_2 +                           A_2 + X1_1 + X2_1 +                           A_1 + X1_0 + X2_0),                    data = msmdata,                    method = \"glm\",                    stabilize = TRUE)"},{"path":"https://ngreifer.github.io/WeightIt/news/index.html","id":"weightit-0141","dir":"Changelog","previous_headings":"","what":"WeightIt 0.14.1","title":"WeightIt 0.14.1","text":"cobalt version 4.5.1 greater now required. Fixed bug using balance Super Learner cobalt 4.5.1. Added section Estimating Effects vignette (vignette(\"estimating-effects\")) estimating effect continuous treatment weighting.","code":""},{"path":"https://ngreifer.github.io/WeightIt/news/index.html","id":"weightit-0140","dir":"Changelog","previous_headings":"","what":"WeightIt 0.14.0","title":"WeightIt 0.14.0","text":"CRAN release: 2023-04-12 Added energy balancing continuous treatments, requested using method = \"energy\", described Huling et al. (2023). weights minimize distance covariance treatment covariates maintaining representativeness. method supports exact balance constraints, distributional balance constraints, sampling weights. implementation similar independenceWeights package. See ?method_energy details. Added new vignette estimating effects weighting, accessible using vignette(\"estimating-effects\", package = \"WeightIt\"). new workflow relies marginaleffects package. main vignette (vignette(\"WeightIt\")) modernized well. Added new dataset, msmdata, demonstrate capabilities longitudinal treatments. twang longer dependency. Methods use balance criterion select tuning parameter, particular GBM balance Super Learner, now rely cobalt’s bal.init() bal.compute() functionality, adds new balance criteria. stop.method argument functions renamed criterion help(\"stop.method\") removed; page now available help(\"bal.compute\", package = \"cobalt\"), describes additional statistics available. also fixes bugs present balance criteria. Renamed method = \"ps\" method = \"glm\". \"ps\" continues work always back compatibility. \"glm\" descriptive name since many methods use propensity scores; distinguishes method uses generalized linear models. Using method = \"ebcw\" empirical balancing calibration weighting longer available ATE package removed. Use method = \"ebal\" entropy balancing instead, essentially identical. Updated trim() documentation clarify form trimming implemented (.e., winsorizing). Suggested David Novgorodsky. Fixed bugs s.weights equal zero method = \"ebal\", “cbps\", \"energy\". Suggested @statzhero. (#41) Improved performance method = \"energy\" ATT. Fixed bug using method = \"energy\" . method = \"energy\", setting int = TRUE automatically sets moments = 1 unspecified. Errors warnings updated use chk. missingness indicator approach now imputes variable median rather 0 missing values. change performance methods, change others, doesn’t affect balance assessment.","code":""},{"path":"https://ngreifer.github.io/WeightIt/news/index.html","id":"weightit-0131","dir":"Changelog","previous_headings":"","what":"WeightIt 0.13.1","title":"WeightIt 0.13.1","text":"CRAN release: 2022-06-28 ordinal multi-category treatments, setting link = \"br.logit\" now uses brglm2::bracl() fit bias-reduced ordinal regression model. Added vignette “Installing Supporting Packages” explain install various packages might needed WeightIt use certain methods, including package CRAN. See vignette vignette(\"installing-packages\"). Fixed bug occur factor character predictor single level passed weightit(). Improved code entropy balancing, fixing bug using s.weights continuous treatment improving messages optimization fails converge. (#33) Improved robustness documentation missing packages. Updated logo, thanks Ben Stillerman.","code":""},{"path":"https://ngreifer.github.io/WeightIt/news/index.html","id":"weightit-0130","dir":"Changelog","previous_headings":"","what":"WeightIt 0.13.0","title":"WeightIt 0.13.0","text":"Fixed bug occur formula.tools package loaded, occur commonly logistf loaded. cause error treatment covariates must number units. (#25) Fixed bug info component included output weightit() using method = \"super\". Added ability specify num.formula list formulas weightitMSM(). primarily get around fact stabilize = TRUE, fully saturated model treatments used compute stabilization factor, , many time points, time-consuming may impossible (especially treatment combinations observed). Thanks @maellecoursonnais bringing issue (#27). ps.cont() retired since functionality available using weightit() method = \"gbm\" twangContinuous package. method = \"energy\", new argument, lambda, can supplied, puts penalty square weights control effective sample size. Typically needed can help balancing aggressive. method = \"energy\", min.w can now negative, allowing negative weights. method = \"energy\", dist.mat can now supplied name method compute distance matrix: \"scaled_euclidean\", \"mahalanobis\", \"euclidean\". Support negative weights added summary(). Negative weights possible (though default) using method = \"energy\" method = \"optweight\". Fixed bug glm() fail converge method = \"ps\" binary treatments due bad starting values. (#31) miss = \"saem\" can used method = \"ps\" missing values present covariates. Fixed bugs processing input formulas. error now thrown incorrect link supplied method = \"ps\".","code":""},{"path":"https://ngreifer.github.io/WeightIt/news/index.html","id":"weightit-0120","dir":"Changelog","previous_headings":"","what":"WeightIt 0.12.0","title":"WeightIt 0.12.0","text":"CRAN release: 2021-04-03 use method = \"twang\" retired now give error message. Use method = \"gbm\" nearly identical functionality options, detailed ?method_gbm. multinomial treatments link = \"logit\" (default), mclogit package installed, can requested estimating propensity score setting option use.mclogit = TRUE, uses mclogit::mblogit(). give results default, uses mlogit, can faster recommended. Added plot() method summary.weightitMSM objects functions just like plot.summary.weightit() time point. Fixed bug summary.weightit() labels top weights incorrect. Thanks Adam Lilly. Fixed bug sbps() using stochastic search (.e., full.search = FALSE 8 moderator levels). (#17) Fixed bug occur weights treatment group NA. Bad weights (.e., ) now produce warning rather error weights can diagnosed manually. (#18) Fixed bug using method = \"energy\" estimand = \"ATE\" improved = TRUE (default). -treatment energy distance contribution half ; now corrected. Added L1 median measure balance criterion. See ?stop.method details. Fixed bug logical treatments yield error. (#21) Fixed bug Warning: Deprecated appear sometimes purrr (part tidyverse) loaded. (#22) Thanks MrFlick StackOverflow solution.","code":""},{"path":"https://ngreifer.github.io/WeightIt/news/index.html","id":"weightit-0110","dir":"Changelog","previous_headings":"","what":"WeightIt 0.11.0","title":"WeightIt 0.11.0","text":"CRAN release: 2021-02-02 Added support estimating propensity scores using Bayesian additive regression trees (BART) method = \"bart\". method fits BART model treatment using functions dbarts package estimate propensity scores used weights. Binary, multinomial, continuous treatments supported. BART uses Bayesian priors hyperparameters, hyperparameter tuning necessary get well-performing predictions. Fixed bug using method = \"gbm\" stop.method = \"cv{#}\". Fixed bug setting estimand = \"ATC\" methods produce propensity score. past, output propensity score probability control group; now, probability treated group, estimands. affect weights. Setting method = \"twang\" now deprecated. Use method = \"gbm\" improved performance increased functionality. method = \"twang\" relies twang package; method = \"gbm\" calls gbm directly. Using method = \"ebal\" longer requires ebal package. Instead, optim() used, continuous treatments. Balance little better, options removed. using method = \"ebal\" continuous treatments, new argument, d.moments, can now specified. controls number moments covariate treatment distributions constrained weighted sample original sample. Vegetabile et al. (2020) recommend setting d.moments least 3 ensure generalizability reduce bias due effect modification. Made minor changes summary.weightit() plot.summary.weightit(). Fixed negative entropy computed. option use.mnlogit weightit() multi-category treatments method = \"ps\" removed mnlogit appears uncooperative. Fixed bug (#16) using method = \"cbps\" factor variables, thanks @danielebottigliengo. Fixed bug using binary factor treatments, thanks Darren Stewart. Cleaned documentation.","code":""},{"path":"https://ngreifer.github.io/WeightIt/news/index.html","id":"weightit-0102","dir":"Changelog","previous_headings":"","what":"WeightIt 0.10.2","title":"WeightIt 0.10.2","text":"CRAN release: 2020-08-27 Fixed bug treatment values accidentally switched methods.","code":""},{"path":"https://ngreifer.github.io/WeightIt/news/index.html","id":"weightit-0101","dir":"Changelog","previous_headings":"","what":"WeightIt 0.10.1","title":"WeightIt 0.10.1","text":"CRAN release: 2020-08-12 method = \"gbm\", added ability tune hyperparameters like interaction.depth distribution using criteria used select optimal tree. summary tuning results included info weightit output object. Fixed bug moments int ignored unless specified. Effective sample sizes now print two digits (believe , don’t need three) print cleanly whole numbers. Fixed bug using , thanks @frankpopham. (#11) Fixed bug using weightitMSM methods process int moments (though probably shouldn’t use anyway). Thanks Sven Reiger. Fixed bug using method = \"npcbps\" weights excessively small mistaken . weights now sum number units.","code":""},{"path":"https://ngreifer.github.io/WeightIt/news/index.html","id":"weightit-0100","dir":"Changelog","previous_headings":"","what":"WeightIt 0.10.0","title":"WeightIt 0.10.0","text":"CRAN release: 2020-07-07 Added support energy balancing method = \"energy\". method minimizes energy distance samples, multivariate distance measure. method uses code written specifically WeightIt (.e., call package specifically designed energy balancing) using osqp package optimization (optweight). See Huling & Mak (2020) details method. Also included option require exact balance moments covariates minimizing energy distance. method works binary multinomial treatments ATE, ATT, ATC. Sampling weights supported. method requires calculation manipulation distance matrix units, can slow /memory intensive large datasets. Improvements method = \"gbm\" method = \"super\" SL.method = \"method.balance\". new suite stop.methods allowed. binary treatments, include energy distance, sample Mahalanobis distance, pseudo-R2 weighted treatment model, among others. See ?stop.method allowable options. addition, performance quite bit faster. multinomial treatments link = \"logit\" (default), mnlogit package installed, can requested estimating propensity score setting option use.mnlogit = TRUE. give results default, uses mlogit, can faster large datasets. Added option estimand = \"ATOS\" “optimal subset” treatment effect described Crump et al. (2009). estimand finds subset units , ATE weights applied, yields treatment effect lowest variance, assuming homoscedasticity (assumptions). available binary treatments method = \"ps\". general makes sense use estimand = \"ATO\" want low-variance estimate don’t care target population, added completeness. available get_w_from_ps() well. make_full_rank() now faster. Cleaning error messages. Fixed bug using link = \"log\" method = \"ps\" binary treatments. Fixed bug using method = \"cbps\" continuous treatments sampling weights. Previously returned weights included sampling weights multiplied ; now separated, scenarios methods. Improved processing non-0/1 binary treatments, including method = \"gbm\". guess made treatment considered “treated”; affects produced propensity scores weights. Changed default value trim() .99 0. Added output number weights equal zero summary.weightit. can especially helpful using \"optweight\" \"energy\" methods using estimand = \"ATOS\".","code":""},{"path":"https://ngreifer.github.io/WeightIt/news/index.html","id":"weightit-090","dir":"Changelog","previous_headings":"","what":"WeightIt 0.9.0","title":"WeightIt 0.9.0","text":"CRAN release: 2020-02-11 Added support entropy balancing (method = \"ebal\") continuous treatments described Tübbicke (2020). Relies hand-written code contributed Stefan Tübbicke rather another R package. Sampling weights base weights supported binary multi-category treatments. Added support Balance SuperLearner described Pirracchio Carone (2018) method = \"super\". Rather using NNLS choose optimal combination predictions, can now optimize balance. , set SL.method = \"method.balance\". need set argument stop.method, works identically method = \"gbm\". example, stop.method = \"es.max\", predicted values given combination predicted values minimizes largest absolute standardized mean difference covariates sample weighted using predicted values propensity scores. Changed statistics displayed using summary(): weight ratio gone (weights can 0, problematic explode ratio), mean absolute deviation entropy weights now present. Added crayon prettier printing summary() output.","code":""},{"path":"https://ngreifer.github.io/WeightIt/news/index.html","id":"weightit-080","dir":"Changelog","previous_headings":"","what":"WeightIt 0.8.0","title":"WeightIt 0.8.0","text":"CRAN release: 2020-01-12 Formula interfaces now accept poly(x, .) matrix-generating functions variables, including rms-class-generating functions rms package (e.g., pol(), rcs(), etc.) (rms package must loaded use latter ones) basis-class-generating functions splines package (.e., bs() ns()). bug early version found @ahinton-mmc. Added support marginal mean weighting stratification (MMWS) described Hong (2010, 2012) weightit() get_w_from_ps() subclass argument (see References ?get_w_from_ps). method, subclasses formed based propensity score weights computed based number units subclass. MMWS can used method produces propensity score. implementation ensures subclasses least one member filling empty subclasses neighboring units. Added stabilize option get_w_from_ps(). new missing argument added weightit() choose missing data covariates handled. methods, \"ind\" (.e., missing indicators single-value imputation) allowed, \"ps\", \"gbm\", \"twang\", methods possible. method = \"ps\", stochastic approximation EM algorithm (SAEM) can used misaem package setting missing = \"saem\". continuous treatments \"ps\", \"gbm\", \"super\" methods (.e., conditional density treatment needs estimated), user can now supply density string function rather using normal density kernel density estimation. example, use density t-distribution 3 degrees freedom, one can set density = \"dt_3\". T-distributions often work better normal distributions extreme values treatment. methods now info component output object. contains information might useful diagnosing reporting method. example, method = \"gbm\", info contains tree used compute weights balance resulting trees, can plotted using plot(). method = \"super\", info contains coefficients stacking model cross-validation risk component methods. method = \"gbm\", best tree can chosen using cross validation rather balance setting stop.method = \"cv5\", e.g., 5-fold cross-validation. method = \"gbm\", new optional argument start.tree can set select tree balance begins computed. can speed things know best tree within first 100 trees, example. using method = \"gbm\" multi-category treatments estimands ATE, ATT, ATC used standardized mean differences stopping rule, mean differences weighted overall sample treatment group. Otherwise, efficiency improvements. using method = \"ps\" multi-category treatments, use use.mlogit = FALSE request multiple binary regressions instead multinomial regression now documented associated bug now fixed, thanks @ahinton-mmc. use method = \"super\", one can now set discrete = TRUE use discrete SuperLearner instead stacked SuperLearner, probably shouldn’t. moments int can now used method = \"npcbps\". Performance enhancements.","code":""},{"path":"https://ngreifer.github.io/WeightIt/news/index.html","id":"weightit-071","dir":"Changelog","previous_headings":"","what":"WeightIt 0.7.1","title":"WeightIt 0.7.1","text":"CRAN release: 2019-10-30 Fixed bug using weightit() inside another function passed argument explicitly. Also changed syntax ; must now either string (always possible) one-sided formula stratifying variable right-hand side. use variable data, must use formula interface. Fixed bug trying use ps weightit().","code":""},{"path":"https://ngreifer.github.io/WeightIt/news/index.html","id":"weightit-070","dir":"Changelog","previous_headings":"","what":"WeightIt 0.7.0","title":"WeightIt 0.7.0","text":"CRAN release: 2019-10-16 Added new sbps() function estimating subgroup balancing propensity score weights, including standard method new smooth version. Setting method = \"gbm\" method = \"twang\" now two different things. method = \"gbm\" uses gbm cobalt functions estimate weights much faster, method = \"twang\" uses twang functions estimate weights. results similar two methods. Prior version, method = \"gbm\" method = \"twang\" method = \"twang\" now. Bug fixes stabilize = TRUE, thanks @ulriksartipy Sven Rieger. Fixes using base.weight argument method = \"ebal\". Now supplied vector length equal number units dataset (contrast use ebalance, requires length equal number control units). Restored dependency cobalt examples vignette. method = \"ps\" treatment ordered (.e., ordinal), MASS::polr() used fit ordinal regression. Make treatment un-ordered use multinomial regression instead. Added support using bias-reduced fitting functions method = \"ps\" provided brglm2 package. can accessed changing link , example, \"br.logit\" \"br.probit\". multinomial treatments, setting link = \"br.logit\" fits bias-reduced multinomial regression model using brglm2::brmultinom(). can helpful regular maximum likelihood models fail converge, though may also sign lack overlap.","code":""},{"path":"https://ngreifer.github.io/WeightIt/news/index.html","id":"weightit-060","dir":"Changelog","previous_headings":"","what":"WeightIt 0.6.0","title":"WeightIt 0.6.0","text":"CRAN release: 2019-09-05 Bug fixes. Functions now work better used inside functions (e.g., lapply). Behavior weightit() presence non-NULL focal changed. focal specified, estimand assumed ATT. Previously, focal ignored unless estimand = \"ATT\". Processing estimand focal improved. Functions smarter guessing group focal group one isn’t specified, especially non-numeric treatments. focal can now used estimand = \"ATC\" indicate group control group, \"ATC\" \"ATT\" now function similarly. Added function get_w_from_ps() transform propensity scores weights (instead go weightit()). Added functions .weightit() .weightitMSM() convert weights treatments components weightit objects summary.weightit() can used . Updated documentation describe missing data covariates handled. bugs related missing data fixed well, thanks Yong Hao Pua. ps.cont() “z-transformed correlation” options removed simplify output. function supporting functions deprecated soon new version twang released. using method = \"ps\" method = \"super\" continuous treatments, setting use.kernel = TRUE plot = TRUE, plot now made ggplot2 rather base R plots. Added plot.summary.weightit() plot distribution weights (feature also optweight). Removed dependency cobalt temporarily, means examples vignette won’t run. Added ggplot2 Imports.","code":""},{"path":"https://ngreifer.github.io/WeightIt/news/index.html","id":"weightit-051","dir":"Changelog","previous_headings":"","what":"WeightIt 0.5.1","title":"WeightIt 0.5.1","text":"CRAN release: 2019-01-16 Fixed bug using ps argument weightit(). Fixed bug setting include.obj = TRUE weightitMSM(). Added warnings using certain methods longitudinal treatments validated may lead incorrect inferences.","code":""},{"path":"https://ngreifer.github.io/WeightIt/news/index.html","id":"weightit-050","dir":"Changelog","previous_headings":"","what":"WeightIt 0.5.0","title":"WeightIt 0.5.0","text":"CRAN release: 2018-11-22 Added super method estimate propensity scores using SuperLearner package. Added optweight method estimate weights using optimization (probably just use optweight package). weightit() now uses correct formula estimate weights ATO multinomial treatments described Li & Li (2018). Added include.obj option weightit() weightitMSM() include fitted object output object inspection. example, method = \"ps\", glm object containing propensity score model included output. Rearranged help pages. method now documentation page, linked weightit help page. Propensity scores now included output binary treatments gbm cbps methods. Thanks @Blanch-Font suggestion. bug fixes minor changes.","code":""},{"path":"https://ngreifer.github.io/WeightIt/news/index.html","id":"weightit-040","dir":"Changelog","previous_headings":"","what":"WeightIt 0.4.0","title":"WeightIt 0.4.0","text":"CRAN release: 2018-06-25 Added trim() function trim weights. Added ps.cont() function, estimates generalized propensity score weights continuous treatments using generalized boosted modeling, twang. function uses syntax ps() twang, can also accessed using weightit() method = \"gbm\". Support functions added make compatible twang functions assessing balance (e.g., summary, bal.table, plot). Thanks Donna Coffman enlightening method providing code implement . input formula now much forgiving, allowing objects environment included. data argument weightit() now optional. simplify things, output object longer contains data field. --hood changes facilitate adding new features debugging. aspects output objects slightly changed, shouldn’t affect use users. Fixed bug variables thrown method = \"ebal\".","code":""},{"path":"https://ngreifer.github.io/WeightIt/news/index.html","id":"weightit-032","dir":"Changelog","previous_headings":"","what":"WeightIt 0.3.2","title":"WeightIt 0.3.2","text":"CRAN release: 2018-03-14 Added new moments int options weightit() methods easily specify moments interactions covariates. Fixed bug using objects data set weightit(). Behavior changed include transformed covariates entered formula weightit() output. Fixed bug resulting potentially colinearity using ebal ebcw. Added vignette.","code":""},{"path":"https://ngreifer.github.io/WeightIt/news/index.html","id":"weightit-031","dir":"Changelog","previous_headings":"","what":"WeightIt 0.3.1","title":"WeightIt 0.3.1","text":"CRAN release: 2018-03-03 Edits code help files protect missing CBPS package. Corrected sampling weights functionality work correctly. Also expanded sampling weights able used methods, including natively allow sampling weights (e.g., ATE). Minor bug fixes spelling corrections.","code":""},{"path":"https://ngreifer.github.io/WeightIt/news/index.html","id":"weightit-030","dir":"Changelog","previous_headings":"","what":"WeightIt 0.3.0","title":"WeightIt 0.3.0","text":"CRAN release: 2018-01-14 Added weightitMSM() function (supporting print() summary() functions) estimate weights marginal structural models time-varying treatments covariates. Fixed bugs, including using CBPS continuous treatments, using focal incorrectly.","code":""},{"path":"https://ngreifer.github.io/WeightIt/news/index.html","id":"weightit-020","dir":"Changelog","previous_headings":"","what":"WeightIt 0.2.0","title":"WeightIt 0.2.0","text":"CRAN release: 2017-11-12 Added method = \"sbw\" stable balancing weights (now removed replaced method = \"optweight\") Allowed estimation multinomial propensity scores using multiple binary regressions mlogit installed Allowed estimation multinomial CBPS using multiple binary CBPS 4 groups Added README NEWS","code":""},{"path":"https://ngreifer.github.io/WeightIt/news/index.html","id":"weightit-010","dir":"Changelog","previous_headings":"","what":"WeightIt 0.1.0","title":"WeightIt 0.1.0","text":"CRAN release: 2017-10-17 First version!","code":""}]

[{"path":"https://ngreifer.github.io/WeightIt/articles/WeightIt.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Using WeightIt to Estimate Balancing Weights","text":"WeightIt contains several functions estimating assessing balancing weights observational studies. weights can used estimate causal parameters marginal structural models. go basics causal inference methods . good introductory articles, see Austin (2011), Austin Stuart (2015), Robins, Hernán, Brumback (2000), Thoemmes Ong (2016). Typically, analysis observation study might proceed follows: identify covariates balance required; assess quality data available, including missingness measurement error; estimate weights balance covariates adequately; estimate treatment effect corresponding standard error confidence interval. guide go steps two observational studies: estimating causal effect point treatment outcome, estimating causal parameters marginal structural model multiple treatment periods. meant definitive guide, rather introduction relevant issues.","code":""},{"path":"https://ngreifer.github.io/WeightIt/articles/WeightIt.html","id":"estimating-the-effect-of-a-point-treatment","dir":"Articles","previous_headings":"","what":"Estimating the Effect of a Point Treatment","title":"Using WeightIt to Estimate Balancing Weights","text":"First use Lalonde dataset estimate effect point treatment. ’ll use version data set resides within cobalt package, use later well. , interested average treatment effect treated (ATT). outcome (re78), treatment (treat), covariates balance desired (age, educ, race, married, nodegree, re74, re75). Using cobalt, can examine initial imbalance covariates: Based output, can see variables imbalanced sense standardized mean differences (continuous variables) differences proportion (binary variables) greater .05 variables. particular, re74 re75 quite imbalanced, troubling given likely strong predictors outcome. estimate weights using weightit() try attain balance covariates. First, ’ll start simple, use inverse probability weights propensity scores generated logistic regression. need supply weightit() formula model, data set, estimand (ATT), method estimation (\"ps\") propensity score weights). Printing output weightit() displays summary weights estimated. Let’s examine quality weights using summary(). Weights low variability desirable improve precision estimator. variability presented several ways: ratio largest weight smallest group, coefficient variation (standard deviation divided mean) weights group, effective sample size computed weights. want small ratio, smaller coefficient variation, large effective sample size (ESS). constitutes values mostly relative, though, must balanced constraints, including covariate balance. metrics best used comparing weighting methods, ESS can give sense much information remains weighted sample familiar scale. weights quite high variability, yield ESS close 100 control group. Let’s see weights managed yield balance covariates. nearly covariates, weights yielded good balance. age remained imbalanced, standardized mean difference greater .05 variance ratio greater 2. Let’s see can better. ’ll choose different method: entropy balancing (Hainmueller 2012), guarantees perfect balance specified moments covariates minimizing entropy (measure dispersion) weights. variability weights changed much, let’s see gains terms balance: Indeed, achieved perfect balance means covariates. However, variance ratio age still quite high. continue try adjust imbalance, reason believe unlikely affect outcome, may best leave . (can try adding (age^2) formula see changes causes.) Now weights stored W., let’s extract estimate treatment effect. Now let’s inference. Although authors recommend using “robust” sandwich standard errors adjust weights (Robins, Hernán, Brumback 2000; Hainmueller 2012), others believe can misleading recommend bootstrapping instead (Reifeis Hudgens 2020; Chan, Yam, Zhang 2016). ’ll examine approaches. svyglm() survey package produces robust standard errors, can use summary() view standard error effect estimate. confidence interval treat contains 0, isn’t evidence treat effect re78. Next let’s use bootstrapping estimate confidence intervals. don’t need use svyglm() can simply use glm() (lm()) compute effect estimates bootstrapped sample computing standard errors, treatment effect estimates . case, confidence intervals similar. Bootstrapping can take time, especially weight estimation methods take longer, SuperLearner (method = \"super\"), covariate balancing propensity score estimation (method = \"cbps\"), generalized boosted modeling (method = \"gbm\"). wanted produce “doubly-robust” treatment effect estimate, add baseline covariates glm() (svyglm()) model (original effect estimation confidence interval estimation).","code":"data(\"lalonde\", package = \"cobalt\") head(lalonde) ##   treat age educ   race married nodegree re74 re75       re78 ## 1     1  37   11  black       1        1    0    0  9930.0460 ## 2     1  22    9 hispan       0        1    0    0  3595.8940 ## 3     1  30   12  black       0        0    0    0 24909.4500 ## 4     1  27   11  black       0        1    0    0  7506.1460 ## 5     1  33    8  black       0        1    0    0   289.7899 ## 6     1  22    9  black       0        1    0    0  4056.4940 library(\"cobalt\") ##  cobalt (Version 4.4.0, Build Date: 2022-08-13) bal.tab(treat ~ age + educ + race + married + nodegree + re74 + re75,         data = lalonde, estimand = \"ATT\", thresholds = c(m = .05)) ## Balance Measures ##                Type Diff.Un      M.Threshold.Un ## age         Contin. -0.3094 Not Balanced, >0.05 ## educ        Contin.  0.0550 Not Balanced, >0.05 ## race_black   Binary  0.6404 Not Balanced, >0.05 ## race_hispan  Binary -0.0827 Not Balanced, >0.05 ## race_white   Binary -0.5577 Not Balanced, >0.05 ## married      Binary -0.3236 Not Balanced, >0.05 ## nodegree     Binary  0.1114 Not Balanced, >0.05 ## re74        Contin. -0.7211 Not Balanced, >0.05 ## re75        Contin. -0.2903 Not Balanced, >0.05 ##  ## Balance tally for mean differences ##                     count ## Balanced, <0.05         0 ## Not Balanced, >0.05     9 ##  ## Variable with the greatest mean difference ##  Variable Diff.Un      M.Threshold.Un ##      re74 -0.7211 Not Balanced, >0.05 ##  ## Sample sizes ##     Control Treated ## All     429     185 library(\"WeightIt\") W.out <- weightit(treat ~ age + educ + race + married + nodegree + re74 + re75,                   data = lalonde, estimand = \"ATT\", method = \"ps\") W.out #print the output ## A weightit object ##  - method: \"ps\" (propensity score weighting) ##  - number of obs.: 614 ##  - sampling weights: none ##  - treatment: 2-category ##  - estimand: ATT (focal: 1) ##  - covariates: age, educ, race, married, nodegree, re74, re75 summary(W.out) ##                  Summary of weights ##  ## - Weight ranges: ##  ##            Min                                  Max ## treated 1.0000         ||                    1.0000 ## control 0.0092 |---------------------------| 3.7432 ##  ## - Units with the 5 most extreme weights by group: ##                                             ##               5      4      3      2      1 ##  treated      1      1      1      1      1 ##             597    573    381    411    303 ##  control 3.0301 3.0592 3.2397 3.5231 3.7432 ##  ## - Weight statistics: ##  ##         Coef of Var   MAD Entropy # Zeros ## treated       0.000 0.000  -0.000       0 ## control       1.818 1.289   1.098       0 ##  ## - Effective Sample Sizes: ##  ##            Control Treated ## Unweighted  429.       185 ## Weighted     99.82     185 bal.tab(W.out, stats = c(\"m\", \"v\"), thresholds = c(m = .05)) ## Call ##  weightit(formula = treat ~ age + educ + race + married + nodegree +  ##     re74 + re75, data = lalonde, method = \"ps\", estimand = \"ATT\") ##  ## Balance Measures ##                 Type Diff.Adj         M.Threshold V.Ratio.Adj ## prop.score  Distance  -0.0205     Balanced, <0.05      1.0324 ## age          Contin.   0.1188 Not Balanced, >0.05      0.4578 ## educ         Contin.  -0.0284     Balanced, <0.05      0.6636 ## race_black    Binary  -0.0022     Balanced, <0.05           . ## race_hispan   Binary   0.0002     Balanced, <0.05           . ## race_white    Binary   0.0021     Balanced, <0.05           . ## married       Binary   0.0186     Balanced, <0.05           . ## nodegree      Binary   0.0184     Balanced, <0.05           . ## re74         Contin.  -0.0021     Balanced, <0.05      1.3206 ## re75         Contin.   0.0110     Balanced, <0.05      1.3938 ##  ## Balance tally for mean differences ##                     count ## Balanced, <0.05         9 ## Not Balanced, >0.05     1 ##  ## Variable with the greatest mean difference ##  Variable Diff.Adj         M.Threshold ##       age   0.1188 Not Balanced, >0.05 ##  ## Effective sample sizes ##            Control Treated ## Unadjusted  429.       185 ## Adjusted     99.82     185 W.out <- weightit(treat ~ age + educ + race + married + nodegree + re74 + re75,                   data = lalonde, estimand = \"ATT\", method = \"ebal\") summary(W.out) ##                  Summary of weights ##  ## - Weight ranges: ##  ##            Min                                  Max ## treated 1.0000    ||                         1.0000 ## control 0.0188 |---------------------------| 9.4195 ##  ## - Units with the 5 most extreme weights by group: ##                                             ##               5      4      3      2      1 ##  treated      1      1      1      1      1 ##             608    381    597    303    411 ##  control 7.1268 7.5013 7.9979 9.0355 9.4195 ##  ## - Weight statistics: ##  ##         Coef of Var   MAD Entropy # Zeros ## treated       0.000 0.000   0.000       0 ## control       1.834 1.287   1.101       0 ##  ## - Effective Sample Sizes: ##  ##            Control Treated ## Unweighted  429.       185 ## Weighted     98.46     185 bal.tab(W.out, stats = c(\"m\", \"v\"), thresholds = c(m = .05)) ## Call ##  weightit(formula = treat ~ age + educ + race + married + nodegree +  ##     re74 + re75, data = lalonde, method = \"ebal\", estimand = \"ATT\") ##  ## Balance Measures ##                Type Diff.Adj     M.Threshold V.Ratio.Adj ## age         Contin.        0 Balanced, <0.05      0.4097 ## educ        Contin.        0 Balanced, <0.05      0.6636 ## race_black   Binary        0 Balanced, <0.05           . ## race_hispan  Binary       -0 Balanced, <0.05           . ## race_white   Binary       -0 Balanced, <0.05           . ## married      Binary       -0 Balanced, <0.05           . ## nodegree     Binary       -0 Balanced, <0.05           . ## re74        Contin.       -0 Balanced, <0.05      1.3264 ## re75        Contin.       -0 Balanced, <0.05      1.3350 ##  ## Balance tally for mean differences ##                     count ## Balanced, <0.05         9 ## Not Balanced, >0.05     0 ##  ## Variable with the greatest mean difference ##  Variable Diff.Adj     M.Threshold ##   married       -0 Balanced, <0.05 ##  ## Effective sample sizes ##            Control Treated ## Unadjusted  429.       185 ## Adjusted     98.46     185 library(survey) d.w <- svydesign(~1, weights = W.out$weights, data = lalonde) fit <- svyglm(re78 ~ treat, design = d.w) coef(fit) ## (Intercept)       treat  ##    5075.929    1273.215 #Robust standard errors and confidence intervals summary(fit) ##  ## Call: ## svyglm(formula = re78 ~ treat, design = d.w) ##  ## Survey design: ## svydesign(~1, weights = W.out$weights, data = lalonde) ##  ## Coefficients: ##             Estimate Std. Error t value Pr(>|t|)     ## (Intercept)   5075.9      589.4   8.612   <2e-16 *** ## treat         1273.2      825.1   1.543    0.123     ## --- ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ##  ## (Dispersion parameter for gaussian family taken to be 45038738) ##  ## Number of Fisher Scoring iterations: 2 confint(fit) ##                2.5 %   97.5 % ## (Intercept) 3918.409 6233.449 ## treat       -347.069 2893.498 #Bootstrapping library(\"boot\") est.fun <- function(data, index) {   W.out <- weightit(treat ~ age + educ + race + married + nodegree + re74 + re75,                     data = data[index,], estimand = \"ATT\", method = \"ebal\")   fit <- glm(re78 ~ treat, data = data[index,], weights = W.out$weights)   return(coef(fit)[\"treat\"]) } boot.out <- boot(est.fun, data = lalonde, R = 999) boot.ci(boot.out, type = \"bca\") #type shouldn't matter so much ## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS ## Based on 999 bootstrap replicates ##  ## CALL :  ## boot.ci(boot.out = boot.out, type = \"bca\") ##  ## Intervals :  ## Level       BCa           ## 95%   (-421, 2886 )   ## Calculations and Intervals on Original Scale"},{"path":"https://ngreifer.github.io/WeightIt/articles/WeightIt.html","id":"estimating-the-effect-of-a-longitudinal-treatment","dir":"Articles","previous_headings":"","what":"Estimating the Effect of a Longitudinal Treatment","title":"Using WeightIt to Estimate Balancing Weights","text":"WeightIt can estimate weights longitudinal treatment marginal structural models well. time, ’ll use sample data set twang estimate weights. Data must “wide” format; go long wide, see example ?weightitMSM. outcome variable (outcome), time-stable baseline variables (gender age), pre-treatment time-varying variables (use0, measured first treatment, use1, use2), three time-varying treatment variables (tx1, tx2, tx3). interested joint, unique, causal effects treatment period outcome. treatment time point, need achieve balance variables measured prior treatment, including previous treatments. Using cobalt, can examine initial imbalance time point overall: bal.tab() indicates significant imbalance covariates time points, need work eliminate imbalance weighted data set. ’ll use weightitMSM() function specify weight models. syntax similar weightit() point treatments bal.tab() longitudinal treatments. ’ll use method = \"ps\" stabilize = TRUE stabilized propensity score weights estimated using logistic regression. matter method selected, weightitMSM() estimates separate weights time period takes product weights individual arrive final estimated weights. Printing output weightitMSM() provides details function call output. can take look quality weights summary(), just point treatments. Displayed summaries weights perform time point respect variability. Next, ’ll examine well perform respect covariate balance. setting .time = .none bal.tab(), can focus overall balance assessment, displays greatest imbalance covariate across time points. can see estimated weights balance covariates time points respect means variances. Now can estimate treatment effects. ’ll sequentially simplify model checking whether interaction terms needed (implying specific patterns treatment yield different outcomes), checking whether different coefficients needed treatments (implying outcomes depend treatments received). Based non-significant p-value, don’t assume specific treatment patterns yield different outcomes, rather treatments received number treatments received sufficient explain variation outcome. Next ’ll narrow options comparing main effects fit one constrains coefficients equal (implying cumulative number treatments received matters), Robins, Hernán, Brumback (2000) describe. Based non-significant p-value, can assume effects treatment close enough treated , indicating number treatments received relevant predictor outcome. Now can examine treatment effect summary(). additional treatment received, outcome expected decrease 0.15 points. confidence interval excludes 0, evidence treatment effect population. can , well. fit different types models estimate weights, stabilized weights stabilize = TRUE including stabilization factors weights using num.formula (see Cole Hernán (2008) details ). ways computing confidence intervals effect estimates (although model comparison straightforward method used).","code":"data(\"iptwExWide\", package = \"twang\") head(iptwExWide) ##      outcome gender age        use0         use1       use2 tx1 tx2 tx3 ## 1 -0.2782802      0  43  1.13496509  0.467482544  0.3174825   1   1   1 ## 2  0.5319329      0  50  1.11193185  0.455965923  0.4059659   1   0   1 ## 3 -0.8173614      1  36 -0.87077763 -0.535388817 -0.5853888   1   0   0 ## 4 -0.1530853      1  63  0.21073159  0.005365793 -0.1446342   1   1   1 ## 5 -0.7344267      0  24  0.06939565 -0.065302176 -0.1153022   1   0   1 ## 6 -0.8519376      1  20 -1.66264885 -0.931324426 -1.0813244   1   1   1 library(\"cobalt\") #if not already attached bal.tab(list(tx1 ~ age + gender + use0,              tx2 ~ tx1 + use1 + age + gender + use0,              tx3 ~ tx2 + use2 + tx1 + use1 + age + gender + use0),         data = iptwExWide, stats = c(\"m\", \"ks\"), thresholds = c(m = .05),         which.time = .all) ## Balance by Time Point ##  ##  - - - Time: 1 - - -  ## Balance Measures ##           Type Diff.Un      M.Threshold.Un  KS.Un ## age    Contin.  0.3799 Not Balanced, >0.05 0.2099 ## gender  Binary  0.2945 Not Balanced, >0.05 0.2945 ## use0   Contin.  0.2668 Not Balanced, >0.05 0.1681 ##  ## Balance tally for mean differences ##                     count ## Balanced, <0.05         0 ## Not Balanced, >0.05     3 ##  ## Variable with the greatest mean difference ##  Variable Diff.Un      M.Threshold.Un ##       age  0.3799 Not Balanced, >0.05 ##  ## Sample sizes ##     Control Treated ## All     294     706 ##  ##  - - - Time: 2 - - -  ## Balance Measures ##           Type Diff.Un      M.Threshold.Un  KS.Un ## tx1     Binary  0.1695 Not Balanced, >0.05 0.1695 ## use1   Contin.  0.0848 Not Balanced, >0.05 0.0763 ## age    Contin.  0.2240 Not Balanced, >0.05 0.1331 ## gender  Binary  0.1927 Not Balanced, >0.05 0.1927 ## use0   Contin.  0.1169 Not Balanced, >0.05 0.0913 ##  ## Balance tally for mean differences ##                     count ## Balanced, <0.05         0 ## Not Balanced, >0.05     5 ##  ## Variable with the greatest mean difference ##  Variable Diff.Un      M.Threshold.Un ##       age   0.224 Not Balanced, >0.05 ##  ## Sample sizes ##     Control Treated ## All     492     508 ##  ##  - - - Time: 3 - - -  ## Balance Measures ##           Type Diff.Un      M.Threshold.Un  KS.Un ## tx2     Binary  0.2423 Not Balanced, >0.05 0.2423 ## use2   Contin.  0.1087 Not Balanced, >0.05 0.1161 ## tx1     Binary  0.1071 Not Balanced, >0.05 0.1071 ## use1   Contin.  0.1662 Not Balanced, >0.05 0.1397 ## age    Contin.  0.3431 Not Balanced, >0.05 0.1863 ## gender  Binary  0.1532 Not Balanced, >0.05 0.1532 ## use0   Contin.  0.1859 Not Balanced, >0.05 0.1350 ##  ## Balance tally for mean differences ##                     count ## Balanced, <0.05         0 ## Not Balanced, >0.05     7 ##  ## Variable with the greatest mean difference ##  Variable Diff.Un      M.Threshold.Un ##       age  0.3431 Not Balanced, >0.05 ##  ## Sample sizes ##     Control Treated ## All     415     585 ##  - - - - - - - - - - - Wmsm.out <- weightitMSM(list(tx1 ~ age + gender + use0,                              tx2 ~ tx1 + use1 + age + gender + use0,                              tx3 ~ tx2 + use2 + tx1 + use1 + age + gender + use0),                         data = iptwExWide, method = \"ps\",                         stabilize = TRUE) Wmsm.out ## A weightitMSM object ##  - method: \"ps\" (propensity score weighting) ##  - number of obs.: 1000 ##  - sampling weights: none ##  - number of time points: 3 (tx1, tx2, tx3) ##  - treatment:  ##     + time 1: 2-category ##     + time 2: 2-category ##     + time 3: 2-category ##  - covariates:  ##     + baseline: age, gender, use0 ##     + after time 1: tx1, use1, age, gender, use0 ##     + after time 2: tx2, use2, tx1, use1, age, gender, use0 ##  - stabilized; stabilization factors: ##     + baseline: (none) ##     + after time 1: tx1 ##     + after time 2: tx1, tx2, tx1:tx2 summary(Wmsm.out) ##                  Summary of weights ##  ##                        Time 1                        ##                  Summary of weights ##  ## - Weight ranges: ##  ##            Min                                  Max ## treated 0.4767  |---------|                  3.8963 ## control 0.2900 |---------------------------| 8.8680 ##  ## - Units with the 5 most extreme weights by group: ##                                             ##             348    951    715    657    442 ##  treated 2.8052 2.9868 3.1041 3.3316 3.8963 ##             206    518     95    282    547 ##  control  3.405 3.6434  4.687  5.121  8.868 ##  ## - Weight statistics: ##  ##         Coef of Var   MAD Entropy # Zeros ## treated       0.420 0.281   0.073       0 ## control       0.775 0.423   0.183       0 ##  ## - Mean of Weights = 1 ##  ## - Effective Sample Sizes: ##  ##            Control Treated ## Unweighted  294.    706.   ## Weighted    183.85  600.26 ##  ##                        Time 2                        ##                  Summary of weights ##  ## - Weight ranges: ##  ##            Min                                  Max ## treated 0.3911 |----------|                  3.8963 ## control 0.2900 |---------------------------| 8.8680 ##  ## - Units with the 5 most extreme weights by group: ##                                             ##             951    980    715    657    442 ##  treated 2.9868 3.0427 3.1041 3.3316 3.8963 ##             206    518     95    282    547 ##  control  3.405 3.6434  4.687  5.121  8.868 ##  ## - Weight statistics: ##  ##         Coef of Var   MAD Entropy # Zeros ## treated       0.482 0.333   0.096       0 ## control       0.598 0.309   0.113       0 ##  ## - Mean of Weights = 1 ##  ## - Effective Sample Sizes: ##  ##            Control Treated ## Unweighted  492.    508.   ## Weighted    362.67  412.39 ##  ##                        Time 3                        ##                  Summary of weights ##  ## - Weight ranges: ##  ##            Min                                  Max ## treated 0.4767  |---------|                  3.8963 ## control 0.2900 |---------------------------| 8.8680 ##  ## - Units with the 5 most extreme weights by group: ##                                            ##             109    715    657   206    442 ##  treated 3.0238 3.1041 3.3316 3.405 3.8963 ##             980    518     95   282    547 ##  control 3.0427 3.6434  4.687 5.121  8.868 ##  ## - Weight statistics: ##  ##         Coef of Var   MAD Entropy # Zeros ## treated       0.488 0.337   0.097       0 ## control       0.609 0.300   0.115       0 ##  ## - Mean of Weights = 1 ##  ## - Effective Sample Sizes: ##  ##            Control Treated ## Unweighted   415.   585.   ## Weighted     302.9  472.55 bal.tab(Wmsm.out, stats = c(\"m\", \"ks\"), thresholds = c(m = .05),         which.time = .none) ## Call ##  weightitMSM(formula.list = list(tx1 ~ age + gender + use0, tx2 ~  ##     tx1 + use1 + age + gender + use0, tx3 ~ tx2 + use2 + tx1 +  ##     use1 + age + gender + use0), data = iptwExWide, method = \"ps\",  ##     stabilize = TRUE) ##  ## Balance summary across all time points ##              Times     Type Max.Diff.Adj         M.Threshold Max.KS.Adj ## prop.score 1, 2, 3 Distance       0.3217                         0.1748 ## age        1, 2, 3  Contin.       0.0153     Balanced, <0.05     0.0850 ## gender     1, 2, 3   Binary       0.0214     Balanced, <0.05     0.0214 ## use0       1, 2, 3  Contin.       0.0549 Not Balanced, >0.05     0.0952 ## tx1           2, 3   Binary       0.1544 Not Balanced, >0.05     0.1544 ## use1          2, 3  Contin.       0.0495     Balanced, <0.05     0.0547 ## tx2              3   Binary       0.2396 Not Balanced, >0.05     0.2396 ## use2             3  Contin.       0.1012 Not Balanced, >0.05     0.0697 ##  ## Balance tally for mean differences ##                     count ## Balanced, <0.05         3 ## Not Balanced, >0.05     4 ##  ## Variable with the greatest mean difference ##  Variable Max.Diff.Adj         M.Threshold ##       tx2       0.2396 Not Balanced, >0.05 ##  ## Effective sample sizes ##  - Time 1 ##            Control Treated ## Unadjusted  294.    706.   ## Adjusted    183.85  600.26 ##  - Time 2 ##            Control Treated ## Unadjusted  492.    508.   ## Adjusted    362.67  412.39 ##  - Time 3 ##            Control Treated ## Unadjusted   415.   585.   ## Adjusted     302.9  472.55 library(\"survey\") d.w.msm <- svydesign(~1, weights = Wmsm.out$weights,                      data = iptwExWide) full.fit <- svyglm(outcome ~ tx1*tx2*tx3, design = d.w.msm) main.effects.fit <- svyglm(outcome ~ tx1 + tx2 + tx3, design = d.w.msm) anova(full.fit, main.effects.fit) ## Working (Rao-Scott+F) LRT for tx1:tx2 tx1:tx3 tx2:tx3 tx1:tx2:tx3 ##  in svyglm(formula = outcome ~ tx1 * tx2 * tx3, design = d.w.msm) ## Working 2logLR =  7.689984 p= 0.10639  ## (scale factors:  1.3 1.1 0.96 0.65 );  denominator df= 992 cum.fit <- svyglm(outcome ~ I(tx1+tx2+tx3), design = d.w.msm) anova(main.effects.fit, cum.fit) ## Working (Rao-Scott+F) LRT for tx1 tx2 tx3 - I(tx1 + tx2 + tx3) ##  in svyglm(formula = outcome ~ tx1 + tx2 + tx3, design = d.w.msm) ## Working 2logLR =  1.840286 p= 0.40005  ## (scale factors:  1 0.96 );  denominator df= 996 anova(full.fit, cum.fit) ## Working (Rao-Scott+F) LRT for tx1 tx2 tx3 tx1:tx2 tx1:tx3 tx2:tx3 tx1:tx2:tx3 - I(tx1 + tx2 + tx3) ##  in svyglm(formula = outcome ~ tx1 * tx2 * tx3, design = d.w.msm) ## Working 2logLR =  9.504989 p= 0.15137  ## (scale factors:  1.3 1.2 1.1 1 0.73 0.58 );  denominator df= 992 summary(cum.fit) ##  ## Call: ## svyglm(formula = outcome ~ I(tx1 + tx2 + tx3), design = d.w.msm) ##  ## Survey design: ## svydesign(~1, weights = Wmsm.out$weights, data = iptwExWide) ##  ## Coefficients: ##                    Estimate Std. Error t value Pr(>|t|)     ## (Intercept)         0.13072    0.05412   2.416   0.0159 *   ## I(tx1 + tx2 + tx3) -0.15157    0.02734  -5.545 3.77e-08 *** ## --- ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ##  ## (Dispersion parameter for gaussian family taken to be 0.5307998) ##  ## Number of Fisher Scoring iterations: 2 confint(cum.fit) ##                          2.5 %      97.5 % ## (Intercept)         0.02452672  0.23691596 ## I(tx1 + tx2 + tx3) -0.20520895 -0.09792381"},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/articles/installing-packages.html","id":"propensity-score-weighting-using-glms-method-ps","dir":"Articles","previous_headings":"","what":"Propensity score weighting using GLMs (method = \"ps\")","title":"Installing Supporting Packages","text":"Several options available estimating propensity score weights using GLMs depending treatment type features desired model. binary treatments, weightit() uses stats::glm() default, continuous treatments, weightit() uses stats::lm() default, additional packages required. multi-category treatments use.mlogit = FALSE, weightit() uses stats::glm().","code":""},{"path":"https://ngreifer.github.io/WeightIt/articles/installing-packages.html","id":"binary-and-continuous-treatments-with-missing-saem","dir":"Articles","previous_headings":"Propensity score weighting using GLMs (method = \"ps\")","what":"Binary and continuous treatments with missing = \"saem\"","title":"Installing Supporting Packages","text":"binary continuous treatments, missing data present missing = \"saem\" supplied, misaem package required. install misaem CRAN, run misaem CRAN, want install development version source, can developer’s GitHub repo using following code:","code":"install.packages(\"misaem\") remotes::install_github(\"julierennes/misaem\")"},{"path":"https://ngreifer.github.io/WeightIt/articles/installing-packages.html","id":"binary-and-multicatgeory-treatments-with-link-br-logit","dir":"Articles","previous_headings":"Propensity score weighting using GLMs (method = \"ps\")","what":"Binary and multicatgeory treatments with link = \"br.logit\"","title":"Installing Supporting Packages","text":"binary multicategory treatments, link supplied \"br.logit\" another link beginning \"br.\", brglm2 package required. install brglm2 CRAN, run brglm2 CRAN, want install development version source, can developer, Ioannis Kosmidis’s, GitHub repo using following code: brglm2 requires compilation, means may need additional software installed computer install source.","code":"install.packages(\"brglm2\") remotes::install_github(\"ikosmidis/brglm2\")"},{"path":"https://ngreifer.github.io/WeightIt/articles/installing-packages.html","id":"multicategory-treatments-with-use-mlogit-true","dir":"Articles","previous_headings":"Propensity score weighting using GLMs (method = \"ps\")","what":"Multicategory treatments with use.mlogit = TRUE","title":"Installing Supporting Packages","text":"multicategory treatments, use.mlogit = TRUE (default), mlogit package required multinomial logistic regression. install mlogit CRAN, run mlogit CRAN, want install development version source, can developer, Yves Croissant’s, R-Forge repo using following code:","code":"install.packages(\"mlogit\") install.packages(\"mlogit\", repos = \"http://R-Forge.R-project.org\")"},{"path":"https://ngreifer.github.io/WeightIt/articles/installing-packages.html","id":"multicategory-treatments-with-use-mclogit-true","dir":"Articles","previous_headings":"Propensity score weighting using GLMs (method = \"ps\")","what":"Multicategory treatments with use.mclogit = TRUE","title":"Installing Supporting Packages","text":"multicategory treatments, use.mclogit = TRUE, mclogit package required multinomial logistic regression. install mclogit CRAN, run mclogit CRAN, want install development version source, can developer, Martin Elff’s, GitHub repo using following code:","code":"install.packages(\"mclogit\") remotes::install_github(\"melff/mclogit\")"},{"path":"https://ngreifer.github.io/WeightIt/articles/installing-packages.html","id":"multicategory-treatments-with-link-bayes-probit","dir":"Articles","previous_headings":"Propensity score weighting using GLMs (method = \"ps\")","what":"Multicategory treatments with link = \"bayes.probit\"","title":"Installing Supporting Packages","text":"multicategory treatments, link = \"bayes.probit\", MNP package required Bayesian multinomial probit regression. install MNP CRAN, run MNP CRAN, want install development version source, can developer, Kosuke Imai’s, GitHub repo using following code: MNP requires compilation, means may need additional software installed computer install source.","code":"install.packages(\"MNP\") remotes::install_github(\"kosukeimai/MNP\")"},{"path":"https://ngreifer.github.io/WeightIt/articles/installing-packages.html","id":"ordinal-multicategory-treatments","dir":"Articles","previous_headings":"Propensity score weighting using GLMs (method = \"ps\")","what":"Ordinal multicategory treatments","title":"Installing Supporting Packages","text":"multicategory ordinal treatments (.e., treatment variable ordered factor), MASS package required unless link = \"br.logit\" (case brglm2 package needed; see ). MASS core package can always installed CRAN using code :","code":"install.packages(\"MASS\")"},{"path":"https://ngreifer.github.io/WeightIt/articles/installing-packages.html","id":"propensity-score-weighting-using-gbm-method-gbm","dir":"Articles","previous_headings":"","what":"Propensity Score weighting using GBM (method = \"gbm\")","title":"Installing Supporting Packages","text":"WeightIt uses R package gbm estimate propensity score weights using GBM. rely twang package . install gbm CRAN, run gbm CRAN, want install development version source, can developer’s GitHub repo using following code: gbm requires compilation, means may need additional software installed computer install source.","code":"install.packages(\"gbm\") remotes::install_github(\"gbm-developers/gbm\")"},{"path":"https://ngreifer.github.io/WeightIt/articles/installing-packages.html","id":"covariate-balancing-propensity-score-weighting-method-cbps-and-method-npcbps","dir":"Articles","previous_headings":"","what":"Covariate Balancing Propensity Score weighting (method = \"cbps\" and method = \"npcbps\")","title":"Installing Supporting Packages","text":"WeightIt uses R package CBPS perform (nonparametric) covariate balancing propensity score weighting. install CBPS CRAN, run CBPS CRAN, want install development version source, can developer, Kosuke Imai’s, GitHub repo using following code:","code":"install.packages(\"CBPS\") remotes::install_github(\"kosukeimai/CBPS\")"},{"path":"https://ngreifer.github.io/WeightIt/articles/installing-packages.html","id":"entropy-balancing-method-ebal","dir":"Articles","previous_headings":"","what":"Entropy balancing (method = \"ebal\")","title":"Installing Supporting Packages","text":"WeightIt uses code written WeightIt, additional packages need installed use entropy balancing.","code":""},{"path":"https://ngreifer.github.io/WeightIt/articles/installing-packages.html","id":"empirical-balancing-calibration-weighting-method-ebcw","dir":"Articles","previous_headings":"","what":"Empirical balancing calibration weighting (method = \"ebcw\")","title":"Installing Supporting Packages","text":"Empirical balancing calibration weighting longer available WeightIt. Use entropy balancing, cases mathematically identical.","code":""},{"path":"https://ngreifer.github.io/WeightIt/articles/installing-packages.html","id":"optimization-based-weighting-method-optweight","dir":"Articles","previous_headings":"","what":"Optimization-based weighting (method = \"optweight\")","title":"Installing Supporting Packages","text":"WeightIt uses R package optweight perform optimization-based weighting. install optweight CRAN, run optweight CRAN, want install development version source, can developer, Noah Greifer’s (), GitHub repo using following code: optweight depends osqp package, requires compilation, means may need additional software installed computer install source.","code":"install.packages(\"optweight\") remotes::install_github(\"ngreifer/optweight\")"},{"path":"https://ngreifer.github.io/WeightIt/articles/installing-packages.html","id":"propensity-score-weighting-using-superlearner-method-super","dir":"Articles","previous_headings":"","what":"Propensity score weighting using SuperLearner (method = \"super\")","title":"Installing Supporting Packages","text":"WeightIt uses R package SuperLearner estimate propensity score weights using SuperLearner. install SuperLearner CRAN, run SuperLearner CRAN, want install development version source, can developer, Eric Polley’s, GitHub repo using following code: SuperLearner wrapper many packages. whole point using SuperLearner include many different machine learning algorithms combine well-fitting stacked model. algorithms exist many different R packages, need installed use . See Suggested packages SuperLearner CRAN page see packages might used SuperLearner. additional functions use SuperLearner SuperLearnerExtra repository. read R session used method = \"super\", use source() raw text file URL. example, read code SL.dbarts, run","code":"install.packages(\"SuperLearner\") remotes::install_github(\"ecpolley/SuperLearner\") source(\"https://raw.githubusercontent.com/ecpolley/SuperLearnerExtra/master/SL/SL.dbarts.R\")"},{"path":"https://ngreifer.github.io/WeightIt/articles/installing-packages.html","id":"propensity-score-weighting-using-bart-method-bart","dir":"Articles","previous_headings":"","what":"Propensity score weighting using BART (method = \"bart\")","title":"Installing Supporting Packages","text":"WeightIt uses R package dbarts estimate propensity score weights using BART. install dbarts CRAN, run dbarts CRAN, want install development version source, can developer, Vincent Dorie’s, GitHub repo using following code: dbarts requires compilation, means may need additional software installed computer install source.","code":"install.packages(\"dbarts\") remotes::install_github(\"vdorie/dbarts\")"},{"path":"https://ngreifer.github.io/WeightIt/articles/installing-packages.html","id":"energy-balancing-method-energy","dir":"Articles","previous_headings":"","what":"Energy Balancing (method = \"energy\")","title":"Installing Supporting Packages","text":"WeightIt uses R package osqp perform optimization required energy balancing. install osqp CRAN, run osqp CRAN, want install development version source, can developer’s site using instructions given , though bit involved installations source.","code":"install.packages(\"osqp\")"},{"path":"https://ngreifer.github.io/WeightIt/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Noah Greifer. Author, maintainer.","code":""},{"path":"https://ngreifer.github.io/WeightIt/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Greifer N (2022). WeightIt: Weighting Covariate Balance Observational Studies. https://ngreifer.github.io/WeightIt/, https://github.com/ngreifer/WeightIt.","code":"@Manual{,   title = {WeightIt: Weighting for Covariate Balance in Observational Studies},   author = {Noah Greifer},   year = {2022},   note = {https://ngreifer.github.io/WeightIt/, https://github.com/ngreifer/WeightIt}, }"},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/index.html","id":"overview","dir":"","previous_headings":"","what":"Overview","title":"Weighting for Covariate Balance in Observational Studies","text":"WeightIt one-stop package generate balancing weights point longitudinal treatments observational studies. Contained within WeightIt methods call R packages estimate weights. value WeightIt unified familiar syntax used generate weights, packages , often challenging navigate, syntax. WeightIt extends capabilities packages generate weights used estimate ATE, ATT, ATC, estimands binary multinomial treatments, treatment effects continuous treatments available. ways, WeightIt weighting MatchIt done matching, MatchIt users find syntax familiar. complete vignette, see website WeightIt vignette(\"WeightIt\"). install load WeightIt, use code : workhorse function WeightIt weightit(), generates weights given formula data input according methods parameters specified user. example use weightit() generate propensity score weights estimating ATE: Evaluating weights two components: evaluating covariate balance produced weights, evaluating whether weights allow sufficient precision eventual effect estimate. first goal, functions cobalt package, fully compatible WeightIt, can used, demonstrated : second goal, qualities distributions weights can assessed using summary(), demonstrated . Desirable qualities include small coefficients variation close 0 large effective sample sizes. table contains available methods WeightIt estimating weights binary, multinomial, continuous treatments using various methods functions various packages. See vignette(\"installing-packages\") information install packages. addition, WeightIt implements subgroup balancing propensity score using function sbps(). Several tools utilities available. Please submit bug reports issues https://github.com/ngreifer/WeightIt/issues. like see package method integrated WeightIt, questions comments WeightIt, please contact author. Fan mail greatly appreciated.","code":"#CRAN version install.packages(\"WeightIt\")  #Development version remotes::install_github(\"ngreifer/WeightIt\")  library(\"WeightIt\") data(\"lalonde\", package = \"cobalt\")  W <- weightit(treat ~ age + educ + nodegree +                  married + race + re74 + re75,                data = lalonde, method = \"ps\",                estimand = \"ATE\") W #> A weightit object #>  - method: \"ps\" (propensity score weighting) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 2-category #>  - estimand: ATE #>  - covariates: age, educ, nodegree, married, race, re74, re75 library(\"cobalt\")  bal.tab(W, un = TRUE) #> Balance Measures #>                 Type Diff.Un Diff.Adj #> prop.score  Distance  1.7569   0.1360 #> age          Contin. -0.2419  -0.1676 #> educ         Contin.  0.0448   0.1296 #> nodegree      Binary  0.1114  -0.0547 #> married       Binary -0.3236  -0.0944 #> race_black    Binary  0.6404   0.0499 #> race_hispan   Binary -0.0827   0.0047 #> race_white    Binary -0.5577  -0.0546 #> re74         Contin. -0.5958  -0.2740 #> re75         Contin. -0.2870  -0.1579 #>  #> Effective sample sizes #>            Control Treated #> Unadjusted  429.    185.   #> Adjusted    329.01   58.33 summary(W) #>                  Summary of weights #>  #> - Weight ranges: #>  #>            Min                                   Max #> treated 1.1721 |---------------------------| 40.0773 #> control 1.0092 |-|                            4.7432 #>  #> - Units with the 5 most extreme weights by group: #>                                                  #>               68     116      10     137     124 #>  treated 13.5451 15.9884 23.2967 23.3891 40.0773 #>              597     573     381     411     303 #>  control  4.0301  4.0592  4.2397  4.5231  4.7432 #>  #> - Weight statistics: #>  #>         Coef of Var   MAD Entropy # Zeros #> treated       1.478 0.807   0.534       0 #> control       0.552 0.391   0.118       0 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted  429.    185.   #> Weighted    329.01   58.33"},{"path":"https://ngreifer.github.io/WeightIt/reference/ESS.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute effective sample size of weighted sample — ESS","title":"Compute effective sample size of weighted sample — ESS","text":"Computes effective sample size (ESS) weighted sample, represents size unweighted sample approximately amount precision weighted sample consideration.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/ESS.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute effective sample size of weighted sample — ESS","text":"","code":"ESS(w)"},{"path":"https://ngreifer.github.io/WeightIt/reference/ESS.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute effective sample size of weighted sample — ESS","text":"w vector weights","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/ESS.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Compute effective sample size of weighted sample — ESS","text":"ESS calculated \\((\\sum w)^2/\\sum w^2\\).","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/ESS.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Compute effective sample size of weighted sample — ESS","text":"McCaffrey, D. F., Ridgeway, G., & Morral, . R. (2004). Propensity Score Estimation Boosted Regression Evaluating Causal Effects Observational Studies. Psychological Methods, 9(4), 403–425. doi:10.1037/1082-989X.9.4.403 Shook‐Sa, B. E., & Hudgens, M. G. (2020). Power sample size observational studies point exposure effects. Biometrics, biom.13405. doi:doi.org/10.1111/biom.13405","code":""},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/reference/ESS.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute effective sample size of weighted sample — ESS","text":"","code":"library(\"cobalt\") #>  cobalt (Version 4.4.0, Build Date: 2022-08-13) data(\"lalonde\", package = \"cobalt\")  #Balancing covariates between treatment groups (binary) (W1 <- weightit(treat ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"ps\", estimand = \"ATE\")) #> A weightit object #>  - method: \"ps\" (propensity score weighting) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 2-category #>  - estimand: ATE #>  - covariates: age, educ, married, nodegree, re74 summary(W1) #>                  Summary of weights #>  #> - Weight ranges: #>  #>            Min                                   Max #> treated 1.5560  |--------------------------| 73.3315 #> control 1.0222 ||                             3.0438 #>  #> - Units with the 5 most extreme weights by group: #>                                                  #>              124     184     172     181     182 #>  treated 11.2281 11.3437 12.0848 26.1775 73.3315 #>              411     595     269     409     296 #>  control  2.3303  2.4365  2.5005  2.6369  3.0438 #>  #> - Weight statistics: #>  #>         Coef of Var   MAD Entropy # Zeros #> treated       1.609 0.555   0.403       0 #> control       0.247 0.211   0.029       0 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted  429.    185.   #> Weighted    404.35   51.73 ESS(W1$weights[W1$treat == 0]) #> [1] 404.3484 ESS(W1$weights[W1$treat == 1]) #> [1] 51.73462"},{"path":"https://ngreifer.github.io/WeightIt/reference/as.weightit.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a weightit object manually — as.weightit","title":"Create a weightit object manually — as.weightit","text":"function allows users get benefits weightit object using weights estimated weightit() weightitMSM(). benefits include diagnostics, plots, direct compatibility cobalt assessing balance.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/as.weightit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a weightit object manually — as.weightit","text":"","code":"as.weightit(...)  # S3 method for default as.weightit(weights,             treat,             covs = NULL,             estimand = NULL,             s.weights = NULL,             ps = NULL,             ...)  as.weightitMSM(...)  # S3 method for default as.weightitMSM(weights,                treat.list,                covs.list = NULL,                estimand = NULL,                s.weights = NULL,                ps.list = NULL,                ...)"},{"path":"https://ngreifer.github.io/WeightIt/reference/as.weightit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a weightit object manually — as.weightit","text":"weights required; numeric vector weights, one unit. treat required; vector treatment statuses, one unit. covs optional data.frame covariates. using WeightIt functions, necessary, use cobalt . estimand optional character length 1 giving estimand. text checked. s.weights optional numeric vector sampling weights, one unit. ps optional numeric vector propensity scores, one unit. treat.list list treatment statuses time point.. covs.list optional list data.frames covariates covariates time point. using WeightIt functions, necessary, use cobalt . ps.list optional list numeric vectors propensity scores time point. ... additional arguments. must named. included output object.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/as.weightit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a weightit object manually — as.weightit","text":"object class weightit (.weightit()) weightitMSM (.weightitMSM()).","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/as.weightit.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Create a weightit object manually — as.weightit","text":"Noah Greifer","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/as.weightit.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a weightit object manually — as.weightit","text":"","code":"treat <- rbinom(500, 1, .3) weights <- rchisq(500, df = 2) W <- as.weightit(weights= weights, treat = treat,                  estimand = \"ATE\") summary(W) #>                  Summary of weights #>  #> - Weight ranges: #>  #>            Min                                   Max #> treated 0.0087 |---------------------------| 14.9355 #> control 0.0060 |-----------------|            9.8568 #>  #> - Units with the 5 most extreme weights by group: #>                                              #>              83    382    27     423      91 #>  treated 7.5027 8.2636  10.1 10.7077 14.9355 #>              57    475   457     375     275 #>  control  7.308 7.9554 8.582  8.5978  9.8568 #>  #> - Weight statistics: #>  #>         Coef of Var   MAD Entropy # Zeros #> treated       1.144 0.776   0.493       0 #> control       0.921 0.718   0.385       0 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted  358.    142.   #> Weighted    193.99   61.75"},{"path":"https://ngreifer.github.io/WeightIt/reference/get_w_from_ps.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute weights from propensity scores — get_w_from_ps","title":"Compute weights from propensity scores — get_w_from_ps","text":"Given vector matrix propensity scores, outputs vector weights target provided estimand.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/get_w_from_ps.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute weights from propensity scores — get_w_from_ps","text":"","code":"get_w_from_ps(ps,               treat,               estimand = \"ATE\",               focal = NULL,               treated = NULL,               subclass = NULL,               stabilize = FALSE)"},{"path":"https://ngreifer.github.io/WeightIt/reference/get_w_from_ps.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute weights from propensity scores — get_w_from_ps","text":"ps vector, matrix, data frame propensity scores. See Details. treat vector treatment status individual. See Details. estimand desired estimand weights target. Current options include \"ATE\" (average treatment effect), \"ATT\" (average treatment effect treated), \"ATC\" (average treatment effect control), \"ATO\" (average treatment effect overlap), \"ATM\" (average treatment effect matched sample), \"ATOS\" (average treatment effect optimal subset). focal estimand ATT ATC, group consider (focal) \"treated\" \"control\" group, respectively. NULL estimand \"ATT\" \"ATC\", estimand automatically set \"ATT\". treated treatment binary, value treat considered \"treated\" group (.e., group propensity scores probability ). NULL, get_w_from_ps() attempt figure using heuristics. really matters treat values 0 1 ps given vector unnamed single-column matrix data frame. subclass numeric; number subclasses use computing weights using marginal mean weighting stratification (also known fine stratification). NULL, standard inverse probability weights (extensions) computed; number greater 1, subclasses formed weights computed based subclass membership. estimand must ATE, ATT, ATC subclass non-NULL. See Details. stabilize logical; whether compute stabilized weights . simply involves multiplying unit's weight proportion units treatment group. saturated outcome models balance checking, make difference; otherwise, can improve performance.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/get_w_from_ps.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Compute weights from propensity scores — get_w_from_ps","text":"get_w_from_ps applies formula computing weights propensity scores desired estimand. See References section information estimands formulas. ps can entered variety ways. binary treatments, ps entered vector unnamed single-column matrix data frame, get_w_from_ps know value treat corresponds \"treated\" group. 0/1 variables, 1 considered treated. types variables, get_w_from_ps() try figure using heuristics, safer supply argument treated. estimand \"ATT\" \"ATC\", supplying value focal sufficient (ATT, focal treated group, ATC, focal control group). entered matrix data frame, columns must named levels treatment, assumed column corresponds probability treatment group. safest way supply ps unless treat 0/1 variable. multi-category treatments, ps can entered vector matrix data frame. entered vector, assumed value corresponds probability treatment actually received; possible estimand \"ATE\". Otherwise, ps must entered named matrix data frame described binary treatments. estimand \"ATT\" \"ATC\", value focal must specified. subclass NULL, marginal mean weighting stratification (MMWS) weights computed. implementation differs slightly described Hong (2010, 2012). First, subclasses formed finding quantiles propensity scores target group (ATE, units; ATT ATC, just units focal group). subclasses lacking members treatment group filled neighboring subclasses subclass always least one member treatment group. new subclass-propensity score matrix formed, unit's subclass-propensity score treatment value computed proportion units treatment value unit's subclass. example, subclass 10 treated units 90 control units , subclass-propensity score treated .1 subclass-propensity score control .9 units subclass. multi-category treatments, propensity scores treatment stratified separately described Hong (2012); binary treatments, one set propensity scores stratified subclass-propensity scores treatment computed complement propensity scores stratified treatment. subclass-propensity scores computed, standard propensity score weighting formulas used compute unstabilized MMWS weights. estimate MMWS weights equivalent described Hong (2010, 2012), stabilize must set TRUE, , standard propensity score weights, optional. Note MMWS weights also known fine stratification weights described Desai et al. (2017). get_w_from_ps() compatible continuous treatments.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/get_w_from_ps.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute weights from propensity scores — get_w_from_ps","text":"vector weights. subclass NULL, subclasses returned \"subclass\" attribute. estimand = \"ATOS\", chosen value alpha (smallest propensity score allowed remain sample) returned \"alpha\" attribute.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/get_w_from_ps.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Compute weights from propensity scores — get_w_from_ps","text":"Noah Greifer","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/get_w_from_ps.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Compute weights from propensity scores — get_w_from_ps","text":"Binary treatments - estimand = \"ATO\" Li, F., Morgan, K. L., & Zaslavsky, . M. (2018). Balancing covariates via propensity score weighting. Journal American Statistical Association, 113(521), 390–400. doi:10.1080/01621459.2016.1260466 - estimand = \"ATM\" Li, L., & Greene, T. (2013). Weighting Analogue Pair Matching Propensity Score Analysis. International Journal Biostatistics, 9(2). doi:10.1515/ijb-2012-0030 - estimand = \"ATOS\" Crump, R. K., Hotz, V. J., Imbens, G. W., & Mitnik, O. . (2009). Dealing limited overlap estimation average treatment effects. Biometrika, 96(1), 187–199. doi:10.1093/biomet/asn055 - estimands Austin, P. C. (2011). Introduction Propensity Score Methods Reducing Effects Confounding Observational Studies. Multivariate Behavioral Research, 46(3), 399–424. doi:10.1080/00273171.2011.568786 - Marginal mean weighting stratification (MMWS) Hong, G. (2010). Marginal mean weighting stratification: Adjustment selection bias multilevel data. Journal Educational Behavioral Statistics, 35(5), 499–531. doi:10.3102/1076998609359785 Desai, R. J., Rothman, K. J., Bateman, B. . T., Hernandez-Diaz, S., & Huybrechts, K. F. (2017). Propensity-score-based Fine Stratification Approach Confounding Adjustment Exposure Infrequent: Epidemiology, 28(2), 249–257. doi:10.1097/EDE.0000000000000595 Multinomial Treatments - estimand = \"ATO\" Li, F., & Li, F. (2019). Propensity score weighting causal inference multiple treatments. Annals Applied Statistics, 13(4), 2389–2415. doi:10.1214/19-AOAS1282 - estimand = \"ATM\" Yoshida, K., Hernández-Díaz, S., Solomon, D. H., Jackson, J. W., Gagne, J. J., Glynn, R. J., & Franklin, J. M. (2017). Matching weights simultaneously compare three treatment groups: Comparison three-way matching. Epidemiology (Cambridge, Mass.), 28(3), 387–395. doi:10.1097/EDE.0000000000000627 - estimands McCaffrey, D. F., Griffin, B. ., Almirall, D., Slaughter, M. E., Ramchand, R., & Burgette, L. F. (2013). Tutorial Propensity Score Estimation Multiple Treatments Using Generalized Boosted Models. Statistics Medicine, 32(19), 3388–3414. doi:10.1002/sim.5753 - Marginal mean weighting stratification Hong, G. (2012). Marginal mean weighting stratification: generalized method evaluating multivalued multiple treatments nonexperimental data. Psychological Methods, 17(1), 44–60. doi:10.1037/a0024918","code":""},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/reference/get_w_from_ps.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute weights from propensity scores — get_w_from_ps","text":"","code":"library(\"cobalt\") data(\"lalonde\", package = \"cobalt\")  ps.fit <- glm(treat ~ age + educ + race + married +                 nodegree + re74 + re75, data = lalonde,               family = binomial) ps <- ps.fit$fitted.values  w1 <- get_w_from_ps(ps, treat = lalonde$treat,                     estimand = \"ATT\")  treatAB <- factor(ifelse(lalonde$treat == 1, \"A\", \"B\")) w2 <- get_w_from_ps(ps, treat = treatAB,                     estimand = \"ATT\", focal = \"A\") all.equal(w1, w2) #> [1] TRUE w3 <- get_w_from_ps(ps, treat = treatAB,                     estimand = \"ATT\", treated = \"A\") all.equal(w1, w3) #> [1] TRUE  #Using MMWS w4 <- get_w_from_ps(ps, treat = lalonde$treat,                     estimand = \"ATE\", subclass = 20,                     stabilize = TRUE) #A multi-category example using GBM predicted probabilities library(gbm) #> Loaded gbm 2.1.8.1 T3 <- factor(sample(c(\"A\", \"B\", \"C\"), nrow(lalonde), replace = TRUE))  gbm.fit <- gbm(T3 ~ age + educ + race + married +                  nodegree + re74 + re75, data = lalonde,                distribution = \"multinomial\", n.trees = 200,                interaction.depth = 3) #> Warning: Setting `distribution = \"multinomial\"` is ill-advised as it is currently broken. It exists only for backwards compatibility. Use at your own risk. ps.multi <- drop(predict(gbm.fit, type = \"response\",                          n.trees = 200)) w <- get_w_from_ps(ps.multi, T3, estimand = \"ATE\")"},{"path":"https://ngreifer.github.io/WeightIt/reference/make_full_rank.html","id":null,"dir":"Reference","previous_headings":"","what":"Make a design matrix full rank — make_full_rank","title":"Make a design matrix full rank — make_full_rank","text":"writing user-defined methods use weightit(), may necessary take potentially non-full rank covs data frame make full rank use downstream function. function performs operation.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/make_full_rank.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Make a design matrix full rank — make_full_rank","text":"","code":"make_full_rank(mat,                with.intercept = TRUE)"},{"path":"https://ngreifer.github.io/WeightIt/reference/make_full_rank.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Make a design matrix full rank — make_full_rank","text":"mat numeric matrix data frame transformed. Typically contains covariates. NAs allowed. .intercept whether intercept (.e., vector 1s) added mat making full rank. TRUE, intercept used determining whether column linearly dependent others. Regardless, intercept included output.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/make_full_rank.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Make a design matrix full rank — make_full_rank","text":"make_full_rank() calls qr() find rank linearly independent columns mat, retained others dropped. .intercept set TRUE, intercept column added matrix calling qr(). Note dependent columns appear later mat dropped first. See example method_user.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/make_full_rank.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Make a design matrix full rank — make_full_rank","text":"Older versions drop columns one value. .intercept = FALSE, one column one value, removed, function though intercept present; column one value, first one remain.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/make_full_rank.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Make a design matrix full rank — make_full_rank","text":"object type mat containing linearly independent columns.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/make_full_rank.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Make a design matrix full rank — make_full_rank","text":"Noah Greifer","code":""},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/reference/make_full_rank.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Make a design matrix full rank — make_full_rank","text":"","code":"set.seed(1000) c1 <- rbinom(10, 1, .4) c2 <- 1-c1 c3 <- rnorm(10) c4 <- 10*c3 mat <- data.frame(c1, c2, c3, c4)  make_full_rank(mat) #leaves c2 and c4 #>    c1          c3 #> 1   0 -0.38548930 #> 2   1 -0.47586788 #> 3   0  0.71975069 #> 4   1 -0.01850562 #> 5   0 -1.37311776 #> 6   0 -0.98242783 #> 7   1 -0.55448870 #> 8   0  0.12138119 #> 9   0 -0.12087232 #> 10  0 -1.33604105  make_full_rank(mat, with.intercept = FALSE) #leaves c1, c2, and c4 #>    c1 c2          c3 #> 1   0  1 -0.38548930 #> 2   1  0 -0.47586788 #> 3   0  1  0.71975069 #> 4   1  0 -0.01850562 #> 5   0  1 -1.37311776 #> 6   0  1 -0.98242783 #> 7   1  0 -0.55448870 #> 8   0  1  0.12138119 #> 9   0  1 -0.12087232 #> 10  0  1 -1.33604105"},{"path":"https://ngreifer.github.io/WeightIt/reference/method_bart.html","id":null,"dir":"Reference","previous_headings":"","what":"Propensity Score Weighting Using BART — method_bart","title":"Propensity Score Weighting Using BART — method_bart","text":"page explains details estimating weights Bayesian additive regression trees (BART)-based propensity scores setting method = \"bart\" call weightit() weightitMSM(). method can used binary, multinomial, continuous treatments. general, method relies estimating propensity scores using BART converting propensity scores weights using formula depends desired estimand. method relies dbarts::bart2() dbarts package.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_bart.html","id":"binary-treatments","dir":"Reference","previous_headings":"","what":"Binary Treatments","title":"Propensity Score Weighting Using BART — method_bart","text":"binary treatments, method estimates propensity scores using dbarts::bart2(). following estimands allowed: ATE, ATT, ATC, ATO, ATM, ATOS. Weights can also computed using marginal mean weighting stratification ATE, ATT, ATC. See get_w_from_ps() details.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_bart.html","id":"multinomial-treatments","dir":"Reference","previous_headings":"","what":"Multinomial Treatments","title":"Propensity Score Weighting Using BART — method_bart","text":"multinomial treatments, propensity scores estimated using several calls dbarts::bart2(), one treatment group; treatment probabilities normalized sum 1. following estimands allowed: ATE, ATT, ATC, ATO, ATM. weights estimand computed using standard formulas mentioned . Weights can also computed using marginal mean weighting stratification ATE, ATT, ATC. See get_w_from_ps() details.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_bart.html","id":"continuous-treatments","dir":"Reference","previous_headings":"","what":"Continuous Treatments","title":"Propensity Score Weighting Using BART — method_bart","text":"continuous treatments, generalized propensity score estimated using dbarts::bart2(). addition, kernel density estimation can used instead assuming normal density numerator denominator generalized propensity score setting use.kernel = TRUE. arguments density() can specified refine density estimation parameters. plot = TRUE can specified plot density numerator denominator, can helpful diagnosing extreme weights.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_bart.html","id":"longitudinal-treatments","dir":"Reference","previous_headings":"","what":"Longitudinal Treatments","title":"Propensity Score Weighting Using BART — method_bart","text":"longitudinal treatments, weights product weights estimated time point.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_bart.html","id":"sampling-weights","dir":"Reference","previous_headings":"","what":"Sampling Weights","title":"Propensity Score Weighting Using BART — method_bart","text":"Sampling weights supported.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_bart.html","id":"missing-data","dir":"Reference","previous_headings":"","what":"Missing Data","title":"Propensity Score Weighting Using BART — method_bart","text":"presence missing data, following value(s) missing allowed: \"ind\" (default) First, variable missingness, new missingness indicator variable created takes value 1 original covariate NA 0 otherwise. missingness indicators added model formula main effects. missing values covariates replaced 0s. weight estimation proceeds new formula set covariates. covariates output resulting weightit object original covariates NAs.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_bart.html","id":"additional-arguments","dir":"Reference","previous_headings":"","what":"Additional Arguments","title":"Propensity Score Weighting Using BART — method_bart","text":"arguments dbarts::bart2() can passed weightit() weightitMSM(), following exceptions: test, weights, subset, offset.test ignored combine.chains always set TRUE sampleronly always set FALSE continuous treatments , following arguments may supplied: density function corresponding conditional density treatment. standardized residuals treatment model fed function produce numerator denominator generalized propensity score weights. blank, dnorm() used recommended Robins et al. (2000). can also supplied string containing name function called. string contains underscores, call split underscores latter splits supplied arguments second argument beyond. example, density = \"dt_2\" specified, density used t-distribution 2 degrees freedom. Using t-distribution can useful extreme outcome values observed (Naimi et al., 2014). Ignored use.kernel = TRUE (described ). use.kernel TRUE, uses kernel density estimation density() estimate numerator denominator densities weights. FALSE, argument density parameter used instead. bw, adjust, kernel, n use.kernel = TRUE, arguments density() function. defaults density except n 10 times number units sample. plot use.kernel = TRUE, whether plot estimated density.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_bart.html","id":"additional-outputs","dir":"Reference","previous_headings":"","what":"Additional Outputs","title":"Propensity Score Weighting Using BART — method_bart","text":"obj include.obj = TRUE, bart2 fit(s) used generate predicted values. multinomial treatments, list fits; otherwise, single fit. predicted probabilities used compute propensity scores can extracted using fitted().","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_bart.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Propensity Score Weighting Using BART — method_bart","text":"BART works fitting sum--trees model treatment probability treatment. number trees determined n.trees argument. Bayesian priors used hyperparameters, result posterior distribution predicted values unit. mean unit taken use computing (generalized) propensity score. Although hyperparameters governing priors can modified supplying arguments weightit() passed BART fitting function, default values tend work well require little modification (though defaults differ continuous categorical treatments; see dbarts::bart2() documentation details). Unlike many machine learning methods, loss function optimized hyperparameters need tuned (e.g., using cross-validation), though performance can benefit tuning. BART tends balance sparseness flexibility using weak learners trees, makes suitable capturing complex functions without specifying particular functional form without overfitting.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_bart.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Propensity Score Weighting Using BART — method_bart","text":"version 0.9-19 dbarts, special care taken ensure reproducibility using method = \"bart\". Setting seed (either set.seed() supplying argument rngSeed) work one thread requested. default use four threads. request one thread used, necessary reproducible results, set n.threads = 1 call weightit() set seed. Note fewer threads used, slower estimation . One can set n.chains lower number (default 4) speed estimation possible expense statistical performance. version 0.9-20 , setting seed set.seed() works correctly results reproducible.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_bart.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Propensity Score Weighting Using BART — method_bart","text":"Hill, J., Weiss, C., & Zhai, F. (2011). Challenges Propensity Score Strategies High-Dimensional Setting Potential Alternative. Multivariate Behavioral Research, 46(3), 477–513. doi:10.1080/00273171.2011.570161 Chipman, H. ., George, E. ., & McCulloch, R. E. (2010). BART: Bayesian additive regression trees. Annals Applied Statistics, 4(1), 266–298. doi:10.1214/09-AOAS285 Note many references deal BART causal inference focus estimating potential outcomes BART, propensity scores, directly relevant using BART estimate propensity scores weights. See method_ps additional references propensity score weighting generally.","code":""},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/reference/method_bart.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Propensity Score Weighting Using BART — method_bart","text":"","code":"library(\"cobalt\") data(\"lalonde\", package = \"cobalt\")  #Balancing covariates between treatment groups (binary) (W1 <- weightit(treat ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"bart\", estimand = \"ATT\")) #> A weightit object #>  - method: \"bart\" (propensity score weighting with BART) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 2-category #>  - estimand: ATT (focal: 1) #>  - covariates: age, educ, married, nodegree, re74 summary(W1) #>                  Summary of weights #>  #> - Weight ranges: #>  #>            Min                                  Max #> treated 1.0000    ||                         1.0000 #> control 0.0026 |---------------------------| 9.0564 #>  #> - Units with the 5 most extreme weights by group: #>                                            #>               6      5      3     2      1 #>  treated      1      1      1     1      1 #>             409    592    569   374    608 #>  control 2.2121 2.6902 2.7388 3.116 9.0564 #>  #> - Weight statistics: #>  #>         Coef of Var   MAD Entropy # Zeros #> treated       0.000 0.000  -0.000       0 #> control       1.736 0.924   0.707       0 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted  429.       185 #> Weighted    107.03     185 bal.tab(W1) #> Call #>  weightit(formula = treat ~ age + educ + married + nodegree +  #>     re74, data = lalonde, method = \"bart\", estimand = \"ATT\") #>  #> Balance Measures #>                Type Diff.Adj #> prop.score Distance   0.4884 #> age         Contin.   0.0662 #> educ        Contin.  -0.0272 #> married      Binary  -0.0311 #> nodegree     Binary   0.0326 #> re74        Contin.  -0.0631 #>  #> Effective sample sizes #>            Control Treated #> Unadjusted  429.       185 #> Adjusted    107.03     185 # \\donttest{ #Balancing covariates with respect to race (multinomial) (W2 <- weightit(race ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"bart\", estimand = \"ATE\")) #> A weightit object #>  - method: \"bart\" (propensity score weighting with BART) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 3-category (black, hispan, white) #>  - estimand: ATE #>  - covariates: age, educ, married, nodegree, re74 summary(W2) #>                  Summary of weights #>  #> - Weight ranges: #>  #>           Min                                   Max #> black  1.2387 |-----------------|            9.1226 #> hispan 2.8031     |-----------------------| 12.9804 #> white  1.0496 |-----------------|            9.1589 #>  #> - Units with the 5 most extreme weights by group: #>                                               #>             226    181    423     244     231 #>   black  7.1289 7.1295 8.0412  8.2077  9.1226 #>             346    426    512     570     564 #>  hispan 12.0497 12.198 12.401 12.4845 12.9804 #>              96     23     60      76     140 #>   white  4.4096 5.3803 5.7011  8.8263  9.1589 #>  #> - Weight statistics: #>  #>        Coef of Var   MAD Entropy # Zeros #> black        0.581 0.377   0.128       0 #> hispan       0.364 0.297   0.066       0 #> white        0.494 0.328   0.092       0 #>  #> - Effective Sample Sizes: #>  #>             black hispan  white #> Unweighted 243.    72.   299.   #> Weighted   181.85  63.68 240.53 bal.tab(W2) #> Call #>  weightit(formula = race ~ age + educ + married + nodegree + re74,  #>     data = lalonde, method = \"bart\", estimand = \"ATE\") #>  #> Balance summary across all treatment pairs #>             Type Max.Diff.Adj #> age      Contin.       0.1862 #> educ     Contin.       0.1751 #> married   Binary       0.0498 #> nodegree  Binary       0.0284 #> re74     Contin.       0.1108 #>  #> Effective sample sizes #>             black hispan  white #> Unadjusted 243.    72.   299.   #> Adjusted   181.85  63.68 240.53  #Balancing covariates with respect to re75 (continuous) #assuming t(3) conditional density for treatment (W3 <- weightit(re75 ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"bart\", density = \"dt_3\")) #> A weightit object #>  - method: \"bart\" (propensity score weighting with BART) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: continuous #>  - covariates: age, educ, married, nodegree, re74 summary(W3) #>                  Summary of weights #>  #> - Weight ranges: #>  #>        Min                                  Max #> all 0.0786 |---------------------------| 19.343 #>  #> - Units with the 5 most extreme weights: #>                                         #>         310    308   469     487    484 #>  all 7.7717 7.8331 8.773 17.1538 19.343 #>  #> - Weight statistics: #>  #>     Coef of Var   MAD Entropy # Zeros #> all       1.098 0.475    0.27       0 #>  #> - Effective Sample Sizes: #>  #>             Total #> Unweighted 614.   #> Weighted   278.51 bal.tab(W3) #> Call #>  weightit(formula = re75 ~ age + educ + married + nodegree + re74,  #>     data = lalonde, method = \"bart\", density = \"dt_3\") #>  #> Balance Measures #>             Type Corr.Adj #> age      Contin.   0.0291 #> educ     Contin.   0.0547 #> married   Binary   0.0830 #> nodegree  Binary  -0.0816 #> re74     Contin.   0.1189 #>  #> Effective sample sizes #>             Total #> Unadjusted 614.   #> Adjusted   278.51 # }"},{"path":"https://ngreifer.github.io/WeightIt/reference/method_cbps.html","id":null,"dir":"Reference","previous_headings":"","what":"Covariate Balancing Propensity Score Weighting — method_cbps","title":"Covariate Balancing Propensity Score Weighting — method_cbps","text":"page explains details estimating weights covariate balancing propensity scores setting method = \"cbps\" call weightit() weightitMSM(). method can used binary, multinomial, continuous treatments. general, method relies estimating propensity scores using generalized method moments converting propensity scores weights using formula depends desired estimand. method relies CBPS::CBPS() CBPS package.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_cbps.html","id":"binary-treatments","dir":"Reference","previous_headings":"","what":"Binary Treatments","title":"Covariate Balancing Propensity Score Weighting — method_cbps","text":"binary treatments, method estimates propensity scores weights using CBPS::CBPS(). following estimands allowed: ATE, ATT, ATC. weights taken output CBPS fit object. estimand ATE, return propensity score probability \"second\" treatment group, .e., levels(factor(treat))[2]; estimand ATC, returned propensity score probability control (.e., non-focal) group.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_cbps.html","id":"multinomial-treatments","dir":"Reference","previous_headings":"","what":"Multinomial Treatments","title":"Covariate Balancing Propensity Score Weighting — method_cbps","text":"multinomial treatments three four categories estimand ATE, method estimates propensity scores weights using one call CBPS::CBPS(). multinomial treatments three four categories estimand ATT, method estimates propensity scores weights using multiple calls CBPS::CBPS(). following estimands allowed: ATE ATT. weights taken output CBPS fit objects.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_cbps.html","id":"continuous-treatments","dir":"Reference","previous_headings":"","what":"Continuous Treatments","title":"Covariate Balancing Propensity Score Weighting — method_cbps","text":"continuous treatments, generalized propensity score weights estimated using CBPS::CBPS().","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_cbps.html","id":"longitudinal-treatments","dir":"Reference","previous_headings":"","what":"Longitudinal Treatments","title":"Covariate Balancing Propensity Score Weighting — method_cbps","text":"longitudinal treatments, weights product weights estimated time point. CBPS::CBMSM() CBPS package estimates weights longitudinal treatments.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_cbps.html","id":"sampling-weights","dir":"Reference","previous_headings":"","what":"Sampling Weights","title":"Covariate Balancing Propensity Score Weighting — method_cbps","text":"Sampling weights supported s.weights scenarios. See Note sampling weights.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_cbps.html","id":"missing-data","dir":"Reference","previous_headings":"","what":"Missing Data","title":"Covariate Balancing Propensity Score Weighting — method_cbps","text":"presence missing data, following value(s) missing allowed: \"ind\" (default) First, variable missingness, new missingness indicator variable created takes value 1 original covariate NA 0 otherwise. missingness indicators added model formula main effects. missing values covariates replaced 0s (value arbitrary affect estimation). weight estimation proceeds new formula set covariates. covariates output resulting weightit object original covariates NAs.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_cbps.html","id":"additional-arguments","dir":"Reference","previous_headings":"","what":"Additional Arguments","title":"Covariate Balancing Propensity Score Weighting — method_cbps","text":"arguments CBPS() can passed weightit() weightitMSM(), following exceptions: method CBPS() replaced argument weightit(). Setting = FALSE weightit() equivalent setting method = \"exact\" CBPS(). sample.weights ignored sampling weights passed using s.weights. standardize ignored. arguments take defaults CBPS(). may useful many cases set = FALSE, especially continuous treatments.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_cbps.html","id":"additional-outputs","dir":"Reference","previous_headings":"","what":"Additional Outputs","title":"Covariate Balancing Propensity Score Weighting — method_cbps","text":"obj include.obj = TRUE, CB(G)PS model fit. binary treatments, multinomial treatments estimand = \"ATE\" four fewer treatment levels, continuous treatments, output call CBPS::CBPS(). multinomial treatments estimand = \"ATT\" four treatment levels, list CBPS fit objects.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_cbps.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Covariate Balancing Propensity Score Weighting — method_cbps","text":"CBPS estimates coefficients logistic regression model (binary treatments), multinomial logistic regression model (form multinomial treatments), linear regression model (continuous treatments) used compute (generalized) propensity scores, weights computed. involves augmenting standard regression score equations balance constraints -identified generalized method moments estimation. idea nudge estimation coefficients toward produce balance weighted sample. just-identified version (exact = FALSE) away score equations coefficients balance constraints (score equation variance error continuous treatment) used. just-identified version therefore produce superior balance means (.e., corresponding balance constraints) binary multinomial treatments linear terms continuous treatments -identified version. Note WeightIt provides less functionality CBPS package terms versions CBPS available; extensions CBPS, CBPS package may preferred.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_cbps.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Covariate Balancing Propensity Score Weighting — method_cbps","text":"Binary treatments Imai, K., & Ratkovic, M. (2014). Covariate balancing propensity score. Journal Royal Statistical Society: Series B (Statistical Methodology), 76(1), 243–263. Multinomial Treatments Imai, K., & Ratkovic, M. (2014). Covariate balancing propensity score. Journal Royal Statistical Society: Series B (Statistical Methodology), 76(1), 243–263. Continuous treatments Fong, C., Hazlett, C., & Imai, K. (2018). Covariate balancing propensity score continuous treatment: Application efficacy political advertisements. Annals Applied Statistics, 12(1), 156–177. doi:10.1214/17-AOAS1101","code":""},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/reference/method_cbps.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Covariate Balancing Propensity Score Weighting — method_cbps","text":"sampling weights used CBPS::CBPS(), estimated weights already incorporate sampling weights. weightit() used method = \"cbps\", estimated weights separated sampling weights, methods.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_cbps.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Covariate Balancing Propensity Score Weighting — method_cbps","text":"","code":"library(\"cobalt\") data(\"lalonde\", package = \"cobalt\")  #Balancing covariates between treatment groups (binary) (W1 <- weightit(treat ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"cbps\", estimand = \"ATT\")) #> A weightit object #>  - method: \"cbps\" (covariate balancing propensity score weighting) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 2-category #>  - estimand: ATT (focal: 1) #>  - covariates: age, educ, married, nodegree, re74 summary(W1) #>                  Summary of weights #>  #> - Weight ranges: #>  #>           Min                                  Max #> treated 1.000              ||               1.0000 #> control 0.017 |---------------------------| 2.2742 #>  #> - Units with the 5 most extreme weights by group: #>                                             #>               5      4      3      2      1 #>  treated      1      1      1      1      1 #>             589    595    269    409    296 #>  control 1.4755 1.4873 1.5799 1.7484 2.2742 #>  #> - Weight statistics: #>  #>         Coef of Var   MAD Entropy # Zeros #> treated       0.000 0.000  -0.000       0 #> control       0.839 0.707   0.341       0 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted  429.       185 #> Weighted    251.99     185 bal.tab(W1) #> Call #>  weightit(formula = treat ~ age + educ + married + nodegree +  #>     re74, data = lalonde, method = \"cbps\", estimand = \"ATT\") #>  #> Balance Measures #>                Type Diff.Adj #> prop.score Distance   0.0163 #> age         Contin.  -0.0032 #> educ        Contin.   0.0017 #> married      Binary  -0.0003 #> nodegree     Binary  -0.0003 #> re74        Contin.   0.0005 #>  #> Effective sample sizes #>            Control Treated #> Unadjusted  429.       185 #> Adjusted    251.99     185  if (FALSE) { #Balancing covariates with respect to race (multinomial) (W2 <- weightit(race ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"cbps\", estimand = \"ATE\")) summary(W2) bal.tab(W2) }  #Balancing covariates with respect to re75 (continuous) (W3 <- weightit(re75 ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"cbps\", over = FALSE)) #> A weightit object #>  - method: \"cbps\" (covariate balancing propensity score weighting) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: continuous #>  - covariates: age, educ, married, nodegree, re74 summary(W3) #>                  Summary of weights #>  #> - Weight ranges: #>  #>        Min                                   Max #> all 0.0153 |---------------------------| 13.1539 #>  #> - Units with the 5 most extreme weights: #>                                             #>         484     482     180     481     483 #>  all 9.2281 10.8362 11.0895 11.9898 13.1539 #>  #> - Weight statistics: #>  #>     Coef of Var   MAD Entropy # Zeros #> all       1.151 0.449   0.288       0 #>  #> - Effective Sample Sizes: #>  #>             Total #> Unweighted 614.   #> Weighted   264.37 bal.tab(W3) #> Call #>  weightit(formula = re75 ~ age + educ + married + nodegree + re74,  #>     data = lalonde, method = \"cbps\", over = FALSE) #>  #> Balance Measures #>             Type Corr.Adj #> age      Contin.       -0 #> educ     Contin.       -0 #> married   Binary       -0 #> nodegree  Binary        0 #> re74     Contin.       -0 #>  #> Effective sample sizes #>             Total #> Unadjusted 614.   #> Adjusted   264.37"},{"path":"https://ngreifer.github.io/WeightIt/reference/method_ebal.html","id":null,"dir":"Reference","previous_headings":"","what":"Entropy Balancing — method_ebal","title":"Entropy Balancing — method_ebal","text":"page explains details estimating weights using entropy balancing setting method = \"ebal\" call weightit() weightitMSM(). method can used binary, multinomial, continuous treatments. general, method relies estimating weights minimizing negative entropy weights subject exact moment balancing constraints. method relies code written WeightIt using optim().","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_ebal.html","id":"binary-treatments","dir":"Reference","previous_headings":"","what":"Binary Treatments","title":"Entropy Balancing — method_ebal","text":"binary treatments, method estimates weights using optim() using formulas described Hainmueller (2012). following estimands allowed: ATE, ATT, ATC. ATE requested, optimization run twice, treatment group.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_ebal.html","id":"multinomial-treatments","dir":"Reference","previous_headings":"","what":"Multinomial Treatments","title":"Entropy Balancing — method_ebal","text":"multinomial treatments, method estimates weights using optim(). following estimands allowed: ATE ATT. ATE requested, optim() run treatment group. ATT requested, optim() run non-focal (.e., control) group.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_ebal.html","id":"continuous-treatments","dir":"Reference","previous_headings":"","what":"Continuous Treatments","title":"Entropy Balancing — method_ebal","text":"continuous treatments, method estimates weights using optim() using formulas described Tübbicke (2022) Vegetabile et al. (2021).","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_ebal.html","id":"longitudinal-treatments","dir":"Reference","previous_headings":"","what":"Longitudinal Treatments","title":"Entropy Balancing — method_ebal","text":"longitudinal treatments, weights product weights estimated time point. method guaranteed yield exact balance time point. NOTE: use entropy balancing longitudinal treatments validated!","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_ebal.html","id":"sampling-weights","dir":"Reference","previous_headings":"","what":"Sampling Weights","title":"Entropy Balancing — method_ebal","text":"Sampling weights supported s.weights scenarios.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_ebal.html","id":"missing-data","dir":"Reference","previous_headings":"","what":"Missing Data","title":"Entropy Balancing — method_ebal","text":"presence missing data, following value(s) missing allowed: \"ind\" (default) First, variable missingness, new missingness indicator variable created takes value 1 original covariate NA 0 otherwise. missingness indicators added model formula main effects. missing values covariates replaced 0s (value arbitrary affect estimation). weight estimation proceeds new formula set covariates. covariates output resulting weightit object original covariates NAs.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_ebal.html","id":"additional-arguments","dir":"Reference","previous_headings":"","what":"Additional Arguments","title":"Entropy Balancing — method_ebal","text":"moments int accepted. See weightit() details. base.weights vector base weights, one unit. works continuous treatments well. correspond base weights \\(q\\) Hainmueller (2012). estimated weights minimize Kullback entropy divergence base weights, defined \\(\\sum w \\log(w/q)\\), subject exact balance constraints. can used supply previously estimated weights newly estimated weights retain properties original weights ensuring balance constraints met. Sampling weights passed base.weights can included weightit() call includes s.weights. d.moments continuous treatments, number moments treatment covariate distributions constrained weighted sample original sample. example, setting d.moments = 3 ensures mean, variance, skew treatment covariates weighted sample unweighted sample. d.moments greater equal moments automatically set accordingly (specified). Vegetabile et al. (2021) recommend setting d.moments = 3, even moments less 3. argument corresponds tuning parameters \\(r\\) \\(s\\) Vegetabile et al. (2021) (set equal). Ignored binary multi-category treatments. arguments maxit reltol can supplied passed control argument optim(). \"BFGS\" method used, defaults correspond . stabilize argument ignored; past reduce variability weights iterative process. want minimize variance weights subject balance constraints, use method = \"optweight\".","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_ebal.html","id":"additional-outputs","dir":"Reference","previous_headings":"","what":"Additional Outputs","title":"Entropy Balancing — method_ebal","text":"obj include.obj = TRUE, output call optim(), contains dual variables convergence information. ATE fits multinomial treatments, list optim() outputs, one weighted group.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_ebal.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Entropy Balancing — method_ebal","text":"Entropy balancing involves specification optimization problem, solution used compute weights. constraints primal optimization problem correspond covariate balance means (binary multinomial treatments) treatment-covariate covariances (continuous treatments), positivity weights, weights sum certain value. turns dual optimization problem much easier solve many variables balance constraints rather weights unit unconstrained. Zhao Percival (2017) found entropy balancing ATT binary treatment actually involves estimation coefficients logistic regression propensity score model using specialized loss function different optimized maximum likelihood. Entropy balancing doubly robust (ATT) sense consistent either true propensity score model logistic regression treatment covariates true outcome model control units linear regression outcome covariates, attains semi-parametric efficiency bound true. Entropy balancing always yield exact mean balance included terms.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_ebal.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Entropy Balancing — method_ebal","text":"Binary Treatments Hainmueller, J. (2012). Entropy Balancing Causal Effects: Multivariate Reweighting Method Produce Balanced Samples Observational Studies. Political Analysis, 20(1), 25–46. doi:10.1093/pan/mpr025 Källberg, D., & Waernbaum, . (2022). Large Sample Properties Entropy Balancing Estimators Average Causal Effects. ArXiv:2204.10623 [Stat]. https://arxiv.org/abs/2204.10623 Zhao, Q., & Percival, D. (2017). Entropy balancing doubly robust. Journal Causal Inference, 5(1). doi:10.1515/jci-2016-0010 Continuous Treatments Tübbicke, S. (2022). Entropy Balancing Continuous Treatments. Journal Econometric Methods, 11(1), 71–89. doi:10.1515/jem-2021-0002 Vegetabile, B. G., Griffin, B. ., Coffman, D. L., Cefalu, M., Robbins, M. W., & McCaffrey, D. F. (2021). Nonparametric estimation population average dose-response curves using entropy balancing weights continuous exposures. Health Services Outcomes Research Methodology, 21(1), 69–110. doi:10.1007/s10742-020-00236-2","code":""},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/reference/method_ebal.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Entropy Balancing — method_ebal","text":"","code":"library(\"cobalt\") data(\"lalonde\", package = \"cobalt\")  #Balancing covariates between treatment groups (binary) (W1 <- weightit(treat ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"ebal\", estimand = \"ATT\")) #> A weightit object #>  - method: \"ebal\" (entropy balancing) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 2-category #>  - estimand: ATT (focal: 1) #>  - covariates: age, educ, married, nodegree, re74 summary(W1) #>                  Summary of weights #>  #> - Weight ranges: #>  #>            Min                                  Max #> treated 1.0000       ||                      1.0000 #> control 0.0398 |---------------------------| 5.2466 #>  #> - Units with the 5 most extreme weights by group: #>                                             #>               5      4      3      2      1 #>  treated      1      1      1      1      1 #>             589    595    269    409    296 #>  control 3.3964 3.4432 3.6553 4.0427 5.2466 #>  #> - Weight statistics: #>  #>         Coef of Var   MAD Entropy # Zeros #> treated       0.000 0.000   0.000       0 #> control       0.839 0.707   0.341       0 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted   429.      185 #> Weighted     252.1     185 bal.tab(W1) #> Call #>  weightit(formula = treat ~ age + educ + married + nodegree +  #>     re74, data = lalonde, method = \"ebal\", estimand = \"ATT\") #>  #> Balance Measures #>             Type Diff.Adj #> age      Contin.   0.0000 #> educ     Contin.   0.0000 #> married   Binary   0.0001 #> nodegree  Binary   0.0000 #> re74     Contin.  -0.0000 #>  #> Effective sample sizes #>            Control Treated #> Unadjusted   429.      185 #> Adjusted     252.1     185  #Balancing covariates with respect to race (multinomial) (W2 <- weightit(race ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"ebal\", estimand = \"ATE\")) #> A weightit object #>  - method: \"ebal\" (entropy balancing) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 3-category (black, hispan, white) #>  - estimand: ATE #>  - covariates: age, educ, married, nodegree, re74 summary(W2) #>                  Summary of weights #>  #> - Weight ranges: #>  #>           Min                                  Max #> black  0.5530   |-------------------------| 5.3496 #> hispan 0.1409 |----------------|            3.3322 #> white  0.3979  |-------|                    1.9224 #>  #> - Units with the 5 most extreme weights by group: #>                                            #>            226    244    485    181    182 #>   black 2.5215 2.5491 2.8059 3.5551 5.3496 #>            392    564    269    345    371 #>  hispan 2.0467   2.53 2.6322 2.7049 3.3322 #>             68    457    599    589    531 #>   white 1.7106 1.7226 1.7426 1.7743 1.9224 #>  #> - Weight statistics: #>  #>        Coef of Var   MAD Entropy # Zeros #> black        0.590 0.413   0.131       0 #> hispan       0.609 0.440   0.163       0 #> white        0.371 0.306   0.068       0 #>  #> - Effective Sample Sizes: #>  #>             black hispan  white #> Unweighted 243.    72.   299.   #> Weighted   180.47  52.71 262.93 bal.tab(W2) #> Call #>  weightit(formula = race ~ age + educ + married + nodegree + re74,  #>     data = lalonde, method = \"ebal\", estimand = \"ATE\") #>  #> Balance summary across all treatment pairs #>             Type Max.Diff.Adj #> age      Contin.       0.0001 #> educ     Contin.       0.0000 #> married   Binary       0.0001 #> nodegree  Binary       0.0001 #> re74     Contin.       0.0001 #>  #> Effective sample sizes #>             black hispan  white #> Unadjusted 243.    72.   299.   #> Adjusted   180.47  52.71 262.93  #Balancing covariates and squares with respect to #re75 (continuous), maintaining 3 moments of the #covariate and treatment distributions (W3 <- weightit(re75 ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"ebal\", moments = 2,                 d.moments = 3)) #> A weightit object #>  - method: \"ebal\" (entropy balancing) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: continuous #>  - covariates: age, educ, married, nodegree, re74 summary(W3) #>                  Summary of weights #>  #> - Weight ranges: #>  #>        Min                                   Max #> all 0.0001 |---------------------------| 17.6716 #>  #> - Units with the 5 most extreme weights: #>                                          #>         484   200    166     171     180 #>  all 6.7602 7.609 8.5222 10.2613 17.6716 #>  #> - Weight statistics: #>  #>     Coef of Var   MAD Entropy # Zeros #> all       1.254 0.615   0.424       0 #>  #> - Effective Sample Sizes: #>  #>             Total #> Unweighted 614.   #> Weighted   239.02 bal.tab(W3) #> Call #>  weightit(formula = re75 ~ age + educ + married + nodegree + re74,  #>     data = lalonde, method = \"ebal\", moments = 2, d.moments = 3) #>  #> Balance Measures #>             Type Corr.Adj #> age      Contin.   0.0001 #> educ     Contin.  -0.0002 #> married   Binary  -0.0001 #> nodegree  Binary   0.0002 #> re74     Contin.  -0.0005 #>  #> Effective sample sizes #>             Total #> Unadjusted 614.   #> Adjusted   239.02"},{"path":"https://ngreifer.github.io/WeightIt/reference/method_energy.html","id":null,"dir":"Reference","previous_headings":"","what":"Energy Balancing — method_energy","title":"Energy Balancing — method_energy","text":"page explains details estimating weights using energy balancing setting method = \"energy\" call weightit() weightitMSM(). method can used binary, multinomial, continuous treatments. general, method relies estimating weights minimizing energy statistic related covariate balance. binary multinomial treatments, energy distance, multivariate distance distributions, treatment groups. continuous treatments, sum distance covariance treatment variable covariates energy distances treatment covariates weighed sample distributions original sample. method relies code written WeightIt using osqp::osqp() osqp package perform optimization. method may slow memory-intensive large datasets.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_energy.html","id":"binary-treatments","dir":"Reference","previous_headings":"","what":"Binary Treatments","title":"Energy Balancing — method_energy","text":"binary treatments, method estimates weights using osqp() using formulas described Huling Mak (2022). following estimands allowed: ATE, ATT, ATC.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_energy.html","id":"multinomial-treatments","dir":"Reference","previous_headings":"","what":"Multinomial Treatments","title":"Energy Balancing — method_energy","text":"multinomial treatments, method estimates weights using osqp() using formulas described Huling Mak (2022). following estimands allowed: ATE ATT.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_energy.html","id":"continuous-treatments","dir":"Reference","previous_headings":"","what":"Continuous Treatments","title":"Energy Balancing — method_energy","text":"continuous treatments, method estimates weights using osqp() using formulas described Huling, Greifer, Chen (2021).","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_energy.html","id":"longitudinal-treatments","dir":"Reference","previous_headings":"","what":"Longitudinal Treatments","title":"Energy Balancing — method_energy","text":"longitudinal treatments, weights product weights estimated time point. method guaranteed yield optimal balance time point. NOTE: use energy balancing longitudinal treatments validated!","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_energy.html","id":"sampling-weights","dir":"Reference","previous_headings":"","what":"Sampling Weights","title":"Energy Balancing — method_energy","text":"Sampling weights supported s.weights scenarios. cases, sampling weights cause optimization fail due lack convexity infeasible constraints.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_energy.html","id":"missing-data","dir":"Reference","previous_headings":"","what":"Missing Data","title":"Energy Balancing — method_energy","text":"presence missing data, following value(s) missing allowed: \"ind\" (default) First, variable missingness, new missingness indicator variable created takes value 1 original covariate NA 0 otherwise. missingness indicators added model formula main effects. missing values covariates replaced 0s (value arbitrary affect estimation). weight estimation proceeds new formula set covariates. covariates output resulting weightit object original covariates NAs.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_energy.html","id":"additional-arguments","dir":"Reference","previous_headings":"","what":"Additional Arguments","title":"Energy Balancing — method_energy","text":"following following additional arguments can specified: dist.mat name method used compute distance matrix covariates numeric distance matrix . Allowable options include \"scaled_euclidean\" Euclidean (L2) distance scaled covariates (default), \"mahalanobis\" Mahalanobis distance, \"euclidean\" raw Euclidean distance. Abbreviations allowed. Note user-supplied distance matrices can cause R session abort due bug within osqp, argument used caution. distance matrix must square, symmetric, numeric matrix zeros along diagonal row column unit. Can also supplied output call dist(). lambda positive numeric scalar used penalize square weights. value divided square total sample size added diagonal quadratic part loss function. Higher values favor weights less variability. Note distinct lambda value described Huling Mak (2022), penalizes complexity individual treatment rules rather weights, correspond lambda Huling et al. (2021). Default .0001, essentially 0. binary multinomial treatments, following additional argument can specified: improved logical; whether use improved energy balancing weights described Huling Mak (2022) estimand = \"ATE\". involves optimizing balance treatment group overall sample, also pair treatment groups. Huling Mak (2022) found improved energy balancing weights generally outperformed standard energy balancing. Default TRUE; set FALSE use standard energy balancing weights instead (recommended). continuous treatments, following additional arguments can specified: d.moments number moments treatment covariate distributions constrained weighted sample original sample. example, setting d.moments = 3 ensures mean, variance, skew treatment covariates weighted sample unweighted sample. d.moments greater equal moments automatically set accordingly (specified). dimension.adj logical; whether include dimensionality adjustment described Huling et al. (2021). TRUE, default, energy distance covariates weighted \\(\\sqrt{p}\\) times much energy distance treatment, \\(p\\) number covariates. FALSE, two energy distances given equal weights. Default TRUE. moments argument functions differently method = \"energy\" methods. unspecified set zero, energy balancing weights estimated described Huling Mak (2022) binary multi-category treatments Huling et al. (2021) contionuous treatments. moments set integer larger 0, additional balance constraints requested moments covariates also included, guaranteeing exact moment balance covariates minimizing energy distance weighted sample. binary multinomial treatments, involves exact balance means entered covariates; continuous treatments, involves exact balance treatment-covariate correlations entered covariates.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_energy.html","id":"additional-outputs","dir":"Reference","previous_headings":"","what":"Additional Outputs","title":"Energy Balancing — method_energy","text":"obj include.obj = TRUE, output call osqp::solve_osqp(), contains dual variables convergence information.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_energy.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Energy Balancing — method_energy","text":"Energy balancing method estimating weights using optimization without propensity score. weights solution constrain quadratic optimization problem objective function concerns covariate balance measured energy distance (continuous tretaments) distance covariance. Energy balancing binary multi-category tretaments involves minimizing energy disance treatment groups treatment group target group (e.g., full sample ATE). energy distance scalar measure difference two multivariate distributions equal 0 two distributions identical. Energy balancing continuous treatments involves minimizing distance covariance treatment covariates; distance covariance scalar measure association two (possibly multivariate) distributions equal 0 two distributions independent. addition, energy distances treatment covariate distributions weighted sample treatment covariate distributions original sample minimized. primary benefit energy balancing features covariate distribution balanced, just means, optimization-based methods like entropy balancing. Still, possible add additional balance constraints require balance individual terms using moments argument, just like entropy balancing. Energy balancing can sometimes yield weights high variability; lambda argument can supplied penalize highly variable weights increase effective sample size expense balance.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_energy.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Energy Balancing — method_energy","text":"Sometimes optimization can fail converge problem convex. warning displayed . cases, try simply re-fitting weights without changing anything. method repeatedly fails, try another method change supplied parameters (though uncommon). Increasing max_iter might help.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_energy.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Energy Balancing — method_energy","text":"Noah Greifer, using code Jared Huling's independenceWeights package continuous treatments.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_energy.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Energy Balancing — method_energy","text":"Binary Multinomial treatments Huling, J. D., & Mak, S. (2022). Energy Balancing Covariate Distributions (arXiv:2004.13962). arXiv. doi:10.48550/arXiv.2004.13962 Continuous treatments Huling, J. D., Greifer, N., & Chen, G. (2021). Independence weights causal inference continuous exposures (arXiv:2107.07086). arXiv. doi:10.48550/arXiv.2107.07086","code":""},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/reference/method_energy.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Energy Balancing — method_energy","text":"","code":"library(\"cobalt\") data(\"lalonde\", package = \"cobalt\")  #Examples may not converge, but may after several runs if (FALSE) { #Balancing covariates between treatment groups (binary) (W1 <- weightit(treat ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"energy\", estimand = \"ATE\")) summary(W1) bal.tab(W1)  #Balancing covariates with respect to race (multinomial) (W2 <- weightit(race ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"energy\", estimand = \"ATT\",                 focal = \"black\")) summary(W2) bal.tab(W2)   #Balancing covariates with respect to re75 (continuous) (W3 <- weightit(re75 ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"energy\", moments = 1)) summary(W3) bal.tab(W3, poly = 2) }"},{"path":"https://ngreifer.github.io/WeightIt/reference/method_gbm.html","id":null,"dir":"Reference","previous_headings":"","what":"Propensity Score Weighting Using Generalized Boosted Models — method_gbm","title":"Propensity Score Weighting Using Generalized Boosted Models — method_gbm","text":"page explains details estimating weights generalized boosted model-based propensity scores setting method = \"gbm\" call weightit() weightitMSM(). method can used binary, multinomial, continuous treatments. general, method relies estimating propensity scores using generalized boosted modeling converting propensity scores weights using formula depends desired estimand. algorithm involves using balance-based prediction-based criterion optimize choosing value tuning parameters (number trees possibly others). method relies gbm package. method mimics functionality functions twang package, improved performance flexible options. See Details section details.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_gbm.html","id":"binary-treatments","dir":"Reference","previous_headings":"","what":"Binary Treatments","title":"Propensity Score Weighting Using Generalized Boosted Models — method_gbm","text":"binary treatments, method estimates propensity scores using gbm::gbm.fit() selects optimal tuning parameter values using method specified stop.method argument. following estimands allowed: ATE, ATT, ATC, ATO, ATM. weights computed estimated propensity scores using get_w_from_ps(), implements standard formulas. Weights can also computed using marginal mean weighting stratification ATE, ATT, ATC. See get_w_from_ps() details.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_gbm.html","id":"multinomial-treatments","dir":"Reference","previous_headings":"","what":"Multinomial Treatments","title":"Propensity Score Weighting Using Generalized Boosted Models — method_gbm","text":"multinomial treatments, method estimates propensity scores using gbm::gbm.fit() distribution = \"multinomial\" selects optimal tuning parameter values using method specified stop.method argument. following estimands allowed: ATE, ATT, ATC, ATO, ATM. weights computed estimated propensity scores using get_w_from_ps(), implements standard formulas. Weights can also computed using marginal mean weighting stratification ATE, ATT, ATC. See get_w_from_ps() details.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_gbm.html","id":"continuous-treatments","dir":"Reference","previous_headings":"","what":"Continuous Treatments","title":"Propensity Score Weighting Using Generalized Boosted Models — method_gbm","text":"continuous treatments, method estimates generalized propensity score using gbm::gbm.fit() selects optimal tuning parameter values using method specified stop.method argument.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_gbm.html","id":"longitudinal-treatments","dir":"Reference","previous_headings":"","what":"Longitudinal Treatments","title":"Propensity Score Weighting Using Generalized Boosted Models — method_gbm","text":"longitudinal treatments, weights product weights estimated time point.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_gbm.html","id":"sampling-weights","dir":"Reference","previous_headings":"","what":"Sampling Weights","title":"Propensity Score Weighting Using Generalized Boosted Models — method_gbm","text":"Sampling weights supported s.weights scenarios.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_gbm.html","id":"missing-data","dir":"Reference","previous_headings":"","what":"Missing Data","title":"Propensity Score Weighting Using Generalized Boosted Models — method_gbm","text":"presence missing data, following value(s) missing allowed: \"ind\" (default) First, variable missingness, new missingness indicator variable created takes value 1 original covariate NA 0 otherwise. missingness indicators added model formula main effects. weight estimation proceeds new formula set covariates using surrogate splitting described . covariates output resulting weightit object original covariates NAs. \"surr\" Surrogate splitting used process NAs. missingness indicators created. Nodes split using non-missing values variable. generate predicted values unit, non-missing variable operates similarly variable missingness used surrogate. Missing values ignored calculating balance statistics choose optimal tree.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_gbm.html","id":"additional-arguments","dir":"Reference","previous_headings":"","what":"Additional Arguments","title":"Propensity Score Weighting Using Generalized Boosted Models — method_gbm","text":"following additional arguments can specified: stop.method string describing balance criterion used select best weights. See stop.method allowable options treatment type. addition, optimize cross-validation error instead balance, stop.method can set \"cv{#}\", {#} replaced number representing number cross-validation folds used (e.g., \"cv5\" 5-fold cross-validation). binary multinomial treatments, default \"es.mean\", minimizes average absolute standard mean difference among covariates treatment groups. continuous treatments, default \"p.mean\", minimizes average absolute Pearson correlation treatment covariates. trim.number supplied trim() trims weights trees choosing best tree. can valuable weights extreme, occurs especially continuous treatments. default 0 (.e., trimming). distribution string distribution used loss function boosted model. supplied distribution argument gbm::gbm.fit(). binary treatments, \"bernoulli\" \"adaboost\" available, \"bernoulli\" default. multinomial treatments, \"multinomial\" allowed. continuous treatments \"gaussian\", \"laplace\", \"tdist\" available, \"gaussian\" default. argument tunable. n.trees maximum number trees used. passed onto n.trees argument gbm.fit(). default 10000 binary multinomial treatments 20000 continuous treatments. start.tree tree start balance checking. know best balance first 100 trees, example, can set start.tree = 101 balance statistics computed first 100 trees. can save time since balance checking takes bulk run time balance-based stopping methods, especially useful running model adding trees. default 1, .e., start first tree assessing balance. interaction.depth depth trees. passed onto interaction.depth argument gbm.fit(). Higher values indicate better ability capture nonlinear nonadditive relationships. default 3 binary multinomial treatments 4 continuous treatments. argument tunable. shrinkage shrinkage parameter applied trees. passed onto shrinkage argument gbm.fit(). default .01 binary multinomial treatments .0005 continuous treatments. lower value , trees one may include reach optimum. argument tunable. bag.fraction fraction units randomly selected propose next tree expansion. passed onto bag.fraction argument gbm.fit(). default 1, smaller values tried. values less 1, subsequent runs parameters yield different results due random sampling; sure seed seed using set.seed() ensure replicability results. arguments take defaults gbm::gbm.fit(), used . w argument gbm.fit() ignored sampling weights passed using s.weights. continuous treatments , following arguments may supplied: density function corresponding conditional density treatment. standardized residuals treatment model fed function produce numerator denominator generalized propensity score weights. blank, dnorm() used recommended Robins et al. (2000). can also supplied string containing name function called. string contains underscores, call split underscores latter splits supplied arguments second argument beyond. example, density = \"dt_2\" specified, density used t-distribution 2 degrees freedom. Using t-distribution can useful extreme outcome values observed (Naimi et al., 2014). Ignored use.kernel = TRUE (described ). use.kernel TRUE, uses kernel density estimation density() function estimate numerator denominator densities weights. FALSE (default), argument density parameter used instead. bw, adjust, kernel, n use.kernel = TRUE, arguments density(). defaults density except n 10 times number units sample. plot use.kernel = TRUE continuous treatments, whether plot estimated density. tunable arguments, multiple entries may supplied, weightit() choose best value optimizing criterion specified stop.method. See additional outputs included arguments supplied tuned. See Examples example tuning.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_gbm.html","id":"additional-outputs","dir":"Reference","previous_headings":"","what":"Additional Outputs","title":"Propensity Score Weighting Using Generalized Boosted Models — method_gbm","text":"info list following entries: best.tree number trees optimum. close n.trees, weightit() rerun larger value n.trees, start.tree can set just best.tree. parameters tuned, best tree value best combination tuned parameters. See example. tree.val data frame two columns: first number trees second value criterion corresponding tree. Running plot() object plot criterion number trees good way see patterns relationship determine trees needed. parameters tuned, number trees criterion values best combination tuned parameters. See example. arguments tuned (.e., supplied one value), following two additional components included info: tune data frame column argument tuned, best value balance criterion given combination parameters, number trees best value reached. best.tune one-row data frame containing values arguments tuned ultimately selected estimate returned weights. obj include.obj = TRUE, gbm fit used generate predicted values.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_gbm.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Propensity Score Weighting Using Generalized Boosted Models — method_gbm","text":"Generalized boosted modeling (GBM, also known gradient boosting machines) machine learning method generates predicted values flexible regression treatment covariates, treated propensity scores used compute weights. building series regression trees, fit residuals last, minimizing loss function depends distribution chosen. optimal number trees tuning parameter must chosen; McCaffrey et al. (2004) innovative using covariate balance select value rather traditional machine learning performance metrics cross-validation accuracy. GBM particularly effective fitting nonlinear treatment models characterized curves interactions, performs worse simpler treatment models. unclear balance measure used select number trees, though research indicated balance measures tend perform better cross-validation accuracy estimating effective propensity score weights. WeightIt offers almost identical functionality twang, first package implement method. Compared current version twang, WeightIt offers options measure balance used select number trees, improved performance, tuning hyperparameters, estimands, support continuous treatments. WeightIt computes weights multinomial treatments differently twang ; rather fitting separate binary GBM pair treatments, WeightIt fits single multi-class GBM model uses balance measures appropriate multinomial treatments.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_gbm.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Propensity Score Weighting Using Generalized Boosted Models — method_gbm","text":"Binary treatments McCaffrey, D. F., Ridgeway, G., & Morral, . R. (2004). Propensity Score Estimation Boosted Regression Evaluating Causal Effects Observational Studies. Psychological Methods, 9(4), 403–425. doi:10.1037/1082-989X.9.4.403 Multinomial Treatments McCaffrey, D. F., Griffin, B. ., Almirall, D., Slaughter, M. E., Ramchand, R., & Burgette, L. F. (2013). Tutorial Propensity Score Estimation Multiple Treatments Using Generalized Boosted Models. Statistics Medicine, 32(19), 3388–3414. doi:10.1002/sim.5753 Continuous treatments Zhu, Y., Coffman, D. L., & Ghosh, D. (2015). Boosting Algorithm Estimating Generalized Propensity Scores Continuous Treatments. Journal Causal Inference, 3(1). doi:10.1515/jci-2014-0022","code":""},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/reference/method_gbm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Propensity Score Weighting Using Generalized Boosted Models — method_gbm","text":"","code":"library(\"cobalt\") data(\"lalonde\", package = \"cobalt\")  #Balancing covariates between treatment groups (binary) (W1 <- weightit(treat ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"gbm\", estimand = \"ATE\",                 stop.method = \"es.max\")) #> A weightit object #>  - method: \"gbm\" (propensity score weighting with GBM) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 2-category #>  - estimand: ATE #>  - covariates: age, educ, married, nodegree, re74 summary(W1) #>                  Summary of weights #>  #> - Weight ranges: #>  #>            Min                                   Max #> treated 1.1137 |---------------------------| 12.6196 #> control 1.0147 |----------------|             8.4841 #>  #> - Units with the 5 most extreme weights by group: #>                                               #>             115     177    184    183     182 #>  treated 9.4968 10.7796 11.052 11.052 12.6196 #>             585     569    592    374     608 #>  control 4.2564  5.3573 5.5494 5.5494  8.4841 #>  #> - Weight statistics: #>  #>         Coef of Var   MAD Entropy # Zeros #> treated       0.848 0.514   0.244       0 #> control       0.460 0.233   0.066       0 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted  429.    185.   #> Weighted    354.26  107.84 bal.tab(W1) #> Call #>  weightit(formula = treat ~ age + educ + married + nodegree +  #>     re74, data = lalonde, method = \"gbm\", estimand = \"ATE\", stop.method = \"es.max\") #>  #> Balance Measures #>                Type Diff.Adj #> prop.score Distance   0.7940 #> age         Contin.  -0.1999 #> educ        Contin.   0.0829 #> married      Binary  -0.0867 #> nodegree     Binary   0.0473 #> re74        Contin.  -0.0599 #>  #> Effective sample sizes #>            Control Treated #> Unadjusted  429.    185.   #> Adjusted    354.26  107.84  if (FALSE) { #Balancing covariates with respect to race (multinomial) (W2 <- weightit(race ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"gbm\", estimand = \"ATT\",                 focal = \"hispan\", stop.method = \"ks.mean\")) summary(W2) bal.tab(W2)  #Balancing covariates with respect to re75 (continuous) (W3 <- weightit(re75 ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"gbm\", use.kernel = TRUE,                 stop.method = \"p.rms\", trim.at = .97)) summary(W3) bal.tab(W3)  #Using a t(3) density and illustrating the search for #more trees. W4a <- weightit(re75 ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"gbm\", density = \"dt_3\",                 stop.method = \"p.max\",                 n.trees = 10000)  W4a$info$best.tree #10000; optimum hasn't been found plot(W4a$info$tree.val) #decreasing at right edge  W4b <- weightit(re75 ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"gbm\", density = \"dt_3\",                 stop.method = \"p.max\",                 start.tree = 10000,                 n.trees = 20000)  W4b$info$best.tree #13417; optimum has been found plot(W4b$info$tree.val) #increasing at right edge  bal.tab(W4b)  #Tuning hyperparameters (W5 <- weightit(treat ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"gbm\", estimand = \"ATT\",                 stop.method = \"ks.max\",                 interaction.depth = 2:4,                 distribution = c(\"bernoulli\", \"adaboost\")))  W5$info$tune  W5$info$best.tune #Best values of tuned parameters  bal.tab(W5, stats = \"k\") }"},{"path":"https://ngreifer.github.io/WeightIt/reference/method_npcbps.html","id":null,"dir":"Reference","previous_headings":"","what":"Nonparametric Covariate Balancing Propensity Score Weighting — method_npcbps","title":"Nonparametric Covariate Balancing Propensity Score Weighting — method_npcbps","text":"page explains details estimating weights nonparametric covariate balancing propensity scores setting method = \"npcbps\" call weightit() weightitMSM(). method can used binary, multinomial, continuous treatments. general, method relies estimating weights maximizing empirical likelihood data subject balance constraints. method relies CBPS::npCBPS() CBPS package.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_npcbps.html","id":"binary-treatments","dir":"Reference","previous_headings":"","what":"Binary Treatments","title":"Nonparametric Covariate Balancing Propensity Score Weighting — method_npcbps","text":"binary treatments, method estimates weights using CBPS::npCBPS(). ATE estimand allowed. weights taken output npCBPS fit object.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_npcbps.html","id":"multinomial-treatments","dir":"Reference","previous_headings":"","what":"Multinomial Treatments","title":"Nonparametric Covariate Balancing Propensity Score Weighting — method_npcbps","text":"multinomial treatments, method estimates weights using CBPS::npCBPS(). ATE estimand allowed. weights taken output npCBPS fit object.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_npcbps.html","id":"continuous-treatments","dir":"Reference","previous_headings":"","what":"Continuous Treatments","title":"Nonparametric Covariate Balancing Propensity Score Weighting — method_npcbps","text":"continuous treatments, method estimates weights using CBPS::npCBPS(). weights taken output npCBPS fit object.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_npcbps.html","id":"longitudinal-treatments","dir":"Reference","previous_headings":"","what":"Longitudinal Treatments","title":"Nonparametric Covariate Balancing Propensity Score Weighting — method_npcbps","text":"longitudinal treatments, weights product weights estimated time point. CBMSM CBPS package estimates weights longitudinal treatments.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_npcbps.html","id":"sampling-weights","dir":"Reference","previous_headings":"","what":"Sampling Weights","title":"Nonparametric Covariate Balancing Propensity Score Weighting — method_npcbps","text":"Sampling weights supported method = \"npcbps\".","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_npcbps.html","id":"missing-data","dir":"Reference","previous_headings":"","what":"Missing Data","title":"Nonparametric Covariate Balancing Propensity Score Weighting — method_npcbps","text":"presence missing data, following value(s) missing allowed: \"ind\" (default) First, variable missingness, new missingness indicator variable created takes value 1 original covariate NA 0 otherwise. missingness indicators added model formula main effects. missing values covariates replaced 0s (value arbitrary affect estimation). weight estimation proceeds new formula set covariates. covariates output resulting weightit object original covariates NAs.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_npcbps.html","id":"additional-arguments","dir":"Reference","previous_headings":"","what":"Additional Arguments","title":"Nonparametric Covariate Balancing Propensity Score Weighting — method_npcbps","text":"arguments npCBPS() can passed weightit() weightitMSM(). arguments take defaults npCBPS().","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_npcbps.html","id":"additional-outputs","dir":"Reference","previous_headings":"","what":"Additional Outputs","title":"Nonparametric Covariate Balancing Propensity Score Weighting — method_npcbps","text":"obj include.obj = TRUE, nonparametric CB(G)PS model fit. output call CBPS::npCBPS().","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_npcbps.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Nonparametric Covariate Balancing Propensity Score Weighting — method_npcbps","text":"Nonparametric CBPS involves specification constrained optimization problem weights. constraints correspond covariate balance, loss function empirical likelihood data given weights. npCBPS similar entropy balancing generally produce similar results. optimization problem npCBPS convex can slow converge converge , approximate balance allowed instead using cor.prior argument, controls average deviation zero correlation treatment covariates allowed.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_npcbps.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Nonparametric Covariate Balancing Propensity Score Weighting — method_npcbps","text":"Fong, C., Hazlett, C., & Imai, K. (2018). Covariate balancing propensity score continuous treatment: Application efficacy political advertisements. Annals Applied Statistics, 12(1), 156–177. doi:10.1214/17-AOAS1101","code":""},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/reference/method_npcbps.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Nonparametric Covariate Balancing Propensity Score Weighting — method_npcbps","text":"","code":"# Examples take a long time to run if (FALSE) { library(\"cobalt\") data(\"lalonde\", package = \"cobalt\")  #Balancing covariates between treatment groups (binary) (W1 <- weightit(treat ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"npcbps\", estimand = \"ATE\")) summary(W1) bal.tab(W1)  #Balancing covariates with respect to race (multinomial) (W2 <- weightit(race ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"npcbps\", estimand = \"ATE\")) summary(W2) bal.tab(W2) }"},{"path":"https://ngreifer.github.io/WeightIt/reference/method_optweight.html","id":null,"dir":"Reference","previous_headings":"","what":"Optimization-Based Weighting — method_optweight","title":"Optimization-Based Weighting — method_optweight","text":"page explains details estimating optimization-based weights 9also known stable balancing weights) setting method = \"optweight\" call weightit() weightitMSM(). method can used binary, multinomial, continuous treatments. general, method relies estimating weights solving quadratic programming problem subject approximate exact balance constraints. method relies optweight::optweight() optweight package. optweight() offers finer control uses syntax weightit(), recommended optweight::optweight() used instead weightit method = \"optweight\".","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_optweight.html","id":"binary-treatments","dir":"Reference","previous_headings":"","what":"Binary Treatments","title":"Optimization-Based Weighting — method_optweight","text":"binary treatments, method estimates weights using optweight::optweight(). following estimands allowed: ATE, ATT, ATC. weights taken output optweight fit object.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_optweight.html","id":"multinomial-treatments","dir":"Reference","previous_headings":"","what":"Multinomial Treatments","title":"Optimization-Based Weighting — method_optweight","text":"multinomial treatments, method estimates weights using optweight::optweight(). following estimands allowed: ATE ATT. weights taken output optweight fit object.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_optweight.html","id":"continuous-treatments","dir":"Reference","previous_headings":"","what":"Continuous Treatments","title":"Optimization-Based Weighting — method_optweight","text":"binary treatments, method estimates weights using optweight::optweight(). weights taken output optweight fit object.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_optweight.html","id":"longitudinal-treatments","dir":"Reference","previous_headings":"","what":"Longitudinal Treatments","title":"Optimization-Based Weighting — method_optweight","text":"longitudinal treatments, optweight() estimates weights simultaneously satisfy balance constraints time points, one model fit obtain weights. Using method = \"optweight\" weightitMSM() causes .MSM.method set TRUE default. Setting FALSE run one model time point multiply weights together, method recommended. NOTE: neither use optimization-based weights longitudinal treatments validated!","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_optweight.html","id":"sampling-weights","dir":"Reference","previous_headings":"","what":"Sampling Weights","title":"Optimization-Based Weighting — method_optweight","text":"Sampling weights supported s.weights scenarios.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_optweight.html","id":"missing-data","dir":"Reference","previous_headings":"","what":"Missing Data","title":"Optimization-Based Weighting — method_optweight","text":"presence missing data, following value(s) missing allowed: \"ind\" (default) First, variable missingness, new missingness indicator variable created takes value 1 original covariate NA 0 otherwise. missingness indicators added model formula main effects. missing values covariates replaced 0s (value arbitrary affect estimation). weight estimation proceeds new formula set covariates. covariates output resulting weightit object original covariates NAs.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_optweight.html","id":"additional-arguments","dir":"Reference","previous_headings":"","what":"Additional Arguments","title":"Optimization-Based Weighting — method_optweight","text":"arguments optweight() can passed weightit() weightitMSM(), following exception: targets used ignored. arguments take defaults optweight().","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_optweight.html","id":"additional-outputs","dir":"Reference","previous_headings":"","what":"Additional Outputs","title":"Optimization-Based Weighting — method_optweight","text":"info list one entry: duals data frame dual variables balance constraint. obj include.obj = TRUE, output call optweight::optweight().","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_optweight.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Optimization-Based Weighting — method_optweight","text":"specification tols differs weightit() optweight(). weightit(), one tolerance value included per level factor variable, whereas optweight(), levels factor given tolerance, one value needs supplied factor variable. potential confusion ambiguity, recommended supply one value tols weightit() applies variables. finer control, use optweight() directly. Seriously, just use optweight::optweight(). syntax almost identical compatible cobalt, .","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_optweight.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Optimization-Based Weighting — method_optweight","text":"Stable balancing weights weights solve constrained optimization problem, constraints correspond covariate balance loss function variance (norm) weights. weights maximize effective sample size weighted sample subject user-supplied balance constraints. advantage method entropy balancing ability allow approximate, rather exact, balance tols argument, can increase precision even slight relaxations constraints.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_optweight.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Optimization-Based Weighting — method_optweight","text":"Binary Treatments Wang, Y., & Zubizarreta, J. R. (2020). Minimal dispersion approximately balancing weights: Asymptotic properties practical considerations. Biometrika, 107(1), 93–105. doi:10.1093/biomet/asz050 Zubizarreta, J. R. (2015). Stable Weights Balance Covariates Estimation Incomplete Outcome Data. Journal American Statistical Association, 110(511), 910–922. doi:10.1080/01621459.2015.1023805 Multinomial Treatments de los Angeles Resa, M., & Zubizarreta, J. R. (2020). Direct stable weight adjustment non-experimental studies multivalued treatments: Analysis effect earthquake post-traumatic stress. Journal Royal Statistical Society: Series (Statistics Society), n/(n/). doi:10.1111/rssa.12561 Continuous Treatments Greifer, N. (2020). Estimating Balancing Weights Continuous Treatments Using Constrained Optimization. doi:10.17615/DYSS-B342","code":""},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/reference/method_optweight.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Optimization-Based Weighting — method_optweight","text":"","code":"library(\"cobalt\") data(\"lalonde\", package = \"cobalt\")  #Balancing covariates between treatment groups (binary) (W1 <- weightit(treat ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"optweight\", estimand = \"ATT\",                 tols = 0)) #> A weightit object #>  - method: \"optweight\" (targeted stable balancing weights) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 2-category #>  - estimand: ATT (focal: 1) #>  - covariates: age, educ, married, nodegree, re74 summary(W1) #>                  Summary of weights #>  #> - Weight ranges: #>  #>         Min                                  Max #> treated   1           ||                  1.0000 #> control   0 |---------------------------| 3.0426 #>  #> - Units with the 5 most extreme weights by group: #>                                             #>               5      4      3      2      1 #>  treated      1      1      1      1      1 #>             411    589    269    409    296 #>  control 2.5261 2.5415 2.6434 2.7396 3.0426 #>  #> - Weight statistics: #>  #>         Coef of Var   MAD Entropy # Zeros #> treated       0.000 0.000   0.000       0 #> control       0.788 0.697   0.393      83 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted  429.       185 #> Weighted    264.88     185 bal.tab(W1) #> Call #>  weightit(formula = treat ~ age + educ + married + nodegree +  #>     re74, data = lalonde, method = \"optweight\", estimand = \"ATT\",  #>     tols = 0) #>  #> Balance Measures #>             Type Diff.Adj #> age      Contin.       -0 #> educ     Contin.       -0 #> married   Binary       -0 #> nodegree  Binary        0 #> re74     Contin.       -0 #>  #> Effective sample sizes #>            Control Treated #> Unadjusted  429.       185 #> Adjusted    264.88     185  #Balancing covariates with respect to race (multinomial) (W2 <- weightit(race ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"optweight\", estimand = \"ATE\",                 tols = .01)) #> A weightit object #>  - method: \"optweight\" (targeted stable balancing weights) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 3-category (black, hispan, white) #>  - estimand: ATE #>  - covariates: age, educ, married, nodegree, re74 summary(W2) #>                  Summary of weights #>  #> - Weight ranges: #>  #>           Min                                  Max #> black  0.4429     |-----------------------| 3.5741 #> hispan 0.0000 |-------------------|         2.5848 #> white  0.2574   |---------|                 1.6593 #>  #> - Units with the 5 most extreme weights by group: #>                                            #>            184    190    485    181    182 #>   black 2.3351 2.3723 2.5586 2.8367 3.5741 #>            392    345    269    564    371 #>  hispan 2.0459 2.0984 2.1887 2.1982 2.5848 #>             68    589    324    599    531 #>   white 1.5706 1.5706 1.5725 1.5968 1.6593 #>  #> - Weight statistics: #>  #>        Coef of Var   MAD Entropy # Zeros #> black        0.550 0.443   0.130       0 #> hispan       0.566 0.449   0.176       2 #> white        0.353 0.295   0.065       0 #>  #> - Effective Sample Sizes: #>  #>             black hispan  white #> Unweighted 243.     72.  299.   #> Weighted   186.76   54.7 266.01 bal.tab(W2) #> Call #>  weightit(formula = race ~ age + educ + married + nodegree + re74,  #>     data = lalonde, method = \"optweight\", estimand = \"ATE\", tols = 0.01) #>  #> Balance summary across all treatment pairs #>             Type Max.Diff.Adj #> age      Contin.         0.01 #> educ     Contin.         0.01 #> married   Binary         0.01 #> nodegree  Binary         0.01 #> re74     Contin.         0.01 #>  #> Effective sample sizes #>             black hispan  white #> Unadjusted 243.     72.  299.   #> Adjusted   186.76   54.7 266.01  #Balancing covariates with respect to re75 (continuous) (W3 <- weightit(re75 ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"optweight\", tols = .05)) #> A weightit object #>  - method: \"optweight\" (targeted stable balancing weights) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: continuous #>  - covariates: age, educ, married, nodegree, re74 summary(W3) #>                  Summary of weights #>  #> - Weight ranges: #>  #>     Min                                 Max #> all   0 |---------------------------| 3.966 #>  #> - Units with the 5 most extreme weights: #>                                       #>         482    483   200    178   180 #>  all 2.9566 3.0117 3.048 3.5934 3.966 #>  #> - Weight statistics: #>  #>     Coef of Var   MAD Entropy # Zeros #> all       0.574 0.445   0.173      34 #>  #> - Effective Sample Sizes: #>  #>             Total #> Unweighted 614.   #> Weighted   462.02 bal.tab(W3) #> Call #>  weightit(formula = re75 ~ age + educ + married + nodegree + re74,  #>     data = lalonde, method = \"optweight\", tols = 0.05) #>  #> Balance Measures #>             Type Corr.Adj #> age      Contin.  -0.0087 #> educ     Contin.  -0.0106 #> married   Binary   0.0499 #> nodegree  Binary  -0.0451 #> re74     Contin.   0.0499 #>  #> Effective sample sizes #>             Total #> Unadjusted 614.   #> Adjusted   462.02"},{"path":"https://ngreifer.github.io/WeightIt/reference/method_ps.html","id":null,"dir":"Reference","previous_headings":"","what":"Propensity Score Weighting Using Generalized Linear Models — method_ps","title":"Propensity Score Weighting Using Generalized Linear Models — method_ps","text":"page explains details estimating weights generalized linear model-based propensity scores setting method = \"ps\" call weightit() weightitMSM(). method can used binary, multinomial, continuous treatments. general, method relies estimating propensity scores parametric generalized linear model converting propensity scores weights using formula depends desired estimand. binary multinomial treatments, binomial multinomial regression model used estimate propensity scores predicted probability treatment given covariates. ordinal treatments, ordinal regression model used estimate generalized propensity scores. continuous treatments, generalized linear model used estimate generalized propensity scores conditional density treatment given covariates.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_ps.html","id":"binary-treatments","dir":"Reference","previous_headings":"","what":"Binary Treatments","title":"Propensity Score Weighting Using Generalized Linear Models — method_ps","text":"binary treatments, method estimates propensity scores using glm(). additional argument link, uses options link family(). default link \"logit\", others, including \"probit\", allowed. following estimands allowed: ATE, ATT, ATC, ATO, ATM, ATOS. Weights can also computed using marginal mean weighting stratification ATE, ATT, ATC. See get_w_from_ps() details.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_ps.html","id":"multinomial-treatments","dir":"Reference","previous_headings":"","what":"Multinomial Treatments","title":"Propensity Score Weighting Using Generalized Linear Models — method_ps","text":"multinomial treatments, propensity scores estimated using multinomial regression one functions depending requested link: logit (\"logit\") probit (\"probit\") links, mlogit::mlogit() mlogit package used; Bayesian probit (\"bayes.probit\") link, MNP::mnp() MNP package used; biased-reduced multinomial logistic regression (\"br.logit\"), brglm2::brmultinom() brglm2 package used. treatment variable ordered factor, MASS::polr() MASS package used fit ordinal regression unless link = \"br.logit\", case brglm2::bracl() brglm2 used. methods allowed method argument polr() can supplied link. following estimands allowed: ATE, ATT, ATC, ATO, ATM. weights estimand computed using standard formulas mentioned . Weights can also computed using marginal mean weighting stratification ATE, ATT, ATC. See get_w_from_ps() details.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_ps.html","id":"continuous-treatments","dir":"Reference","previous_headings":"","what":"Continuous Treatments","title":"Propensity Score Weighting Using Generalized Linear Models — method_ps","text":"continuous treatments, generalized propensity score estimated using linear regression. conditional density can specified normal another distribution. addition, kernel density estimation can used instead assuming specific density numerator denominator generalized propensity score setting use.kernel = TRUE. arguments density() can specified refine density estimation parameters. plot = TRUE can specified plot density numerator denominator, can helpful diagnosing extreme weights.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_ps.html","id":"longitudinal-treatments","dir":"Reference","previous_headings":"","what":"Longitudinal Treatments","title":"Propensity Score Weighting Using Generalized Linear Models — method_ps","text":"longitudinal treatments, weights product weights estimated time point.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_ps.html","id":"sampling-weights","dir":"Reference","previous_headings":"","what":"Sampling Weights","title":"Propensity Score Weighting Using Generalized Linear Models — method_ps","text":"Sampling weights supported s.weights scenarios except multinomial treatments link = \"bayes.probit\" binary continuous treatments missing = \"saem\" (see ). Warning messages may appear otherwise non-integer successes, can ignored.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_ps.html","id":"missing-data","dir":"Reference","previous_headings":"","what":"Missing Data","title":"Propensity Score Weighting Using Generalized Linear Models — method_ps","text":"presence missing data, following value(s) missing allowed: \"ind\" (default) First, variable missingness, new missingness indicator variable created takes value 1 original covariate NA 0 otherwise. missingness indicators added model formula main effects. missing values covariates replaced 0s (value arbitrary affect estimation). weight estimation proceeds new formula set covariates. covariates output resulting weightit object original covariates NAs. \"saem\" binary treatments link = \"logit\" continuous treatments, stochastic approximation version EM algorithm (SAEM) used via misaem package. additional covariates created. See Jiang et al. (2019) information method. cases, suitable alternative multiple imputation.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_ps.html","id":"additional-arguments","dir":"Reference","previous_headings":"","what":"Additional Arguments","title":"Propensity Score Weighting Using Generalized Linear Models — method_ps","text":"following additional arguments can specified: link link used generalized linear model propensity scores. binary treatments, link can allowed binomial(). br. prefix can added (e.g., \"br.logit\"); changes fitting method bias-corrected generalized linear models implemented brglm2 package. multicategory treatments, link can \"logit\", \"probit\", \"bayes.probit\", \"br.logit\". ordered treatments, link can allowed method argument MASS::polr() \"br.logit\". continuous treatments, link can allowed gaussian(). continuous treatments , following arguments may supplied: density function corresponding conditional density treatment. standardized residuals treatment model fed function produce numerator denominator generalized propensity score weights. blank, dnorm() used recommended Robins et al. (2000). can also supplied string containing name function called. string contains underscores, call split underscores latter splits supplied arguments second argument beyond. example, density = \"dt_2\" specified, density used t-distribution 2 degrees freedom. Using t-distribution can useful extreme outcome values observed (Naimi et al., 2014). Ignored use.kernel = TRUE (described ). use.kernel TRUE, uses kernel density estimation density() function estimate numerator denominator densities weights. FALSE, argument density parameter used instead. bw, adjust, kernel, n use.kernel = TRUE, arguments density() function. defaults density except n 10 times number units sample. plot use.kernel = TRUE continuous treatments, whether plot estimated density. binary treatments, additional arguments glm() can specified well. method argument glm() renamed glm.method. can used supply alternative fitting functions, implemented glm2 package. arguments weightit() passed ... glm(). presence missing data link = \"logit\" missing = \"saem\", additional arguments passed miss.glm predict.miss.glm, except method argument predict.miss.glm replaced saem.method. multi-category treatments link = \"logit\" \"probit\", default use multinomial logistic probit regression using mlogit package. request separate binary logistic probit regressions run instead, set use.mlogit = FALSE. can helpful mlogit slow fails converge. link = \"logit\", option use.mclogit = TRUE can specified request mclogit::mblogit() mclogit package used instead, can faster recommended. continuous treatments presence missing data missing = \"saem\", additional arguments passed miss.lm predict.miss.lm.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_ps.html","id":"additional-outputs","dir":"Reference","previous_headings":"","what":"Additional Outputs","title":"Propensity Score Weighting Using Generalized Linear Models — method_ps","text":"obj include.obj = TRUE, (generalized) propensity score model fit. binary treatments, output call glm(). ordinal treatments, output call MASS::polr(). multinomial treatments link = \"logit\" \"probit\" use.mlogit = TRUE, output call mlogit::mlogit(). multinomial treatments use.mlogit = FALSE, list glm() fits. multinomial treatments link = \"br.logit\", output call brglm2::brmultinom(). multinomial treatments link = \"bayes.probit\", output call MNP::mnp(). continuous treatments, output call glm() predicted values denominator density.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_ps.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Propensity Score Weighting Using Generalized Linear Models — method_ps","text":"Binary treatments - estimand = \"ATO\" Li, F., Morgan, K. L., & Zaslavsky, . M. (2018). Balancing covariates via propensity score weighting. Journal American Statistical Association, 113(521), 390–400. doi:10.1080/01621459.2016.1260466 - estimand = \"ATM\" Li, L., & Greene, T. (2013). Weighting Analogue Pair Matching Propensity Score Analysis. International Journal Biostatistics, 9(2). doi:10.1515/ijb-2012-0030 - estimand = \"ATOS\" Crump, R. K., Hotz, V. J., Imbens, G. W., & Mitnik, O. . (2009). Dealing limited overlap estimation average treatment effects. Biometrika, 96(1), 187–199. doi:10.1093/biomet/asn055 - estimands Austin, P. C. (2011). Introduction Propensity Score Methods Reducing Effects Confounding Observational Studies. Multivariate Behavioral Research, 46(3), 399–424. doi:10.1080/00273171.2011.568786 - Marginal mean weighting stratification Hong, G. (2010). Marginal mean weighting stratification: Adjustment selection bias multilevel data. Journal Educational Behavioral Statistics, 35(5), 499–531. doi:10.3102/1076998609359785 - Bias-reduced logistic regression See references brglm2 package. - SAEM logistic regression missing data Jiang, W., Josse, J., & Lavielle, M. (2019). Logistic regression missing covariates — Parameter estimation, model selection prediction within joint-modeling framework. Computational Statistics & Data Analysis, 106907. doi:10.1016/j.csda.2019.106907 Multinomial Treatments - estimand = \"ATO\" Li, F., & Li, F. (2019). Propensity score weighting causal inference multiple treatments. Annals Applied Statistics, 13(4), 2389–2415. doi:10.1214/19-AOAS1282 - estimand = \"ATM\" Yoshida, K., Hernández-Díaz, S., Solomon, D. H., Jackson, J. W., Gagne, J. J., Glynn, R. J., & Franklin, J. M. (2017). Matching weights simultaneously compare three treatment groups: Comparison three-way matching. Epidemiology (Cambridge, Mass.), 28(3), 387–395. doi:10.1097/EDE.0000000000000627 - estimands McCaffrey, D. F., Griffin, B. ., Almirall, D., Slaughter, M. E., Ramchand, R., & Burgette, L. F. (2013). Tutorial Propensity Score Estimation Multiple Treatments Using Generalized Boosted Models. Statistics Medicine, 32(19), 3388–3414. doi:10.1002/sim.5753 - Marginal mean weighting stratification Hong, G. (2012). Marginal mean weighting stratification: generalized method evaluating multivalued multiple treatments nonexperimental data. Psychological Methods, 17(1), 44–60. doi:10.1037/a0024918 Continuous treatments Robins, J. M., Hernán, M. Á., & Brumback, B. (2000). Marginal Structural Models Causal Inference Epidemiology. Epidemiology, 11(5), 550–560. - Using non-normal conditional densities Naimi, . ., Moodie, E. E. M., Auger, N., & Kaufman, J. S. (2014). Constructing Inverse Probability Weights Continuous Exposures: Comparison Methods. Epidemiology, 25(2), 292–299. doi:10.1097/EDE.0000000000000053 - SAEM linear regression missing data Jiang, W., Josse, J., & Lavielle, M. (2019). Logistic regression missing covariates — Parameter estimation, model selection prediction within joint-modeling framework. Computational Statistics & Data Analysis, 106907. doi:10.1016/j.csda.2019.106907","code":""},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/reference/method_ps.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Propensity Score Weighting Using Generalized Linear Models — method_ps","text":"","code":"library(\"cobalt\") data(\"lalonde\", package = \"cobalt\")  #Balancing covariates between treatment groups (binary) (W1 <- weightit(treat ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"ps\", estimand = \"ATT\",                 link = \"probit\")) #> A weightit object #>  - method: \"ps\" (propensity score weighting) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 2-category #>  - estimand: ATT (focal: 1) #>  - covariates: age, educ, married, nodegree, re74 summary(W1) #>                  Summary of weights #>  #> - Weight ranges: #>  #>            Min                                  Max #> treated 1.0000                 ||            1.0000 #> control 0.0176 |---------------------------| 1.8338 #>  #> - Units with the 5 most extreme weights by group: #>                                          #>              10     9     8     4      3 #>  treated      1     1     1     1      1 #>             612   595   269   409    296 #>  control 1.2781 1.351 1.412 1.518 1.8338 #>  #> - Weight statistics: #>  #>         Coef of Var   MAD Entropy # Zeros #> treated       0.000 0.000  -0.000       0 #> control       0.804 0.691   0.322       0 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted  429.       185 #> Weighted    260.83     185 bal.tab(W1) #> Call #>  weightit(formula = treat ~ age + educ + married + nodegree +  #>     re74, data = lalonde, method = \"ps\", estimand = \"ATT\", link = \"probit\") #>  #> Balance Measures #>                Type Diff.Adj #> prop.score Distance   0.0252 #> age         Contin.   0.0716 #> educ        Contin.  -0.0565 #> married      Binary   0.0058 #> nodegree     Binary   0.0128 #> re74        Contin.  -0.0507 #>  #> Effective sample sizes #>            Control Treated #> Unadjusted  429.       185 #> Adjusted    260.83     185  #Balancing covariates with respect to race (multinomial) (W2 <- weightit(race ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"ps\", estimand = \"ATE\",                 use.mlogit = FALSE)) #> A weightit object #>  - method: \"ps\" (propensity score weighting) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 3-category (black, hispan, white) #>  - estimand: ATE #>  - covariates: age, educ, married, nodegree, re74 summary(W2) #>                  Summary of weights #>  #> - Weight ranges: #>  #>           Min                                   Max #> black  1.4486 |---------------------------| 29.1514 #> hispan 1.8148  |------------------------|   26.9274 #> white  1.1180 |-|                            4.1126 #>  #> - Units with the 5 most extreme weights by group: #>                                                 #>             226     231     485     181     182 #>   black  7.6815  7.8028   8.566 12.2152 29.1514 #>             346     392     345     269     371 #>  hispan 17.5087 18.3157 18.4663 21.5593 26.9274 #>             432      68     404     599     531 #>   white  3.8059  3.8309  3.8835  4.0487  4.1126 #>  #> - Weight statistics: #>  #>        Coef of Var   MAD Entropy # Zeros #> black        0.861 0.426   0.188       0 #> hispan       0.546 0.407   0.134       0 #> white        0.385 0.313   0.069       0 #>  #> - Effective Sample Sizes: #>  #>             black hispan  white #> Unweighted 243.    72.   299.   #> Weighted   139.74  55.66 260.44 bal.tab(W2) #> Call #>  weightit(formula = race ~ age + educ + married + nodegree + re74,  #>     data = lalonde, method = \"ps\", estimand = \"ATE\", use.mlogit = FALSE) #>  #> Balance summary across all treatment pairs #>             Type Max.Diff.Adj #> age      Contin.       0.0374 #> educ     Contin.       0.1187 #> married   Binary       0.0540 #> nodegree  Binary       0.0510 #> re74     Contin.       0.1682 #>  #> Effective sample sizes #>             black hispan  white #> Unadjusted 243.    72.   299.   #> Adjusted   139.74  55.66 260.44  #Balancing covariates with respect to re75 (continuous) (W3 <- weightit(re75 ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"ps\", use.kernel = TRUE)) #> A weightit object #>  - method: \"ps\" (propensity score weighting) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: continuous #>  - covariates: age, educ, married, nodegree, re74 summary(W3) #>                  Summary of weights #>  #> - Weight ranges: #>  #>       Min                                   Max #> all 0.088 |---------------------------| 83.1017 #>  #> - Units with the 5 most extreme weights: #>                                            #>         482     481    484     483     485 #>  all 56.933 56.9865 70.237 77.1173 83.1017 #>  #> - Weight statistics: #>  #>     Coef of Var   MAD Entropy # Zeros #> all       2.908 1.049   1.156       0 #>  #> - Effective Sample Sizes: #>  #>             Total #> Unweighted 614.   #> Weighted    65.03 bal.tab(W3) #> Call #>  weightit(formula = re75 ~ age + educ + married + nodegree + re74,  #>     data = lalonde, method = \"ps\", use.kernel = TRUE) #>  #> Balance Measures #>             Type Corr.Adj #> age      Contin.  -0.0501 #> educ     Contin.   0.0016 #> married   Binary  -0.0427 #> nodegree  Binary  -0.0196 #> re74     Contin.  -0.0773 #>  #> Effective sample sizes #>             Total #> Unadjusted 614.   #> Adjusted    65.03"},{"path":"https://ngreifer.github.io/WeightIt/reference/method_super.html","id":null,"dir":"Reference","previous_headings":"","what":"Propensity Score Weighting Using SuperLearner — method_super","title":"Propensity Score Weighting Using SuperLearner — method_super","text":"page explains details estimating weights SuperLearner-based propensity scores setting method = \"super\" call weightit() weightitMSM(). method can used binary, multinomial, continuous treatments. general, method relies estimating propensity scores using SuperLearner algorithm stacking predictions converting propensity scores weights using formula depends desired estimand. binary multinomial treatments, one binary classification algorithms used estimate propensity scores predicted probability treatment given covariates. continuous treatments, regression algorithms used estimate generalized propensity scores conditional density treatment given covariates. method relies SuperLearner::SuperLearner() SuperLearner package.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_super.html","id":"binary-treatments","dir":"Reference","previous_headings":"","what":"Binary Treatments","title":"Propensity Score Weighting Using SuperLearner — method_super","text":"binary treatments, method estimates propensity scores using SuperLearner::SuperLearner(). following estimands allowed: ATE, ATT, ATC, ATO, ATM, ATOS. Weights can also computed using marginal mean weighting stratification ATE, ATT, ATC. See get_w_from_ps() details.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_super.html","id":"multinomial-treatments","dir":"Reference","previous_headings":"","what":"Multinomial Treatments","title":"Propensity Score Weighting Using SuperLearner — method_super","text":"multinomial treatments, propensity scores estimated using several calls SuperLearner::SuperLearner(), one treatment group; treatment probabilities normalized sum 1. following estimands allowed: ATE, ATT, ATC, ATO, ATM. weights estimand computed using standard formulas mentioned . Weights can also computed using marginal mean weighting stratification ATE, ATT, ATC. See get_w_from_ps() details.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_super.html","id":"continuous-treatments","dir":"Reference","previous_headings":"","what":"Continuous Treatments","title":"Propensity Score Weighting Using SuperLearner — method_super","text":"continuous treatments, generalized propensity score estimated using SuperLearner::SuperLearner(). addition, kernel density estimation can used instead assuming normal density numerator denominator generalized propensity score setting use.kernel = TRUE. arguments density() can specified refine density estimation parameters. plot = TRUE can specified plot density numerator denominator, can helpful diagnosing extreme weights.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_super.html","id":"longitudinal-treatments","dir":"Reference","previous_headings":"","what":"Longitudinal Treatments","title":"Propensity Score Weighting Using SuperLearner — method_super","text":"longitudinal treatments, weights product weights estimated time point.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_super.html","id":"sampling-weights","dir":"Reference","previous_headings":"","what":"Sampling Weights","title":"Propensity Score Weighting Using SuperLearner — method_super","text":"Sampling weights supported s.weights scenarios.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_super.html","id":"missing-data","dir":"Reference","previous_headings":"","what":"Missing Data","title":"Propensity Score Weighting Using SuperLearner — method_super","text":"presence missing data, following value(s) missing allowed: \"ind\" (default) First, variable missingness, new missingness indicator variable created takes value 1 original covariate NA 0 otherwise. missingness indicators added model formula main effects. missing values covariates replaced 0s. weight estimation proceeds new formula set covariates. covariates output resulting weightit object original covariates NAs.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_super.html","id":"additional-arguments","dir":"Reference","previous_headings":"","what":"Additional Arguments","title":"Propensity Score Weighting Using SuperLearner — method_super","text":"discrete TRUE, uses discrete SuperLearner, simply selects best performing method. Default FALSE, finds optimal combination predictions libraries using SL.method. argument SL.library must supplied. see list available entries, use SuperLearner::listWrappers(). arguments SuperLearner::SuperLearner() can passed weightit() weightitMSM(), following exceptions: obsWeights ignored sampling weights passed using s.weights. method SuperLearner() replaced argument SL.method weightit(). continuous treatments , following arguments may supplied: density function corresponding conditional density treatment. standardized residuals treatment model fed function produce numerator denominator generalized propensity score weights. blank, dnorm() used recommended Robins et al. (2000). can also supplied string containing name function called. string contains underscores, call split underscores latter splits supplied arguments second argument beyond. example, density = \"dt_2\" specified, density used t-distribution 2 degrees freedom. Using t-distribution can useful extreme outcome values observed (Naimi et al., 2014). Ignored use.kernel = TRUE (described ). use.kernel TRUE, uses kernel density estimation density() function estimate numerator denominator densities weights. FALSE, argument density parameter used instead. bw, adjust, kernel, n use.kernel = TRUE, arguments density() function. defaults density except n 10 times number units sample. plot use.kernel = TRUE, whether plot estimated density.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_super.html","id":"balance-superlearner","dir":"Reference","previous_headings":"","what":"Balance SuperLearner","title":"Propensity Score Weighting Using SuperLearner — method_super","text":"addition methods allowed SuperLearner(), one can specify SL.method = \"method.balance\" use \"Balance SuperLearner\" described Pirracchio Carone (2018), wherein covariate balance used choose optimal combination predictions methods specified SL.library. Coefficients chosen (one prediction method) weights generated weighted combination predictions optimize balance criterion, must set stop.method argument, described . stop.method string describing balance criterion used select best weights. See stop.method allowable options treatment type. binary multinomial treatments, default \"es.mean\", minimizes average absolute standard mean difference among covariates treatment groups. continuous treatments, default \"p.mean\", minimizes average absolute Pearson correlation treatment covariates. Note implementation differs Pirracchio Carone (2018) , balance measured terms included model formula (.e., interactions unless specifically included), balance results sample weighted using estimated predicted values propensity scores, sample matched using propensity score matching predicted values. Binary continuous treatments supported, currently multinomial treatments .","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_super.html","id":"additional-outputs","dir":"Reference","previous_headings":"","what":"Additional Outputs","title":"Propensity Score Weighting Using SuperLearner — method_super","text":"info binary continuous treatments, list two entries, coef cvRisk. multinomial treatments, list lists two entries, one treatment level. coef coefficients linear combination predictions method SL.library. Higher values indicate corresponding method plays larger role determining resulting predicted value, values close zero indicate method plays little role determining predicted value. discrete = TRUE, correspond coefficients estimated discrete FALSE. cvRisk cross-validation risk method SL.library. Higher values indicate method worse cross-validation accuracy. SL.method = \"method.balance\", sample weighted balance statistic requested stop.method. Higher values indicate worse balance. obj include.obj = TRUE, SuperLearner fit(s) used generate predicted values. binary continuous treatments, output call SuperLearner::SuperLearner(). multinomial treatments, list outputs calls SuperLearner::SuperLearner().","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_super.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Propensity Score Weighting Using SuperLearner — method_super","text":"SuperLearner works fitting several machine learning models treatment covariates taking weighted combination generated predicted values use propensity scores, used construct weights. machine learning models used supplied using SL.library argument; models supplied, higher chance correctly modeling propensity score. predicted values combined using method supplied SL.method argument (nonnegative least squares default). benefit SuperLearner , asymptotically, guaranteed perform well better best-performing method included library. Using Balance SuperLearner setting SL.method = \"method.balance\" works selecting combination predicted values minimizes imbalance measure.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_super.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Propensity Score Weighting Using SuperLearner — method_super","text":"Binary treatments Pirracchio, R., Petersen, M. L., & van der Laan, M. (2015). Improving Propensity Score Estimators’ Robustness Model Misspecification Using Super Learner. American Journal Epidemiology, 181(2), 108–119. doi:10.1093/aje/kwu253 Continuous treatments Kreif, N., Grieve, R., Díaz, ., & Harrison, D. (2015). Evaluation Effect Continuous Treatment: Machine Learning Approach Application Treatment Traumatic Brain Injury. Health Economics, 24(9), 1213–1228. doi:10.1002/hec.3189 - Balance SuperLearner (SL.method = \"method.balance\") Pirracchio, R., & Carone, M. (2018). Balance Super Learner: robust adaptation Super Learner improve estimation average treatment effect treated based propensity score matching. Statistical Methods Medical Research, 27(8), 2504–2518. doi:10.1177/0962280216682055 See method_ps additional references.","code":""},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/reference/method_super.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Propensity Score Weighting Using SuperLearner — method_super","text":"methods formerly available SuperLearner now SuperLearnerExtra, can found GitHub https://github.com/ecpolley/SuperLearnerExtra.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_super.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Propensity Score Weighting Using SuperLearner — method_super","text":"","code":"library(\"cobalt\") data(\"lalonde\", package = \"cobalt\")  #Balancing covariates between treatment groups (binary) (W1 <- weightit(treat ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"super\", estimand = \"ATT\",                 SL.library = c(\"SL.glm\", \"SL.stepAIC\",                                \"SL.glm.interaction\"))) #> Loading required package: nnls #> A weightit object #>  - method: \"super\" (propensity score weighting with SuperLearner) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 2-category #>  - estimand: ATT (focal: 1) #>  - covariates: age, educ, married, nodegree, re74 summary(W1) #>                  Summary of weights #>  #> - Weight ranges: #>  #>            Min                                  Max #> treated 1.0000      ||                       1.0000 #> control 0.0062 |---------------------------| 5.6573 #>  #> - Units with the 5 most extreme weights by group: #>                                             #>               5      4      3      2      1 #>  treated      1      1      1      1      1 #>             411    589    269    409    296 #>  control 2.4116 2.5004 2.7322 3.3402 5.6573 #>  #> - Weight statistics: #>  #>         Coef of Var   MAD Entropy # Zeros #> treated       0.000 0.000  -0.000       0 #> control       1.116 0.738   0.442       0 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted  429.       185 #> Weighted    191.27     185 bal.tab(W1) #> Call #>  weightit(formula = treat ~ age + educ + married + nodegree +  #>     re74, data = lalonde, method = \"super\", estimand = \"ATT\",  #>     SL.library = c(\"SL.glm\", \"SL.stepAIC\", \"SL.glm.interaction\")) #>  #> Balance Measures #>                Type Diff.Adj #> prop.score Distance   0.0599 #> age         Contin.  -0.1408 #> educ        Contin.   0.0167 #> married      Binary  -0.0049 #> nodegree     Binary   0.0123 #> re74        Contin.  -0.0251 #>  #> Effective sample sizes #>            Control Treated #> Unadjusted  429.       185 #> Adjusted    191.27     185 # \\donttest{ #Balancing covariates with respect to race (multinomial) (W2 <- weightit(race ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"super\", estimand = \"ATE\",                 SL.library = c(\"SL.glm\", \"SL.stepAIC\",                                \"SL.glm.interaction\"))) #> A weightit object #>  - method: \"super\" (propensity score weighting with SuperLearner) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 3-category (black, hispan, white) #>  - estimand: ATE #>  - covariates: age, educ, married, nodegree, re74 summary(W2) #>                  Summary of weights #>  #> - Weight ranges: #>  #>           Min                                   Max #> black  1.3125 |-------------------|         14.0815 #> hispan 1.7509  |--------------------------| 19.0788 #> white  1.0815 |----|                         4.9344 #>  #> - Units with the 5 most extreme weights by group: #>                                                 #>             184     244     485     182     181 #>   black  7.5917  7.7461 10.1133 11.6337 14.0815 #>             346     392     371     269     345 #>  hispan 16.6957 17.1862 17.2636 17.8517 19.0788 #>             457      23     409     589     296 #>   white  4.0945  4.1846  4.3505  4.6072  4.9344 #>  #> - Weight statistics: #>  #>        Coef of Var   MAD Entropy # Zeros #> black        0.640 0.391   0.138       0 #> hispan       0.476 0.372   0.110       0 #> white        0.388 0.317   0.069       0 #>  #> - Effective Sample Sizes: #>  #>             black hispan  white #> Unweighted 243.    72.   299.   #> Weighted   172.57  58.85 260.06 bal.tab(W2) #> Call #>  weightit(formula = race ~ age + educ + married + nodegree + re74,  #>     data = lalonde, method = \"super\", estimand = \"ATE\", SL.library = c(\"SL.glm\",  #>         \"SL.stepAIC\", \"SL.glm.interaction\")) #>  #> Balance summary across all treatment pairs #>             Type Max.Diff.Adj #> age      Contin.       0.0968 #> educ     Contin.       0.0794 #> married   Binary       0.0450 #> nodegree  Binary       0.0246 #> re74     Contin.       0.0328 #>  #> Effective sample sizes #>             black hispan  white #> Unadjusted 243.    72.   299.   #> Adjusted   172.57  58.85 260.06  #Balancing covariates with respect to re75 (continuous) #assuming t(8) conditional density for treatment (W3 <- weightit(re75 ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"super\", density = \"dt_8\",                 SL.library = c(\"SL.glm\", \"SL.ridge\",                                \"SL.glm.interaction\"))) #> A weightit object #>  - method: \"super\" (propensity score weighting with SuperLearner) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: continuous #>  - covariates: age, educ, married, nodegree, re74 summary(W3) #>                  Summary of weights #>  #> - Weight ranges: #>  #>        Min                                   Max #> all 0.0439 |---------------------------| 21.0528 #>  #> - Units with the 5 most extreme weights: #>                                            #>         431    483     484     485     354 #>  all 9.4162 16.527 18.1028 19.0692 21.0528 #>  #> - Weight statistics: #>  #>     Coef of Var   MAD Entropy # Zeros #> all       1.334 0.504   0.341       0 #>  #> - Effective Sample Sizes: #>  #>             Total #> Unweighted 614.   #> Weighted   221.04 bal.tab(W3) #> Call #>  weightit(formula = re75 ~ age + educ + married + nodegree + re74,  #>     data = lalonde, method = \"super\", density = \"dt_8\", SL.library = c(\"SL.glm\",  #>         \"SL.ridge\", \"SL.glm.interaction\")) #>  #> Balance Measures #>             Type Corr.Adj #> age      Contin.   0.0311 #> educ     Contin.   0.0348 #> married   Binary   0.0613 #> nodegree  Binary  -0.0599 #> re74     Contin.   0.0405 #>  #> Effective sample sizes #>             Total #> Unadjusted 614.   #> Adjusted   221.04 # } #Balancing covariates between treatment groups (binary) # using balance SuperLearner to minimize the average # KS statistic (W4 <- weightit(treat ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"super\", estimand = \"ATT\",                 SL.library = c(\"SL.glm\", \"SL.stepAIC\",                                \"SL.lda\"),                 SL.method = \"method.balance\",                 stop.method = \"ks.mean\")) #> A weightit object #>  - method: \"super\" (propensity score weighting with SuperLearner) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 2-category #>  - estimand: ATT (focal: 1) #>  - covariates: age, educ, married, nodegree, re74 summary(W4) #>                  Summary of weights #>  #> - Weight ranges: #>  #>            Min                                  Max #> treated 1.0000                    ||         1.0000 #> control 0.0354 |---------------------------| 1.4688 #>  #> - Units with the 5 most extreme weights by group: #>                                             #>               8      7      4      3      1 #>  treated      1      1      1      1      1 #>             411    595    269    409    296 #>  control 1.1051 1.1654 1.2041 1.2831 1.4688 #>  #> - Weight statistics: #>  #>         Coef of Var   MAD Entropy # Zeros #> treated       0.000 0.000  -0.000       0 #> control       0.739 0.662   0.278       0 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted  429.       185 #> Weighted    277.67     185 bal.tab(W4) #> Call #>  weightit(formula = treat ~ age + educ + married + nodegree +  #>     re74, data = lalonde, method = \"super\", estimand = \"ATT\",  #>     SL.library = c(\"SL.glm\", \"SL.stepAIC\", \"SL.lda\"), SL.method = \"method.balance\",  #>     stop.method = \"ks.mean\") #>  #> Balance Measures #>                Type Diff.Adj #> prop.score Distance   0.0856 #> age         Contin.   0.0855 #> educ        Contin.  -0.0174 #> married      Binary  -0.0045 #> nodegree     Binary   0.0291 #> re74        Contin.  -0.0760 #>  #> Effective sample sizes #>            Control Treated #> Unadjusted  429.       185 #> Adjusted    277.67     185"},{"path":"https://ngreifer.github.io/WeightIt/reference/method_user.html","id":null,"dir":"Reference","previous_headings":"","what":"User-Defined Functions for Estimating Weights — method_user","title":"User-Defined Functions for Estimating Weights — method_user","text":"page explains details estimating weights using user-defined function. function must take arguments passed weightit() weightitMSM() return vector weights list containing weights. supply  user-defined function, function object entered directly method; example, function fun, method = fun.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_user.html","id":"point-treatments","dir":"Reference","previous_headings":"","what":"Point Treatments","title":"User-Defined Functions for Estimating Weights — method_user","text":"following arguments automatically passed user-defined function, named parameters corresponding : treat: vector treatment status unit. comes directly left hand side formula passed weightit type (e.g., numeric, factor, etc.), may need converted. covs: data frame covariate values unit. comes directly right hand side formula passed weightit. covariates processed columns numeric; factor variables split dummies interactions evaluated. levels factor variables given dummies, matrix covariates full rank. Users can use make_full_rank, accepts numeric matrix data frame removes columns make full rank, full rank covariate matrix desired. s.weights: numeric vector sampling weights, one unit. ps: numeric vector propensity scores. subset: logical vector length treat TRUE units included estimation FALSE otherwise. used subset input objects exact used. treat, covs, s.weights, ps, supplied, already subsetted subset. estimand: character vector length 1 containing desired estimand. characters converted uppercase. \"ATC\" supplied estimand, weightit sets focal control level (usually 0 lowest level treat) sets estimand \"ATT\". focal: character vector length 1 containing focal level treatment estimand ATT (ATC detailed ). weightit() ensures value focal level treat. stabilize: logical vector length 1. processed weightit() reaches fitting function. moments: numeric vector length 1. processed weightit() reaches fitting function except .integer applied . used methods determine whether polynomials entered covariates used weight estimation. int: logical vector length 1. processed weightit() reaches fitting function. used methods determine whether interactions entered covariates used weight estimation. None parameters required fitting function. simply automatically available. addition, additional arguments supplied weightit() passed fitting function. weightit() ensures arguments correspond parameters fitting function throws error incorrectly named argument supplied fitting function include ... parameter. fitting function must output either numeric vector weights list (list-like object) entry named wither \"w\" \"weights\". list, list can contain named entries, entries named \"w\", \"weights\", \"ps\", \"fit.obj\" processed. \"ps\" vector propensity scores \"fit.obj\" object used fitting process user may want examine included weightit output object \"obj\" include.obj = TRUE. \"ps\" \"fit.obj\" components optional, \"weights\" \"w\" required.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/method_user.html","id":"longitudinal-treatments","dir":"Reference","previous_headings":"","what":"Longitudinal Treatments","title":"User-Defined Functions for Estimating Weights — method_user","text":"Longitudinal treatments can handled either running fitting function point treatments time point multiplying resulting weights together running method accommodates multiple time points outputs single set weights. former, weightitMSM() can used user-defined function just weightit(). latter method yet accommodated weightitMSM(), someday, maybe.","code":""},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/reference/method_user.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"User-Defined Functions for Estimating Weights — method_user","text":"","code":"library(\"cobalt\") data(\"lalonde\", package = \"cobalt\")  #A user-defined version of method = \"ps\" my.ps <- function(treat, covs, estimand, focal = NULL) {   covs <- make_full_rank(covs)   d <- data.frame(treat, covs)   f <- formula(d)   ps <- glm(f, data = d, family = \"binomial\")$fitted   w <- get_w_from_ps(ps, treat = treat, estimand = estimand,                      focal = focal)    return(list(w = w, ps = ps)) }  #Balancing covariates between treatment groups (binary) (W1 <- weightit(treat ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = my.ps, estimand = \"ATT\")) #> A weightit object #>  - method: \"my.ps\" (a user-defined method) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 2-category #>  - estimand: ATT (focal: 1) #>  - covariates: age, educ, married, nodegree, re74 summary(W1) #>                  Summary of weights #>  #> - Weight ranges: #>  #>            Min                                  Max #> treated 1.0000               ||              1.0000 #> control 0.0222 |---------------------------| 2.0438 #>  #> - Units with the 5 most extreme weights by group: #>                                             #>               5      4      3      2      1 #>  treated      1      1      1      1      1 #>             411    595    269    409    296 #>  control 1.3303 1.4365 1.5005 1.6369 2.0438 #>  #> - Weight statistics: #>  #>         Coef of Var   MAD Entropy # Zeros #> treated       0.000 0.000   -0.00       0 #> control       0.823 0.701    0.33       0 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted  429.       185 #> Weighted    255.99     185 bal.tab(W1) #> Call #>  weightit(formula = treat ~ age + educ + married + nodegree +  #>     re74, data = lalonde, method = my.ps, estimand = \"ATT\") #>  #> Balance Measures #>                Type Diff.Adj #> prop.score Distance   0.0199 #> age         Contin.   0.0459 #> educ        Contin.  -0.0360 #> married      Binary   0.0044 #> nodegree     Binary   0.0080 #> re74        Contin.  -0.0275 #>  #> Effective sample sizes #>            Control Treated #> Unadjusted  429.       185 #> Adjusted    255.99     185 #Balancing covariates for longitudinal treatments # my.ps is used at each time point. library(\"twang\") #> To reproduce results from prior versions of the twang package, please see the version=\"legacy\" option described in the documentation. data(\"iptwExWide\", package = \"twang\") (W2 <- weightitMSM(list(tx1 ~ age + gender + use0,                         tx2 ~ tx1 + use1 + age + gender + use0,                         tx3 ~ tx2 + use2 + tx1 + use1 + age + gender + use0),                    data = iptwExWide,                    method = my.ps)) #> A weightitMSM object #>  - method: \"my.ps\" (a user-defined method) #>  - number of obs.: 1000 #>  - sampling weights: none #>  - number of time points: 3 (tx1, tx2, tx3) #>  - treatment:  #>     + time 1: 2-category #>     + time 2: 2-category #>     + time 3: 2-category #>  - covariates:  #>     + baseline: age, gender, use0 #>     + after time 1: tx1, use1, age, gender, use0 #>     + after time 2: tx2, use2, tx1, use1, age, gender, use0 summary(W2) #>                  Summary of weights #>  #>                        Time 1                        #>                  Summary of weights #>  #> - Weight ranges: #>  #>            Min                                   Max #> treated 1.6667 |---|                         15.5292 #> control 2.6123 |---------------------------| 82.2349 #>  #> - Units with the 5 most extreme weights by group: #>                                                  #>              477     442     307     409     906 #>  treated 13.4464 13.6233 13.7059  14.436 15.5292 #>              206     282     641     547     980 #>  control 44.8027 46.1355 58.4164 79.8919 82.2349 #>  #> - Weight statistics: #>  #>         Coef of Var   MAD Entropy # Zeros #> treated       0.481 0.376   0.117       0 #> control       0.768 0.494   0.222       0 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted  294.     706.  #> Weighted    185.18   573.6 #>  #>                        Time 2                        #>                  Summary of weights #>  #> - Weight ranges: #>  #>            Min                                   Max #> treated 1.6667 |---------------------------| 82.2349 #> control 2.6123 |--------------------------|  79.8919 #>  #> - Units with the 5 most extreme weights by group: #>                                                  #>               34     561     553     641     980 #>  treated  33.371 34.7405 42.9442 58.4164 82.2349 #>              109      95     206     282     547 #>  control 39.7862 42.2256 44.8027 46.1355 79.8919 #>  #> - Weight statistics: #>  #>         Coef of Var   MAD Entropy # Zeros #> treated       0.960 0.653   0.336       0 #> control       0.737 0.384   0.159       0 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted   492.   508.   #> Weighted     318.9  264.49 #>  #>                        Time 3                        #>                  Summary of weights #>  #> - Weight ranges: #>  #>            Min                                   Max #> treated 1.6667 |-------------|               44.8027 #> control 2.6123 |---------------------------| 82.2349 #>  #> - Units with the 5 most extreme weights by group: #>                                                  #>              936     650     763     109     206 #>  treated  26.003  26.471 28.0582 39.7862 44.8027 #>              553     282     641     547     980 #>  control 42.9442 46.1355 58.4164 79.8919 82.2349 #>  #> - Weight statistics: #>  #>         Coef of Var   MAD Entropy # Zeros #> treated       0.773 0.565   0.245       0 #> control       0.873 0.469   0.227       0 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted  415.     585.  #> Weighted    235.67   366.4 #>  bal.tab(W2) #> Call #>  weightitMSM(formula.list = list(tx1 ~ age + gender + use0, tx2 ~  #>     tx1 + use1 + age + gender + use0, tx3 ~ tx2 + use2 + tx1 +  #>     use1 + age + gender + use0), data = iptwExWide, method = my.ps) #>  #> Balance summary across all time points #>              Times     Type Max.Diff.Adj #> prop.score 1, 2, 3 Distance       0.0251 #> age        1, 2, 3  Contin.       0.0703 #> gender     1, 2, 3   Binary       0.0263 #> use0       1, 2, 3  Contin.       0.0558 #> tx1           2, 3   Binary       0.0171 #> use1          2, 3  Contin.       0.0316 #> tx2              3   Binary       0.0085 #> use2             3  Contin.       0.0315 #>  #> Effective sample sizes #>  - Time 1 #>            Control Treated #> Unadjusted  294.     706.  #> Adjusted    185.18   573.6 #>  - Time 2 #>            Control Treated #> Unadjusted   492.   508.   #> Adjusted     318.9  264.49 #>  - Time 3 #>            Control Treated #> Unadjusted  415.     585.  #> Adjusted    235.67   366.4 # Kernel balancing using the KBAL package, available # using devtools::install_github(\"chadhazlett/KBAL\"). # Only the ATT and ATC are available. Use 'kbal.method' # instead of 'method' in weightit() to choose between # \"ebal\" and \"el\".  if (FALSE) { kbal.fun <- function(treat, covs, estimand, focal, ...) {     args <- list(...)     if (is_not_null(focal))         treat <- as.numeric(treat == focal)     else if (estimand != \"ATT\")         stop(\"estimand must be 'ATT' or 'ATC'.\", call. = FALSE)     if (\"kbal.method\" %in% names(args)) {         names(args)[names(args) == \"kbal.method\"] <- \"method\"     }     args[!names(args) %in% setdiff(names(formals(KBAL::kbal)),         c(\"X\", \"D\"))] <- NULL     k.out <- do.call(KBAL::kbal, c(list(X = covs, D = treat),         args))     w <- k.out$w     return(list(w = w)) }  (Wk <- weightit(treat ~ age + educ + married +                 nodegree + re74, data = lalonde,                 method = kbal.fun, estimand = \"ATT\",                 kbal.method = \"ebal\")) summary(Wk) bal.tab(Wk, disp.ks = TRUE) }"},{"path":"https://ngreifer.github.io/WeightIt/reference/sbps.html","id":null,"dir":"Reference","previous_headings":"","what":"Subgroup Balancing Propensity Score — sbps","title":"Subgroup Balancing Propensity Score — sbps","text":"Implements subgroup balancing propensity score (SBPS), algorithm attempts achieve balance subgroups sharing information overall sample subgroups. (Dong, Zhang, Zeng, & Li, 2020; DZZL). subgroup can use either weights estimated using whole sample, weights estimated using just subgroup, combination two. optimal combination chosen minimizes imbalance criterion includes subgroup well overall balance.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/sbps.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Subgroup Balancing Propensity Score — sbps","text":"","code":"sbps(obj, obj2 = NULL,      moderator = NULL,      formula = NULL,      data = NULL,      smooth = FALSE,      full.search)  # S3 method for weightit.sbps print(x, ...)  # S3 method for weightit.sbps summary(object, top = 5,         ignore.s.weights = FALSE, ...)  # S3 method for summary.weightit.sbps print(x, ...)"},{"path":"https://ngreifer.github.io/WeightIt/reference/sbps.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Subgroup Balancing Propensity Score — sbps","text":"obj weightit object containing weights estimated overall sample. obj2 weightit object containing weights estimated subgroups. Typically estimated including call weightit(). Either obj2 moderator must specified. moderator optional; string containing name variable data weighting done within subgroups one-sided formula subgrouping variable right-hand side. argument analogous argument weightit(), fact passed . Either obj2 moderator must specified. formula optional formula covariates balance optimized. specified, formula obj$call used. data optional data set form data frame contains variables formula moderator. smooth logical; whether smooth version SBPS used. compatible weightit methods return propensity score. full.search logical; smooth = FALSE, whether every combination subgroup overall weights evaluated. FALSE, stochastic search described DZZL used instead. TRUE, 2^R combinations checked, R number subgroups, can take long time many subgroups. unspecified, default TRUE R <= 8 FALSE otherwise. x weightit.sbps summary.weightit.sbps object; output call sbps() summary.weightit.sbps(). object weightit.sbps object; output call sbps(). top many largest smallest weights display. Default 5. ignore.s.weights whether ignore sampling weights computing weight summary. FALSE, default, estimated weights multiplied sampling weights () values computed. ... print, arguments passed print(). Ignored otherwise.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/sbps.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Subgroup Balancing Propensity Score — sbps","text":"SBPS relies two sets weights: one estimated overall sample one estimated within subgroup. algorithm decides whether subgroup use weights estimated overall sample estimated subgroup. 2^R permutations overall subgroup weights, R number subgroups. optimal permutation chosen minimizes balance criterion described DZZL. balance criterion used , binary multinomial treatments, sum squared standardized mean differences within subgroups overall, computed using col_w_smd() cobalt, continuous treatments, sum squared correlations covariate treatment within subgroups overall, computed using col_w_corr() cobalt. smooth version estimates weights determine relative contribution overall subgroup propensity scores weighted average propensity score subgroup. P_O propensity scores estimated overall sample P_S propensity scores estimated subgroup, smooth SBPS finds R coefficients C subgroup, ultimate propensity score \\(C*P_S + (1-C)*P_O\\), weights computed propensity score. coefficients estimated using optim() method = \"L-BFGS-B\". C estimated 1 0 subgroup, smooth SBPS coincides standard SBPS. obj2 specified moderator , sbps() attempt refit model specified obj moderator argument. relies environment obj created intact can take time obj hard fit. safer estimate obj obj2 (latter simply including moderator argument) supply sbps().","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/sbps.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Subgroup Balancing Propensity Score — sbps","text":"weightit.sbps object, inherits weightit. contains information obj weights, propensity scores, call, possibly covariates updated sbps(). addition, prop.subgroup component contains values coefficients C subgroups (either 0 1 standard SBPS), moderator component contains data.frame moderator. object summary methods compatible cobalt functions. cluster argument used cobalt functions accurately reflect performance weights balancing subgroups.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/sbps.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Subgroup Balancing Propensity Score — sbps","text":"Dong, J., Zhang, J. L., Zeng, S., & Li, F. (2020). Subgroup balancing propensity score. Statistical Methods Medical Research, 29(3), 659–676. doi:10.1177/0962280219870836","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/sbps.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Subgroup Balancing Propensity Score — sbps","text":"Noah Greifer","code":""},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/reference/sbps.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Subgroup Balancing Propensity Score — sbps","text":"","code":"library(\"cobalt\") data(\"lalonde\", package = \"cobalt\")  #Balancing covariates between treatment groups within races (W1 <- weightit(treat ~ age + educ + married +                 nodegree + race + re74, data = lalonde,                 method = \"ps\", estimand = \"ATT\")) #> A weightit object #>  - method: \"ps\" (propensity score weighting) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 2-category #>  - estimand: ATT (focal: 1) #>  - covariates: age, educ, married, nodegree, race, re74  (W2 <- weightit(treat ~ age + educ + married +                 nodegree + race + re74, data = lalonde,                 method = \"ps\", estimand = \"ATT\",                 by = \"race\")) #> A weightit object #>  - method: \"ps\" (propensity score weighting) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 2-category #>  - estimand: ATT (focal: 1) #>  - covariates: age, educ, married, nodegree, race, re74 #>  - by: race S <- sbps(W1, W2) print(S) #> A weightit.sbps object #>  - method: \"ps\" (propensity score weighting) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 2-category #>  - estimand: ATT (focal: 1) #>  - covariates: age, educ, married, nodegree, race, re74 #>  - moderator: race (3 subgroups) summary(S) #> Summary of weights: #>  #>  - Overall vs. subgroup proportion contribution: #>          race = black race = hispan race = white #> Overall             0             0            0 #> Subgroup            1             1            1 #>  #>  - - - - - - - Subgroup race = black - - - - - - - #> - Weight ranges: #>           Min                                  Max #> treated 1.000      ||                       1.0000 #> control 0.466 |---------------------------| 3.5903 #>  #> - Units with 5 greatest weights by group: #>                                            #>               1      2     3      4      5 #>  treated      1      1     1      1      1 #>             221    228   188    185    174 #>  control 2.9494 2.9494 3.006 3.0637 3.5903 #>  #>          Ratio Coef of Var #> treated 1.0000      0.0000 #> control 7.7042      0.4250 #> overall 7.7042      0.4616 #>  #> - Effective Sample Sizes: #>            Control Treated #> Unweighted  87.000     156 #> Weighted    73.818     156 #>  #>  - - - - - - - Subgroup race = hispan - - - - - - - #> - Weight ranges: #>            Min                                    Max #> treated 1.0000                              || 1.0000 #> control 0.0209   |------------|                0.5046 #>  #> - Units with 5 greatest weights by group: #>                                             #>               2      3      4      5      7 #>  treated      1      1      1      1      1 #>              56     54     49     48     47 #>  control 0.4117 0.4767 0.4835 0.4968 0.5046 #>  #>           Ratio Coef of Var #> treated  1.0000      0.0000 #> control 24.1741      0.7143 #> overall 47.9120      1.0352 #>  #> - Effective Sample Sizes: #>            Control Treated #> Unweighted  61.000      11 #> Weighted    40.616      11 #>  #>  - - - - - - - Subgroup race = white - - - - - - - #> - Weight ranges: #>            Min                                   Max #> treated 1.0000                              || 1.000 #> control 0.0002   |---------|                   0.385 #>  #> - Units with 5 greatest weights by group: #>                                            #>               1      2      3      4     5 #>  treated      1      1      1      1     1 #>             289    287    285    280   267 #>  control 0.2393 0.2699 0.2937 0.2956 0.385 #>  #>            Ratio Coef of Var #> treated    1.000      0.0000 #> control 1825.568      1.1538 #> overall 4742.156      1.9499 #>  #> - Effective Sample Sizes: #>            Control Treated #> Unweighted 281.000      18 #> Weighted   120.777      18 bal.tab(S, cluster = \"race\") #> Call #>  sbps(obj = W1, obj2 = W2) #>  #> Balance by cluster #>  #>  - - - Cluster: black - - -  #> Balance Measures #>                 Type Diff.Adj #> prop.score  Distance   0.0016 #> age          Contin.   0.0126 #> educ         Contin.  -0.0332 #> married       Binary   0.0030 #> nodegree      Binary   0.0062 #> race_black    Binary   0.0000 #> race_hispan   Binary   0.0000 #> race_white    Binary   0.0000 #> re74         Contin.  -0.0826 #>  #> Effective sample sizes #>            Control Treated #> Unadjusted   87.       156 #> Adjusted     73.82     156 #>  #>  - - - Cluster: hispan - - -  #> Balance Measures #>                 Type Diff.Adj #> prop.score  Distance  -0.2678 #> age          Contin.   0.1196 #> educ         Contin.  -0.0756 #> married       Binary   0.0217 #> nodegree      Binary   0.0018 #> race_black    Binary   0.0000 #> race_hispan   Binary   0.0000 #> race_white    Binary   0.0000 #> re74         Contin.   0.0114 #>  #> Effective sample sizes #>            Control Treated #> Unadjusted   61.        11 #> Adjusted     40.62      11 #>  #>  - - - Cluster: white - - -  #> Balance Measures #>                 Type Diff.Adj #> prop.score  Distance   0.0652 #> age          Contin.   0.0191 #> educ         Contin.  -0.0185 #> married       Binary  -0.0015 #> nodegree      Binary   0.0039 #> race_black    Binary   0.0000 #> race_hispan   Binary   0.0000 #> race_white    Binary   0.0000 #> re74         Contin.  -0.0117 #>  #> Effective sample sizes #>            Control Treated #> Unadjusted  281.        18 #> Adjusted    120.78      18 #>  - - - - - - - - - - - - - -  #>   #Could also have run #  sbps(W1, moderator = \"race\")  S_ <- sbps(W1, W2, smooth = TRUE) print(S_) #> A weightit.sbps object #>  - method: \"ps\" (propensity score weighting) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 2-category #>  - estimand: ATT (focal: 1) #>  - covariates: age, educ, married, nodegree, race, re74 #>  - moderator: race (3 subgroups) summary(S_) #> Summary of weights: #>  #>  - Overall vs. subgroup proportion contribution: #>          race = black race = hispan race = white #> Overall          0.17          0.25            0 #> Subgroup         0.83          0.75            1 #>  #>  - - - - - - - Subgroup race = black - - - - - - - #> - Weight ranges: #>            Min                                  Max #> treated 1.0000      ||                       1.0000 #> control 0.4654 |---------------------------| 3.5703 #>  #> - Units with 5 greatest weights by group: #>                                             #>               1      2      4      5      6 #>  treated      1      1      1      1      1 #>             221    228    188    185    174 #>  control 2.9787 2.9787 3.0338 3.0899 3.5703 #>  #>          Ratio Coef of Var #> treated 1.0000      0.0000 #> control 7.6708      0.4264 #> overall 7.6708      0.4625 #>  #> - Effective Sample Sizes: #>            Control Treated #> Unweighted  87.000     156 #> Weighted    73.744     156 #>  #>  - - - - - - - Subgroup race = hispan - - - - - - - #> - Weight ranges: #>            Min                                    Max #> treated 1.0000                              || 1.0000 #> control 0.0254   |-----------|                 0.4743 #>  #> - Units with 5 greatest weights by group: #>                                             #>               1      2      4      5      6 #>  treated      1      1      1      1      1 #>              56     54     48     47     28 #>  control 0.3908 0.4496 0.4557 0.4704 0.4743 #>  #>           Ratio Coef of Var #> treated  1.0000      0.0000 #> control 18.6516      0.6795 #> overall 39.3245      1.0314 #>  #> - Effective Sample Sizes: #>            Control Treated #> Unweighted   61.00      11 #> Weighted     41.95      11 #>  #>  - - - - - - - Subgroup race = white - - - - - - - #> - Weight ranges: #>            Min                                   Max #> treated 1.0000                              || 1.000 #> control 0.0002   |---------|                   0.385 #>  #> - Units with 5 greatest weights by group: #>                                            #>               1      2      3      4     5 #>  treated      1      1      1      1     1 #>             289    287    285    280   267 #>  control 0.2393 0.2699 0.2937 0.2956 0.385 #>  #>            Ratio Coef of Var #> treated    1.000      0.0000 #> control 1825.568      1.1538 #> overall 4742.156      1.9499 #>  #> - Effective Sample Sizes: #>            Control Treated #> Unweighted 281.000      18 #> Weighted   120.777      18 bal.tab(S_, cluster = \"race\") #> Call #>  sbps(obj = W1, obj2 = W2, smooth = TRUE) #>  #> Balance by cluster #>  #>  - - - Cluster: black - - -  #> Balance Measures #>                 Type Diff.Adj #> prop.score  Distance   0.0019 #> age          Contin.   0.0388 #> educ         Contin.  -0.0305 #> married       Binary   0.0096 #> nodegree      Binary   0.0086 #> race_black    Binary   0.0000 #> race_hispan   Binary   0.0000 #> race_white    Binary   0.0000 #> re74         Contin.  -0.0561 #>  #> Effective sample sizes #>            Control Treated #> Unadjusted   87.       156 #> Adjusted     73.74     156 #>  #>  - - - Cluster: hispan - - -  #> Balance Measures #>                 Type Diff.Adj #> prop.score  Distance  -0.1909 #> age          Contin.   0.0167 #> educ         Contin.  -0.0654 #> married       Binary   0.0314 #> nodegree      Binary   0.0084 #> race_black    Binary   0.0000 #> race_hispan   Binary   0.0000 #> race_white    Binary   0.0000 #> re74         Contin.  -0.0175 #>  #> Effective sample sizes #>            Control Treated #> Unadjusted   61.        11 #> Adjusted     41.95      11 #>  #>  - - - Cluster: white - - -  #> Balance Measures #>                 Type Diff.Adj #> prop.score  Distance   0.0652 #> age          Contin.   0.0191 #> educ         Contin.  -0.0185 #> married       Binary  -0.0015 #> nodegree      Binary   0.0039 #> race_black    Binary   0.0000 #> race_hispan   Binary   0.0000 #> race_white    Binary   0.0000 #> re74         Contin.  -0.0117 #>  #> Effective sample sizes #>            Control Treated #> Unadjusted  281.        18 #> Adjusted    120.78      18 #>  - - - - - - - - - - - - - -  #>"},{"path":"https://ngreifer.github.io/WeightIt/reference/stop.method.html","id":null,"dir":"Reference","previous_headings":"","what":"Balance criteria for tuning — stop.method","title":"Balance criteria for tuning — stop.method","text":"methods involve tuning parameters use measure covariate balance criterion select optimal parameter values. example method = \"gbm\", balance criterion can used select optimal number trees use. WeightIt, argument stop.method controls criterion optimized, using naming conventions originally used twang. page documents available arguments stop.method. research balance criteria perform better others; see References section articles. course, method fares given dataset depends variety factors, multiple methods tried compared moving forward set weights. treatment type balance criteria available.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/stop.method.html","id":"binary-treatments","dir":"Reference","previous_headings":"","what":"Binary Treatments","title":"Balance criteria for tuning — stop.method","text":"\"es.mean\", \"es.max\", \"es.rms\" average, maximum, root mean squared absolute standardized mean difference (ASMD) among covariates, respectively. ASMD computed using col_w_smd() cobalt. covariates standardized, including binary covariates (note cobalt, raw mean differences used binary variables default). standardization factor (.e., s.d.denom) depends estimand requested follows conventions used cobalt. \"ks.mean\", \"ks.max\", \"ks.rms\" average, maximum, root mean squared Kolmogorov-Smirnov (KS) statistic among covariates, respectively. KS statistic computed using col_w_ks() cobalt. \"mahalanobis\" sample Mahalanobis distance weighted sample. similar \"es.rms\" except variables redundant downweighted. sample Mahalanobis distance computed using (generalized) inverse unweighted covariance matrix computed focal group estimand ATT ATC using (generalized) inverse average unweighted covariance matrices computed within treatment group (analogous ASMD, uses average group variances denominator). \"energy.dist\" energy distance weighted samples, described Huling & Mak (2022). \"improved\" energy distance used estimand ATT. (Note weights directly minimizing energy distance can found using method = \"energy\".) \"r2\", \"r2.2\", \"r2.3\" \"r2\", pseudo-R2 logistic regression treatment covariates weights applied. \"r2.2\" \"r2.3\" include squares cubes covariates model well. Franklin et al. (2014) consider similar metric, post-matching C-statistic, pseudo-R2 accomplishes goal without requiring decision boundary. pseudo-R2 used McKelvy & Zavoina pseudo-R2. \"L1.med\" L1 statistic weighted samples, half average absolute difference proportion categories multidimensional histogram formed coarsening covariates. coarsening used one yields median unweighted L1 statistic among 101 random coarsenings data. continuous covariate coarsened 2 12 bins, categorical covariate combined 2 12 levels (however many levels available). coarsening random, seed set ensure results replicable.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/stop.method.html","id":"multinomial-treatments","dir":"Reference","previous_headings":"","what":"Multinomial Treatments","title":"Balance criteria for tuning — stop.method","text":"\"es.mean\", \"es.max\", \"es.rms\" average, maximum, root mean squared absolute standardized mean difference (ASMD) among covariates, respectively, across pairs treatments. ASMD computed using col_w_smd() cobalt. covariates standardized, including binary covariates (note cobalt, raw mean differences used binary variables default). standardization factor (.e., s.d.denom) depends estimand requested follows conventions used cobalt. standardization factor used across treatment pairs covariate. estimand ATT, differences focal group group computed. \"ks.mean\", \"ks.max\", \"ks.rms\" average, maximum, root mean squared Kolmogorov-Smirnov (KS) statistic among covariates, respectively, across pairs treatments. KS statistic computed using col_w_ks() cobalt. estimand ATT, differences focal group group computed. \"energy.dist\" total energy distance among weighted samples, described Huling & Mak (2020). \"improved\" energy distance used estimand ATT. (Note weights directly minimizing energy distance can found using method = \"energy\".) \"L1.med\" L1 statistic weighted samples, average absolute difference proportion categories multidimensional histogram formed coarsening covariates, divided number treatment levels. coarsening used one yields median unweighted L1 statistic among 101 random coarsenings data. continuous covariate coarsened 2 12 bins, categorical covariate combined 2 12 levels (however many levels available). coarsening random, seed set ensure results replicable.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/stop.method.html","id":"continuous-treatments","dir":"Reference","previous_headings":"","what":"Continuous Treatments","title":"Balance criteria for tuning — stop.method","text":"\"p.mean\", \"p.max\", \"p.rms\" average, maximum, root mean squared absolute Pearson correlation treatment covariates, respectively. Pearson correlation computed using col_w_cov() cobalt. correlation uses unweighted standard deviations treatment covariates denominator. \"s.mean\", \"s.max\", \"s.rms\" average, maximum, root mean squared absolute Spearman correlation treatment covariates, respectively. Spearman correlation computed using col_w_cov() cobalt. correlation uses unweighted standard deviations rank-transformed treatment covariates denominator. \"r2\", \"r2.2\", \"r2.3\" \"r2\", model R2 linear regression treatment covariates weights applied. standard R2 used. \"r2.2\" \"r2.3\" include squares cubes covariates model well. \"L1.med\" L1 statistic weighted samples, average absolute difference proportion categories multidimensional histogram formed coarsening covariates treatment, divided number levels coarsened treatment. coarsening used one yields median unweighted L1 statistic among 101 random coarsenings data. treatment continuous covariate coarsened 2 12 bins, categorical covariate combined 2 12 levels (however many levels available). coarsening random, seed set ensure results replicable. \"distance.cov\" distance covariance treatment covariates weights applied described Huling, Greifer, Chen (2021). Unlike method = \"energy\", energy distance treatment covariates weighted sample tretament covariates original sample included.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/stop.method.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Balance criteria for tuning — stop.method","text":"Ali, M. S., Groenwold, R. H. H., Pestman, W. R., Belitser, S. V., Roes, K. C. B., Hoes, . W., de Boer, ., & Klungel, O. H. (2014). Propensity score balance measures pharmacoepidemiology: simulation study. Pharmacoepidemiology Drug Safety, 23(8), 802–811. doi:10.1002/pds.3574 Belitser, S. V., Martens, E. P., Pestman, W. R., Groenwold, R. H. H., de Boer, ., & Klungel, O. H. (2011). Measuring balance model selection propensity score methods. Pharmacoepidemiology Drug Safety, 20(11), 1115–1129. doi:10.1002/pds.2188 Franklin, J. M., Rassen, J. ., Ackermann, D., Bartels, D. B., & Schneeweiss, S. (2014). Metrics covariate balance cohort studies causal effects. Statistics Medicine, 33(10), 1685–1699. doi:10.1002/sim.6058 Huling, J. D., Greifer, N., & Chen, G. (2021). Independence weights causal inference continuous exposures (arXiv:2107.07086). arXiv. https://doi.org/10.48550/arXiv.2107.07086 Huling, J. D., & Mak, S. (2022). Energy Balancing Covariate Distributions. ArXiv:2004.13962 [Stat]. https://arxiv.org/abs/2004.13962 Griffin, B. ., McCaffrey, D. F., Almirall, D., Burgette, L. F., & Setodji, C. M. (2017). Chasing Balance Recommendations Improving Nonparametric Propensity Score Models. Journal Causal Inference, 5(2). doi:10.1515/jci-2015-0026","code":""},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/reference/summary.weightit.html","id":null,"dir":"Reference","previous_headings":"","what":"Print and Summarize Output — summary.weightit","title":"Print and Summarize Output — summary.weightit","text":"summary() generates summary weightit weightitMSM object evaluate properties estimated weights. plot() plots distribution weights.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/summary.weightit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print and Summarize Output — summary.weightit","text":"","code":"# S3 method for weightit summary(object, top = 5,         ignore.s.weights = FALSE, ...)  # S3 method for summary.weightit print(x, ...)  # S3 method for summary.weightit plot(x, binwidth = NULL, bins = NULL, ...)  # S3 method for weightitMSM summary(object, top = 5,         ignore.s.weights = FALSE, ...)  # S3 method for summary.weightitMSM print(x, ...)"},{"path":"https://ngreifer.github.io/WeightIt/reference/summary.weightit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print and Summarize Output — summary.weightit","text":"object weightit weightitMSM object; output call weightit() weightitMSM(). top many largest smallest weights display. Default 5. ignore.s.weights whether ignore sampling weights computing weight summary. FALSE, default, estimated weights multiplied sampling weights () values computed. binwidth, bins arguments passed ggplot2::geom_histogram() control size /number bins. x summary.weightit summary.weightitMSM object; output call summary.weightit() summary.weightitMSM(). ... print(), arguments passed print(). plot(), additional arguments passed graphics::hist() determine number bins, though ggplot2::geom_histogram() ggplot2 actually used create plot.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/summary.weightit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Print and Summarize Output — summary.weightit","text":"point treatments (.e., weightit objects), summary.weightit object following elements: weight.range range (minimum maximum) weight treatment group. weight.top units greatest weights treatment group; many included determined top. coef..var (Coef Var) coefficient variation (standard deviation divided mean) weights treatment group overall. scaled.mad (MAD) mean absolute deviation weights treatment group overall divided mean weights corresponding group. negative entropy (Entropy) negative entropy (\\(\\sum w log(w)\\)) weights treatment group overall divided mean weights corresponding group. num.zeros number weights equal zero. effective.sample.size effective sample size treatment group weighting. See ESS(). longitudinal treatments (.e., weightitMSM objects), list elements treatment period. plot() returns ggplot object histogram displaying distribution estimated weights. estimand ATT ATC, weights non-focal group(s) displayed (since weights focal group 1). dotted line displayed mean weights.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/summary.weightit.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Print and Summarize Output — summary.weightit","text":"Noah Greifer","code":""},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/reference/summary.weightit.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Print and Summarize Output — summary.weightit","text":"","code":"# See example at ?weightit or ?weightitMSM"},{"path":"https://ngreifer.github.io/WeightIt/reference/trim.html","id":null,"dir":"Reference","previous_headings":"","what":"Trim (Winsorize) Large Weights — trim","title":"Trim (Winsorize) Large Weights — trim","text":"Trims (.e., winsorizes) large weights setting weights higher given quantile weight quantile. can useful controlling extreme weights, can reduce effective sample size enlarging variability weights. Note observations fully discarded using trim(), may differ uses word \"trim\".","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/trim.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Trim (Winsorize) Large Weights — trim","text":"","code":"# S3 method for weightit trim(w, at = 0, lower = FALSE, ...)  # S3 method for numeric trim(w, at = 0, lower = FALSE, treat = NULL, ...)"},{"path":"https://ngreifer.github.io/WeightIt/reference/trim.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Trim (Winsorize) Large Weights — trim","text":"w weightit object vector weights. numeric; either quantile weights weights trimmed. single number .5 1, number weights trimmed (e.g., = 3 top 3 weights set 4th largest weight). lower logical; whether also trim lower quantile (e.g., = .9, trimming .1 .9, = 3, trimming top bottom 3 weights). treat vector treatment status unit. always included w numeric, can get away leaving treatment continuous estimand ATE binary multi-category treatments. ... used.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/trim.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Trim (Winsorize) Large Weights — trim","text":"trim() takes weightit object (output call weightit() weightitMSM()) numeric vector weights trims (winsorizes) specified quantile. weights quantile set weight quantile. lower = TRUE, weights 1 minus quantile set weight 1 minus quantile. general, trimming weights decreases balance also decreases variability weights, improving precision potential expense unbiasedness (Cole & Hernán, 2008). See Lee, Lessler, Stuart (2011) Thoemmes Ong (2015) discussions simulation results trimming weights various quantiles. Note trimming weights can also change target population therefore estimand. using trim() numeric vector weights, helpful include treatment vector well. helps determine type treatment estimand, used specify trimming performed. particular, estimand determined ATT ATC, weights target (.e., focal) group ignored, since equal 1. Otherwise, estimand ATE treatment continuous, weights considered trimming. general, weights group weights considered trimming.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/trim.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Trim (Winsorize) Large Weights — trim","text":"input weightit object, output weightit object weights replaced trimmed weights additional attribute, \"trim\", equal quantile trimming. input numeric vector weights, output numeric vector trimmed weights, aforementioned attribute.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/trim.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Trim (Winsorize) Large Weights — trim","text":"Cole, S. R., & Hernán, M. Á. (2008). Constructing Inverse Probability Weights Marginal Structural Models. American Journal Epidemiology, 168(6), 656–664. Lee, B. K., Lessler, J., & Stuart, E. . (2011). Weight Trimming Propensity Score Weighting. PLoS ONE, 6(3), e18174. Thoemmes, F., & Ong, . D. (2016). Primer Inverse Probability Treatment Weighting Marginal Structural Models. Emerging Adulthood, 4(1), 40–59.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/trim.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Trim (Winsorize) Large Weights — trim","text":"Noah Greifer","code":""},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/reference/trim.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Trim (Winsorize) Large Weights — trim","text":"","code":"library(\"cobalt\") data(\"lalonde\", package = \"cobalt\")  (W <- weightit(treat ~ age + educ + married +                  nodegree + re74, data = lalonde,                method = \"ps\", estimand = \"ATT\")) #> A weightit object #>  - method: \"ps\" (propensity score weighting) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 2-category #>  - estimand: ATT (focal: 1) #>  - covariates: age, educ, married, nodegree, re74 summary(W) #>                  Summary of weights #>  #> - Weight ranges: #>  #>            Min                                  Max #> treated 1.0000               ||              1.0000 #> control 0.0222 |---------------------------| 2.0438 #>  #> - Units with the 5 most extreme weights by group: #>                                             #>              10      8      4      3      1 #>  treated      1      1      1      1      1 #>             411    595    269    409    296 #>  control 1.3303 1.4365 1.5005 1.6369 2.0438 #>  #> - Weight statistics: #>  #>         Coef of Var   MAD Entropy # Zeros #> treated       0.000 0.000   -0.00       0 #> control       0.823 0.701    0.33       0 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted  429.       185 #> Weighted    255.99     185  #Trimming the top and bottom 5 weights trim(W, at = 5, lower = TRUE) #> Trimming the top and bottom 5 weights where treat is not 1. #> A weightit object #>  - method: \"ps\" (propensity score weighting) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 2-category #>  - estimand: ATT (focal: 1) #>  - covariates: age, educ, married, nodegree, re74 #>  - weights trimmed at the top and bottom 5   #Trimming at 90th percentile (W.trim <- trim(W, at = .9)) #> Trimming weights where treat is not 1 to 90%. #> A weightit object #>  - method: \"ps\" (propensity score weighting) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 2-category #>  - estimand: ATT (focal: 1) #>  - covariates: age, educ, married, nodegree, re74 #>  - weights trimmed at 90%   summary(W.trim) #>                  Summary of weights #>  #> - Weight ranges: #>  #>            Min                                    Max #> treated 1.0000                              || 1.0000 #> control 0.0222   |-------------------------|   0.9407 #>  #> - Units with the 5 most extreme weights by group: #>                                             #>              10      8      4      3      1 #>  treated      1      1      1      1      1 #>             303    296    285    269    264 #>  control 0.9407 0.9407 0.9407 0.9407 0.9407 #>  #> - Weight statistics: #>  #>         Coef of Var   MAD Entropy # Zeros #> treated       0.000 0.000  -0.000       0 #> control       0.766 0.682   0.303       0 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted  429.       185 #> Weighted    270.58     185 #Note that only the control weights were trimmed  #Trimming a numeric vector of weights all.equal(trim(W$weights, at = .9, treat = lalonde$treat),           W.trim$weights) #> Trimming weights where treat is not 1 to 90%. #> [1] TRUE  #Using made up data and as.weightit() treat <- rbinom(500, 1, .3) weights <- rchisq(500, df = 2) W <- as.weightit(weights = weights, treat = treat,                  estimand = \"ATE\") summary(W) #>                  Summary of weights #>  #> - Weight ranges: #>  #>            Min                                   Max #> treated 0.0782 |-----------------|            7.3680 #> control 0.0030 |---------------------------| 11.3178 #>  #> - Units with the 5 most extreme weights by group: #>                                              #>             203    333    436    335     103 #>  treated 5.5532 6.1825 6.2122 7.1462   7.368 #>             408    337    201    196      45 #>  control 8.8746 8.9234 9.7306 9.8349 11.3178 #>  #> - Weight statistics: #>  #>         Coef of Var   MAD Entropy # Zeros #> treated       0.808 0.640   0.309       0 #> control       0.929 0.714   0.391       0 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted  370.    130.   #> Weighted    198.76   78.93 summary(trim(W, at = .95)) #> Trimming weights to 95%. #>                  Summary of weights #>  #> - Weight ranges: #>  #>            Min                                  Max #> treated 0.0782 |---------------------------| 6.1935 #> control 0.0030 |---------------------------| 6.1935 #>  #> - Units with the 5 most extreme weights by group: #>                                             #>             203    333    436    335    103 #>  treated 5.5532 6.1825 6.1935 6.1935 6.1935 #>             152    131    115    114     45 #>  control 6.1935 6.1935 6.1935 6.1935 6.1935 #>  #> - Weight statistics: #>  #>         Coef of Var   MAD Entropy # Zeros #> treated       0.790 0.635   0.301       0 #> control       0.848 0.692   0.354       0 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted  370.    130.   #> Weighted    215.45   80.28"},{"path":"https://ngreifer.github.io/WeightIt/reference/weightit.fit.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Balancing Weights with Minimal Input Processing — weightit.fit","title":"Generate Balancing Weights with Minimal Input Processing — weightit.fit","text":"weightit.fit() dispatches one weight estimation methods determined method. internal function called weightit() probably used except special cases. Unlike weightit(), weightit.fit() accept formula data frame interface instead requires covariates treatment supplied numeric matrix atomic vector, respectively. way, weightit.fit() weightit() lm.fit() lm() - thinner, slightly faster interface performs minimal argument checking.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/weightit.fit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Balancing Weights with Minimal Input Processing — weightit.fit","text":"","code":"weightit.fit(covs,              treat,              method = \"ps\",              s.weights = NULL,              by.factor = NULL,              estimand = \"ATE\",              focal = NULL,              stabilize = FALSE,              ps = NULL,              moments = NULL,              int = FALSE,              subclass = NULL,              is.MSM.method = FALSE,              missing = NULL,              verbose = FALSE,              include.obj = FALSE,              ...)"},{"path":"https://ngreifer.github.io/WeightIt/reference/weightit.fit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Balancing Weights with Minimal Input Processing — weightit.fit","text":"covs numeric matrix covariates. treat vector treatment statuses. method string length 1 containing name method used estimate weights. See weightit() allowable options. default \"ps\" propensity score weighting. s.weights numeric vector sampling weights. See individual pages method information whether sampling weights can supplied. .factor factor variable weighting done within levels. Corresponds argument weightit(). estimand desired estimand. binary multi-category treatments, can \"ATE\", \"ATT\", \"ATC\", , methods, \"ATO\", \"ATM\", \"ATOS\". default \"ATE\". argument ignored continuous treatments. See individual pages method information estimands allowed method literature read interpret estimands. stabilize logical; whether stabilize weights. methods involve estimating propensity scores, involves multiplying unit's weight proportion units treatment group. Default FALSE. focal multi-category treatments used ATT weights requested, group consider \"treated\" focal group. group weighted, groups weighted like focal group. Must non-NULL estimand = \"ATT\" \"ATC\". ps vector propensity scores. specified, method ignored set \"ps\". moments, int, subclass arguments customize weight estimation. See weightit() details. .MSM.method see weightitMSM(). Typically can ignored. missing character; missing data handled. options depend method used. NULL, covs covs checked NA values, present, missing set \"ind\". \"\", covs covs checked NA values; can faster known none. verbose whether print additional information output fitting function. include.obj whether include output fit objects created process estimating weights. example, method = \"ps\", glm objects containing propensity score model included. See individual pages method information object included TRUE. ... arguments functions called weightit.fit() control aspects fitting covered arguments.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/weightit.fit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Balancing Weights with Minimal Input Processing — weightit.fit","text":"weightit.fit object following elements: weights estimated weights, one unit. ps estimated provided propensity scores. Estimated propensity scores returned binary treatments method \"ps\", \"gbm\", \"cbps\", \"super\", \"bart\". fit.obj include.obj = TRUE, fit object. info Additional information fitting. See individual methods pages included. weightit.fit object specialized print(), summary(), plot() methods. simply list containing components.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/weightit.fit.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Generate Balancing Weights with Minimal Input Processing — weightit.fit","text":"weightit.fit() called weightit() arguments weightit() checked processed. weightit.fit() dispatches function used actually estimate weights, passing supplied arguments directly. weightit.fit() meant used anyone experienced users specific use case mind. returned object contain information supplied arguments details estimation method; processed weightit. Less argument checking processing occurs weightit.fit() weightit(), means supplying incorrect arguments can result errors, crashes, invalid weights, error warning messages may helpful diagnosing problem. weightit.fit() check make sure weights actually estimated, though. weightit.fit() may useful speeding simulation simulation studies use weightit() covariates can supplied numeric matrix, often generated simulations, without go potentially slow process extracting covariates treatment formula data frame. user certain arguments valid (e.g., ensuring estimated weights consistent estimated weightit() arguments), less time needs spent processing arguments. Also, returned object much smaller weightit object covariates returned alongside weights.","code":""},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/reference/weightit.fit.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Generate Balancing Weights with Minimal Input Processing — weightit.fit","text":"Noah Greifer","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/weightit.fit.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate Balancing Weights with Minimal Input Processing — weightit.fit","text":"","code":"library(\"cobalt\") data(\"lalonde\", package = \"cobalt\")  #Balancing covariates between treatment groups (binary) covs_mat <- as.matrix(lalonde[c(\"age\", \"educ\", \"married\",                                 \"nodegree\", \"re74\", \"re75\")]) wf1 <- weightit.fit(covs_mat, treat = lalonde$treat,                     method = \"ps\", estimand = \"ATT\") str(wf1) #> List of 4 #>  $ weights: num [1:614] 1 1 1 1 1 1 1 1 1 1 ... #>  $ ps     : num [1:614] 0.296 0.462 0.391 0.558 0.483 ... #>  $ fit.obj: NULL #>  $ info   : Named list() #>  - attr(*, \"class\")= chr \"weightit.fit\""},{"path":"https://ngreifer.github.io/WeightIt/reference/weightit.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Balancing Weights — weightit","title":"Generate Balancing Weights — weightit","text":"weightit() allows easy generation balancing weights using variety available methods binary, continuous, multi-category treatments. Many methods exist packages, weightit() calls; packages must installed use desired method. Also included print() summary() methods examining output.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/weightit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Balancing Weights — weightit","text":"","code":"weightit(formula,          data = NULL,          method = \"ps\",          estimand = \"ATE\",          stabilize = FALSE,          focal = NULL,          by = NULL,          s.weights = NULL,          ps = NULL,          moments = NULL,          int = FALSE,          subclass = NULL,          missing = NULL,          verbose = FALSE,          include.obj = FALSE,          ...)  # S3 method for weightit print(x, ...)"},{"path":"https://ngreifer.github.io/WeightIt/reference/weightit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Balancing Weights — weightit","text":"formula formula treatment variable left hand side covariates balanced right hand side. See glm() details. Interactions functions covariates allowed. data optional data set form data frame contains variables formula. method string length 1 containing name method used estimate weights. See Details allowable options. default \"ps\" propensity score weighting. estimand desired estimand. binary multi-category treatments, can \"ATE\", \"ATT\", \"ATC\", , methods, \"ATO\", \"ATM\", \"ATOS\". default \"ATE\". argument ignored continuous treatments. See individual pages method information estimands allowed method literature read interpret estimands. stabilize logical; whether stabilize weights. methods involve estimating propensity scores, involves multiplying unit's weight proportion units treatment group. Default FALSE. focal multi-category treatments used ATT weights requested, group consider \"treated\" focal group. group weighted, groups weighted like focal group. specified, estimand automatically set \"ATT\". string containing name variable data weighting done within categories one-sided formula stratifying variable right-hand side. example, = \"gender\" = ~gender, weights generated separately within level variable \"gender\". argument used called exact, still work message. one variable allowed; stratify multiply variables simultaneously, create new variable full cross variables using interaction(). s.weights vector sampling weights name variable data contains sampling weights. can also matching weights weighting used matched data. See individual pages method information whether sampling weights can supplied. ps vector propensity scores name variable data containing propensity scores. NULL, method ignored, propensity scores used create weights. formula must include treatment variable data, listed covariates play role weight estimation. Using ps similar calling get_w_from_ps() directly, produces full weightit object rather just producing weights. moments numeric; methods, greatest power covariate balanced. example, moments = 3, non-categorical covariate, covariate, square, cube balanced. argument ignored methods; balance powers covariates, appropriate functions must entered formula. See individual pages method information whether accept moments. int logical; methods, whether first-order interactions covariates balanced. argument ignored methods; balance interactions variables, appropriate functions must entered formula. See individual pages method information whether accept int. subclass numeric; number subclasses use computing weights using marginal mean weighting subclasses (MMWS). NULL, standard inverse probability weights (extensions) computed; number greater 1, subclasses formed weights computed based subclass membership. Attempting set non-NULL value methods compute propensity score result error; see method's help page information whether MMWS weights compatible method. See get_w_from_ps() details references. missing character; missing data handled. options defaults depend method used. Ignored missing data present. noted multiple imputation outperforms available missingness methods available weightit() probably used instead. Consider MatchThem package use weightit() multiply imputed data. verbose logical; whether print additional information output fitting function. include.obj logical; whether include output fit objects created process estimating weights. example, method = \"ps\", glm objects containing propensity score model included. See individual pages method information object included TRUE. ... arguments functions called weightit() control aspects fitting covered arguments. See Details. x weightit object; output call weightit().","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/weightit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Balancing Weights — weightit","text":"weightit object following elements: weights estimated weights, one unit. treat values treatment variable. covs covariates used fitting. includes raw covariates, may altered fitting process. estimand estimand requested. method weight estimation method specified. ps estimated provided propensity scores. Estimated propensity scores returned binary treatments method \"ps\", \"gbm\", \"cbps\", \"super\", \"bart\". s.weights provided sampling weights. focal focal variable ATT requested multi-category treatment. data.frame containing variable specified. obj include.obj = TRUE, fit object. info Additional information fitting. See individual methods pages included.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/weightit.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Generate Balancing Weights — weightit","text":"primary purpose weightit() dispatcher functions perform estimation balancing weights using requested method. methods allowed links pages containing information , including additional arguments outputs (e.g., include.obj = TRUE), missing values treated, estimands allowed, whether sampling weights allowed. \"ps\" - Propensity score weighting using generalized linear models. \"gbm\" - Propensity score weighting using generalized boosted modeling. \"cbps\" - Covariate Balancing Propensity Score weighting. \"npcbps\" - Non-parametric Covariate Balancing Propensity Score weighting. \"ebal\" - Entropy balancing. \"optweight\" - Optimization-based weighting. \"super\" - Propensity score weighting using SuperLearner. \"bart\" - Propensity score weighting using Bayesian additive regression trees (BART). \"energy\" - Energy balancing. method can also supplied user-defined function; see method_user instructions examples. using weightit(), please cite WeightIt package (using citation(\"WeightIt\")) paper(s) references section method used.","code":""},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/reference/weightit.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Generate Balancing Weights — weightit","text":"Noah Greifer","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/weightit.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate Balancing Weights — weightit","text":"","code":"library(\"cobalt\") data(\"lalonde\", package = \"cobalt\")  #Balancing covariates between treatment groups (binary) (W1 <- weightit(treat ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"ps\", estimand = \"ATT\")) #> A weightit object #>  - method: \"ps\" (propensity score weighting) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 2-category #>  - estimand: ATT (focal: 1) #>  - covariates: age, educ, married, nodegree, re74 summary(W1) #>                  Summary of weights #>  #> - Weight ranges: #>  #>            Min                                  Max #> treated 1.0000               ||              1.0000 #> control 0.0222 |---------------------------| 2.0438 #>  #> - Units with the 5 most extreme weights by group: #>                                             #>              10      8      4      3      1 #>  treated      1      1      1      1      1 #>             411    595    269    409    296 #>  control 1.3303 1.4365 1.5005 1.6369 2.0438 #>  #> - Weight statistics: #>  #>         Coef of Var   MAD Entropy # Zeros #> treated       0.000 0.000   -0.00       0 #> control       0.823 0.701    0.33       0 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted  429.       185 #> Weighted    255.99     185 bal.tab(W1) #> Call #>  weightit(formula = treat ~ age + educ + married + nodegree +  #>     re74, data = lalonde, method = \"ps\", estimand = \"ATT\") #>  #> Balance Measures #>                Type Diff.Adj #> prop.score Distance   0.0199 #> age         Contin.   0.0459 #> educ        Contin.  -0.0360 #> married      Binary   0.0044 #> nodegree     Binary   0.0080 #> re74        Contin.  -0.0275 #>  #> Effective sample sizes #>            Control Treated #> Unadjusted  429.       185 #> Adjusted    255.99     185 #Balancing covariates with respect to race (multi-category) (W2 <- weightit(race ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"ebal\", estimand = \"ATE\")) #> A weightit object #>  - method: \"ebal\" (entropy balancing) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: 3-category (black, hispan, white) #>  - estimand: ATE #>  - covariates: age, educ, married, nodegree, re74 summary(W2) #>                  Summary of weights #>  #> - Weight ranges: #>  #>           Min                                  Max #> black  0.5530   |-------------------------| 5.3496 #> hispan 0.1409 |----------------|            3.3322 #> white  0.3979  |-------|                    1.9224 #>  #> - Units with the 5 most extreme weights by group: #>                                            #>            226    244    485    181    182 #>   black 2.5215 2.5491 2.8059 3.5551 5.3496 #>            392    564    269    345    371 #>  hispan 2.0467   2.53 2.6322 2.7049 3.3322 #>             68    457    599    589    531 #>   white 1.7106 1.7226 1.7426 1.7743 1.9224 #>  #> - Weight statistics: #>  #>        Coef of Var   MAD Entropy # Zeros #> black        0.590 0.413   0.131       0 #> hispan       0.609 0.440   0.163       0 #> white        0.371 0.306   0.068       0 #>  #> - Effective Sample Sizes: #>  #>             black hispan  white #> Unweighted 243.    72.   299.   #> Weighted   180.47  52.71 262.93 bal.tab(W2) #> Call #>  weightit(formula = race ~ age + educ + married + nodegree + re74,  #>     data = lalonde, method = \"ebal\", estimand = \"ATE\") #>  #> Balance summary across all treatment pairs #>             Type Max.Diff.Adj #> age      Contin.       0.0001 #> educ     Contin.       0.0000 #> married   Binary       0.0001 #> nodegree  Binary       0.0001 #> re74     Contin.       0.0001 #>  #> Effective sample sizes #>             black hispan  white #> Unadjusted 243.    72.   299.   #> Adjusted   180.47  52.71 262.93 #Balancing covariates with respect to re75 (continuous) (W3 <- weightit(re75 ~ age + educ + married +                   nodegree + re74, data = lalonde,                 method = \"cbps\", over = FALSE)) #> A weightit object #>  - method: \"cbps\" (covariate balancing propensity score weighting) #>  - number of obs.: 614 #>  - sampling weights: none #>  - treatment: continuous #>  - covariates: age, educ, married, nodegree, re74 summary(W3) #>                  Summary of weights #>  #> - Weight ranges: #>  #>        Min                                   Max #> all 0.0153 |---------------------------| 13.1539 #>  #> - Units with the 5 most extreme weights: #>                                             #>         484     482     180     481     483 #>  all 9.2281 10.8362 11.0895 11.9898 13.1539 #>  #> - Weight statistics: #>  #>     Coef of Var   MAD Entropy # Zeros #> all       1.151 0.449   0.288       0 #>  #> - Effective Sample Sizes: #>  #>             Total #> Unweighted 614.   #> Weighted   264.37 bal.tab(W3) #> Call #>  weightit(formula = re75 ~ age + educ + married + nodegree + re74,  #>     data = lalonde, method = \"cbps\", over = FALSE) #>  #> Balance Measures #>             Type Corr.Adj #> age      Contin.       -0 #> educ     Contin.       -0 #> married   Binary       -0 #> nodegree  Binary        0 #> re74     Contin.       -0 #>  #> Effective sample sizes #>             Total #> Unadjusted 614.   #> Adjusted   264.37"},{"path":"https://ngreifer.github.io/WeightIt/reference/weightitMSM.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Balancing Weights for Longitudinal Treatments — weightitMSM","title":"Generate Balancing Weights for Longitudinal Treatments — weightitMSM","text":"weightitMSM() allows easy generation balancing weights marginal structural models time-varying treatments using variety available methods binary, continuous, multinomial treatments. Many methods exist packages, weightit() calls; packages must installed use desired method. Also included print() summary() methods examining output. Currently \"wide\" data sets, row corresponds unit's entire variable history, supported. can use reshape() functions transform data format; see example .","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/weightitMSM.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Balancing Weights for Longitudinal Treatments — weightitMSM","text":"","code":"weightitMSM(formula.list,             data = NULL,             method = \"ps\",             stabilize = FALSE,             by = NULL,             s.weights = NULL,             num.formula = NULL,             moments = NULL,             int = FALSE,             missing = NULL,             verbose = FALSE,             include.obj = FALSE,             is.MSM.method,             weightit.force = FALSE,             ...)  # S3 method for weightitMSM print(x, ...)"},{"path":"https://ngreifer.github.io/WeightIt/reference/weightitMSM.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Balancing Weights for Longitudinal Treatments — weightitMSM","text":"formula.list list formulas corresponding time point time-specific treatment variable left hand side pre-treatment covariates balanced right hand side. formulas must temporal order, must contain covariates balanced time point (.e., treatments covariates featured early formulas appear later ones). Interactions functions covariates allowed. data optional data set form data frame contains variables formulas formula.list. must wide data set exactly one row per unit. method string length 1 containing name method used estimate weights. See weightit() allowable options. default \"ps\", estimates weights using generalized linear models. stabilize logical; whether stabilize weights. Stabilizing weights involves fitting model predicting treatment time point treatment status prior time points. TRUE, fully saturated model fit (.e., interactions treatments time point), essentially using observed treatment probabilities numerator (binary multinomial treatments). may yield error combinations observed. Default FALSE. manually specify stabilization model formulas, e.g., specify non-saturated models, use num.formula. many time points, saturated models may time-consuming impossible fit. num.formula optional; one-sided formula stabilization factors (previous treatments) right hand side, adds, time point, stabilization factors model saturated previous treatments. See Cole & Hernán (2008) discussion specify model; including stabilization factors can change estimand without proper adjustment, done caution. Can also list one-sided formulas, one time point. Unless know , recommend setting stabilize = TRUE ignoring num.formula. string containing name variable data weighting done within categories one-sided formula stratifying variable right-hand side. example, = \"gender\" = ~ gender, weights generated separately within level variable \"gender\". argument used called exact, still work message. one variable allowed. s.weights vector sampling weights name variable data contains sampling weights. ignored methods. moments numeric; methods, greatest power covariate balanced. example, moments = 3, non-categorical covariate, covariate, square, cube balanced. argument ignored methods; balance powers covariates, appropriate functions must entered formula. See specific methods help pages information whether accept moments. int logical; methods, whether first-order interactions covariates balanced. argument ignored methods; balance interactions variables, appropriate functions must entered formula. See specific methods help pages information whether accept int. missing character; missing data handled. options defaults depend method used. Ignored missing data present. noted multiple imputation outperforms available missingness methods available weightit() probably used instead. See MatchThem package use weightit() multiply imputed data. verbose whether print additional information output fitting function. include.obj whether include output list fit objects created process estimating weights time point. example, method = \"ps\", list glm objects containing propensity score models time point included. See help pages method information object included TRUE. .MSM.method whether method estimates weights multiple time points (TRUE) estimating weights time point multiplying together (FALSE). relevant method = \"optweight\"), estimates weights longitudinal treatments , user-specified functions. weightit.force several methods valid estimating weights longitudinal treatments, produce error message attempted. Set TRUE bypass error message. ... arguments functions called weightit() control aspects fitting covered arguments. See Details weightit(). x weightitMSM object; output call weightitMSM().","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/weightitMSM.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Balancing Weights for Longitudinal Treatments — weightitMSM","text":"weightitMSM object following elements: weights estimated weights, one unit. treat.list list values time-varying treatment variables. covs.list list covariates used fitting time point. includes raw covariates, may altered fitting process. data data.frame originally entered weightitMSM(). estimand \"ATE\", currently estimand MSMs binary multinomial treatments. method weight estimation method specified. ps.list list estimated propensity scores () time point. s.weights provided sampling weights. data.frame containing variable specified. stabilization stabilization factors, .","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/weightitMSM.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Generate Balancing Weights for Longitudinal Treatments — weightitMSM","text":"general, weightitMSM() works separating estimation weights separate procedures time period based formulas provided. formula, weightitMSM() simply calls weightit() formula, collects weights time period, multiplies together arrive longitudinal balancing weights. formula contain covariates balanced . example, formula corresponding second time period contain baseline covariates, treatment variable first time period, time-varying covariates took values first treatment second. Currently, wide data sets supported, unit represented exactly one row contains covariate treatment history encoded separate variables. \"cbps\" method, calls CBPS() CBPS, yield different results CBMSM() CBPS CBMSM() takes different approach generating weights simply estimating several time-specific models.","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/weightitMSM.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Generate Balancing Weights for Longitudinal Treatments — weightitMSM","text":"Noah Greifer","code":""},{"path":[]},{"path":"https://ngreifer.github.io/WeightIt/reference/weightitMSM.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Generate Balancing Weights for Longitudinal Treatments — weightitMSM","text":"Cole, S. R., & Hernán, M. . (2008). Constructing Inverse Probability Weights Marginal Structural Models. American Journal Epidemiology, 168(6), 656–664. doi:10.1093/aje/kwn164","code":""},{"path":"https://ngreifer.github.io/WeightIt/reference/weightitMSM.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate Balancing Weights for Longitudinal Treatments — weightitMSM","text":"","code":"library(\"twang\") library(\"cobalt\")  data(\"iptwExWide\", package = \"twang\") (W1 <- weightitMSM(list(tx1 ~ age + gender + use0,                       tx2 ~ tx1 + use1 + age + gender + use0,                       tx3 ~ tx2 + use2 + tx1 + use1 + age + gender + use0),                  data = iptwExWide,                  method = \"ps\")) #> A weightitMSM object #>  - method: \"ps\" (propensity score weighting) #>  - number of obs.: 1000 #>  - sampling weights: none #>  - number of time points: 3 (tx1, tx2, tx3) #>  - treatment:  #>     + time 1: 2-category #>     + time 2: 2-category #>     + time 3: 2-category #>  - covariates:  #>     + baseline: age, gender, use0 #>     + after time 1: tx1, use1, age, gender, use0 #>     + after time 2: tx2, use2, tx1, use1, age, gender, use0 summary(W1) #>                  Summary of weights #>  #>                        Time 1                        #>                  Summary of weights #>  #> - Weight ranges: #>  #>            Min                                   Max #> treated 1.6667 |---|                         15.5292 #> control 2.6123 |---------------------------| 82.2349 #>  #> - Units with the 5 most extreme weights by group: #>                                                  #>              477     442     307     409     906 #>  treated 13.4464 13.6233 13.7059  14.436 15.5292 #>              206     282     641     547     980 #>  control 44.8027 46.1355 58.4164 79.8919 82.2349 #>  #> - Weight statistics: #>  #>         Coef of Var   MAD Entropy # Zeros #> treated       0.481 0.376   0.117       0 #> control       0.768 0.494   0.222       0 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted  294.     706.  #> Weighted    185.18   573.6 #>  #>                        Time 2                        #>                  Summary of weights #>  #> - Weight ranges: #>  #>            Min                                   Max #> treated 1.6667 |---------------------------| 82.2349 #> control 2.6123 |--------------------------|  79.8919 #>  #> - Units with the 5 most extreme weights by group: #>                                                  #>               34     561     553     641     980 #>  treated  33.371 34.7405 42.9442 58.4164 82.2349 #>              109      95     206     282     547 #>  control 39.7862 42.2256 44.8027 46.1355 79.8919 #>  #> - Weight statistics: #>  #>         Coef of Var   MAD Entropy # Zeros #> treated       0.960 0.653   0.336       0 #> control       0.737 0.384   0.159       0 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted   492.   508.   #> Weighted     318.9  264.49 #>  #>                        Time 3                        #>                  Summary of weights #>  #> - Weight ranges: #>  #>            Min                                   Max #> treated 1.6667 |-------------|               44.8027 #> control 2.6123 |---------------------------| 82.2349 #>  #> - Units with the 5 most extreme weights by group: #>                                                  #>              936     650     763     109     206 #>  treated  26.003  26.471 28.0582 39.7862 44.8027 #>              553     282     641     547     980 #>  control 42.9442 46.1355 58.4164 79.8919 82.2349 #>  #> - Weight statistics: #>  #>         Coef of Var   MAD Entropy # Zeros #> treated       0.773 0.565   0.245       0 #> control       0.873 0.469   0.227       0 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted  415.     585.  #> Weighted    235.67   366.4 #>  bal.tab(W1) #> Call #>  weightitMSM(formula.list = list(tx1 ~ age + gender + use0, tx2 ~  #>     tx1 + use1 + age + gender + use0, tx3 ~ tx2 + use2 + tx1 +  #>     use1 + age + gender + use0), data = iptwExWide, method = \"ps\") #>  #> Balance summary across all time points #>              Times     Type Max.Diff.Adj #> prop.score 1, 2, 3 Distance       0.0251 #> age        1, 2, 3  Contin.       0.0703 #> gender     1, 2, 3   Binary       0.0263 #> use0       1, 2, 3  Contin.       0.0558 #> tx1           2, 3   Binary       0.0171 #> use1          2, 3  Contin.       0.0316 #> tx2              3   Binary       0.0085 #> use2             3  Contin.       0.0315 #>  #> Effective sample sizes #>  - Time 1 #>            Control Treated #> Unadjusted  294.     706.  #> Adjusted    185.18   573.6 #>  - Time 2 #>            Control Treated #> Unadjusted   492.   508.   #> Adjusted     318.9  264.49 #>  - Time 3 #>            Control Treated #> Unadjusted  415.     585.  #> Adjusted    235.67   366.4  ##Going from long to wide data data(\"iptwExLong\", package = \"twang\") wide_data <- reshape(iptwExLong$covariates,    #long data                      timevar = \"time\",         #time variable                      v.names = c(\"use\", \"tx\"), #time-varying                      idvar = \"ID\",             #time-stable                      direction = \"wide\",                      sep = \"\")  (W2 <- weightitMSM(list(tx1 ~ age + gender + use1,                       tx2 ~ tx1 + use2 + age + gender + use1,                       tx3 ~ tx2 + use3 + tx1 + use2 + age +                           gender + use1),                  data = wide_data,                  method = \"ps\")) #> A weightitMSM object #>  - method: \"ps\" (propensity score weighting) #>  - number of obs.: 1000 #>  - sampling weights: none #>  - number of time points: 3 (tx1, tx2, tx3) #>  - treatment:  #>     + time 1: 2-category #>     + time 2: 2-category #>     + time 3: 2-category #>  - covariates:  #>     + baseline: age, gender, use1 #>     + after time 1: tx1, use2, age, gender, use1 #>     + after time 2: tx2, use3, tx1, use2, age, gender, use1 summary(W2) #>                  Summary of weights #>  #>                        Time 1                        #>                  Summary of weights #>  #> - Weight ranges: #>  #>            Min                                   Max #> treated 1.6667 |---|                         15.5292 #> control 2.6123 |---------------------------| 82.2349 #>  #> - Units with the 5 most extreme weights by group: #>                                                  #>              477     442     307     409     906 #>  treated 13.4464 13.6233 13.7059  14.436 15.5292 #>              206     282     641     547     980 #>  control 44.8027 46.1355 58.4164 79.8919 82.2349 #>  #> - Weight statistics: #>  #>         Coef of Var   MAD Entropy # Zeros #> treated       0.481 0.376   0.117       0 #> control       0.768 0.494   0.222       0 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted  294.     706.  #> Weighted    185.18   573.6 #>  #>                        Time 2                        #>                  Summary of weights #>  #> - Weight ranges: #>  #>            Min                                   Max #> treated 1.6667 |---------------------------| 82.2349 #> control 2.6123 |--------------------------|  79.8919 #>  #> - Units with the 5 most extreme weights by group: #>                                                  #>               34     561     553     641     980 #>  treated  33.371 34.7405 42.9442 58.4164 82.2349 #>              109      95     206     282     547 #>  control 39.7862 42.2256 44.8027 46.1355 79.8919 #>  #> - Weight statistics: #>  #>         Coef of Var   MAD Entropy # Zeros #> treated       0.960 0.653   0.336       0 #> control       0.737 0.384   0.159       0 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted   492.   508.   #> Weighted     318.9  264.49 #>  #>                        Time 3                        #>                  Summary of weights #>  #> - Weight ranges: #>  #>            Min                                   Max #> treated 1.6667 |-------------|               44.8027 #> control 2.6123 |---------------------------| 82.2349 #>  #> - Units with the 5 most extreme weights by group: #>                                                  #>              936     650     763     109     206 #>  treated  26.003  26.471 28.0582 39.7862 44.8027 #>              553     282     641     547     980 #>  control 42.9442 46.1355 58.4164 79.8919 82.2349 #>  #> - Weight statistics: #>  #>         Coef of Var   MAD Entropy # Zeros #> treated       0.773 0.565   0.245       0 #> control       0.873 0.469   0.227       0 #>  #> - Effective Sample Sizes: #>  #>            Control Treated #> Unweighted  415.     585.  #> Weighted    235.67   366.4 #>   all.equal(W1$weights, W2$weights) #> [1] TRUE  #Using stabilization factors W3 <- weightitMSM(list(tx1 ~ age + gender + use0,                        tx2 ~ tx1 + use1 + age + gender + use0,                        tx3 ~ tx2 + use2 + tx1 + use1 + age + gender + use0),                   data = iptwExWide,                   method = \"ps\",                   stabilize = TRUE,                   num.formula = list(~ 1,                                      ~ tx1,                                      ~ tx2 + tx1))  #Same as above but with fully saturated stabilization factors #(i.e., making the last entry in 'num.formula' tx2*tx2) W4 <- weightitMSM(list(tx1 ~ age + gender + use0,                        tx2 ~ tx1 + use1 + age + gender + use0,                        tx3 ~ tx2 + use2 + tx1 + use1 + age + gender + use0),                   data = iptwExWide,                   method = \"ps\",                   stabilize = TRUE)"},{"path":"https://ngreifer.github.io/WeightIt/news/index.html","id":"weightit-development-version","dir":"Changelog","previous_headings":"","what":"WeightIt (development version)","title":"WeightIt (development version)","text":"Added energy balancing continuous treatments, requested using method = \"energy\", described Huling et al. (2021). weights minimize distance covariance treatment covariates maintaining representativeness. method supports exact balance constraints, distributional balance constraints, sampling weights. implementation similar independenceWeights package. See ?method_energy details. Added new stop.method continuous treatments, \"distance.cov\", finds weights minimize distance covariance treatment covariates. Added new stop.methods binary continuous treatments: \"r2.2\" \"r2.3\", find weights minimize R2 weighted model covariates plus squares squares cubes, respectively, predictors. Using method = \"ebcw\" empirical balancing calibration weighting longer available ATE package removed. Use method = \"ebal\" entropy balancing instead, essentially identical. Updated trim() documentation clarify form trimming implemented (.e., winsorizing). Suggested David Novgorodsky. Fixed bug using method = \"energy\" . Previously, stop.method = \"r2\" fail include intercept weighted model used compute R2. Now intercept included.","code":""},{"path":"https://ngreifer.github.io/WeightIt/news/index.html","id":"weightit-0131","dir":"Changelog","previous_headings":"","what":"WeightIt 0.13.1","title":"WeightIt 0.13.1","text":"CRAN release: 2022-06-28 ordinal multi-category treatments, setting link = \"br.logit\" now uses brglm2::bracl() fit bias-reduced ordinal regression model. Added vignette “Installing Supporting Packages” explain install various packages might needed WeightIt use certain methods, including package CRAN. See vignette vignette(\"installing-packages\"). Fixed bug occur factor character predictor single level passed weightit(). Improved code entropy balancing, fixing bug using s.weights continuous treatment improving messages optimization fails converge. (#33) Improved robustness documentation missing packages. Updated logo, thanks Ben Stillerman.","code":""},{"path":"https://ngreifer.github.io/WeightIt/news/index.html","id":"weightit-0130","dir":"Changelog","previous_headings":"","what":"WeightIt 0.13.0","title":"WeightIt 0.13.0","text":"Fixed bug occur formula.tools package loaded, occur commonly logistf loaded. cause error treatment covariates must number units. (#25) Fixed bug info component included output weightit() using method = \"super\". Added ability specify num.formula list formulas weightitMSM(). primarily get around fact stabilize = TRUE, fully saturated model treatments used compute stabilization factor, , many time points, time-consuming may impossible (especially treatment combinations observed). Thanks @maellecoursonnais bringing issue (#27). ps.cont() retired since functionality available using weightit() method = \"gbm\" twangContinuous package. method = \"energy\", new argument, lambda, can supplied, puts penalty square weights control effective sample size. Typically needed can help balancing aggressive. method = \"energy\", min.w can now negative, allowing negative weights. method = \"energy\", dist.mat can now supplied name method compute distance matrix: \"scaled_euclidean\", \"mahalanobis\", \"euclidean\". Support negative weights added summary(). Negative weights possible (though default) using method = \"energy\" method = \"optweight\". Fixed bug glm() fail converge method = \"ps\" binary treatments due bad starting values. (#31) miss = \"saem\" can used method = \"ps\" missing values present covariates. Fixed bugs processing input formulas. error now thrown incorrect link supplied method = \"ps\".","code":""},{"path":"https://ngreifer.github.io/WeightIt/news/index.html","id":"weightit-0120","dir":"Changelog","previous_headings":"","what":"WeightIt 0.12.0","title":"WeightIt 0.12.0","text":"CRAN release: 2021-04-03 use method = \"twang\" retired now give error message. Use method = \"gbm\" nearly identical functionality options, detailed ?method_gbm. multinomial treatments link = \"logit\" (default), mclogit package installed, can requested estimating propensity score setting option use.mclogit = TRUE, uses mclogit::mblogit(). give results default, uses mlogit, can faster recommended. Added plot() method summary.weightitMSM objects functions just like plot.summary.weightit() time point. Fixed bug summary.weightit() labels top weights incorrect. Thanks Adam Lilly. Fixed bug sbps() using stochastic search (.e., full.search = FALSE 8 moderator levels). (#17) Fixed bug occur weights treatment group NA. Bad weights (.e., ) now produce warning rather error weights can diagnosed manually. (#18) Fixed bug using method = \"energy\" estimand = \"ATE\" improved = TRUE (default). -treatment energy distance contribution half ; now corrected. Added L1 median measure balance criterion. See ?stop.method details. Fixed bug logical treatments yield error. (#21) Fixed bug Warning: Deprecated appear sometimes purrr (part tidyverse) loaded. (#22) Thanks MrFlick StackOverflow solution.","code":""},{"path":"https://ngreifer.github.io/WeightIt/news/index.html","id":"weightit-0110","dir":"Changelog","previous_headings":"","what":"WeightIt 0.11.0","title":"WeightIt 0.11.0","text":"CRAN release: 2021-02-02 Added support estimating propensity scores using Bayesian additive regression trees (BART) method = \"bart\". method fits BART model treatment using functions dbarts package estimate propensity scores used weights. Binary, multinomial, continuous treatments supported. BART uses Bayesian priors hyperparameters, hyperparameter tuning necessary get well-performing predictions. Fixed bug using method = \"gbm\" stop.method = \"cv{#}\". Fixed bug setting estimand = \"ATC\" methods produce propensity score. past, output propensity score probability control group; now, probability treated group, estimands. affect weights. Setting method = \"twang\" now deprecated. Use method = \"gbm\" improved performance increased functionality. method = \"twang\" relies twang package; method = \"gbm\" calls gbm directly. Using method = \"ebal\" longer requires ebal package. Instead, optim() used, continuous treatments. Balance little better, options removed. using method = \"ebal\" continuous treatments, new argument, d.moments, can now specified. controls number moments covariate treatment distributions constrained weighted sample original sample. Vegetabile et al. (2020) recommend setting d.moments least 3 ensure generalizability reduce bias due effect modification. Made minor changes summary.weightit() plot.summary.weightit(). Fixed negative entropy computed. option use.mnlogit weightit() multi-category treatments method = \"ps\" removed mnlogit appears uncooperative. Fixed bug (#16) using method = \"cbps\" factor variables, thanks @danielebottigliengo. Fixed bug using binary factor treatments, thanks Darren Stewart. Cleaned documentation.","code":""},{"path":"https://ngreifer.github.io/WeightIt/news/index.html","id":"weightit-0102","dir":"Changelog","previous_headings":"","what":"WeightIt 0.10.2","title":"WeightIt 0.10.2","text":"CRAN release: 2020-08-27 Fixed bug treatment values accidentally switched methods.","code":""},{"path":"https://ngreifer.github.io/WeightIt/news/index.html","id":"weightit-0101","dir":"Changelog","previous_headings":"","what":"WeightIt 0.10.1","title":"WeightIt 0.10.1","text":"CRAN release: 2020-08-12 method = \"gbm\", added ability tune hyperparameters like interaction.depth distribution using criteria used select optimal tree. summary tuning results included info weightit output object. Fixed bug moments int ignored unless specified. Effective sample sizes now print two digits (believe , don’t need three) print cleanly whole numbers. Fixed bug using , thanks @frankpopham. (#11) Fixed bug using weightitMSM methods process int moments (though probably shouldn’t use anyway). Thanks Sven Reiger. Fixed bug using method = \"npcbps\" weights excessively small mistaken . weights now sum number units.","code":""},{"path":"https://ngreifer.github.io/WeightIt/news/index.html","id":"weightit-0100","dir":"Changelog","previous_headings":"","what":"WeightIt 0.10.0","title":"WeightIt 0.10.0","text":"CRAN release: 2020-07-07 Added support energy balancing method = \"energy\". method minimizes energy distance samples, multivariate distance measure. method uses code written specifically WeightIt (.e., call package specifically designed energy balancing) using osqp package optimization (optweight). See Huling & Mak (2020) details method. Also included option require exact balance moments covariates minimizing energy distance. method works binary multinomial treatments ATE, ATT, ATC. Sampling weights supported. method requires calculation manipulation distance matrix units, can slow /memory intensive large datasets. Improvements method = \"gbm\" method = \"super\" SL.method = \"method.balance\". new suite stop.methods allowed. binary treatments, include energy distance, sample Mahalanobis distance, pseudo-R2 weighted treatment model, among others. See ?stop.method allowable options. addition, performance quite bit faster. multinomial treatments link = \"logit\" (default), mnlogit package installed, can requested estimating propensity score setting option use.mnlogit = TRUE. give results default, uses mlogit, can faster large datasets. Added option estimand = \"ATOS\" “optimal subset” treatment effect described Crump et al. (2009). estimand finds subset units , ATE weights applied, yields treatment effect lowest variance, assuming homoscedasticity (assumptions). available binary treatments method = \"ps\". general makes sense use estimand = \"ATO\" want low-variance estimate don’t care target population, added completeness. available get_w_from_ps() well. make_full_rank() now faster. Cleaning error messages. Fixed bug using link = \"log\" method = \"ps\" binary treatments. Fixed bug using method = \"cbps\" continuous treatments sampling weights. Previously returned weights included sampling weights multiplied ; now separated, scenarios methods. Improved processing non-0/1 binary treatments, including method = \"gbm\". guess made treatment considered “treated”; affects produced propensity scores weights. Changed default value trim() .99 0. Added output number weights equal zero summary.weightit. can especially helpful using \"optweight\" \"energy\" methods using estimand = \"ATOS\".","code":""},{"path":"https://ngreifer.github.io/WeightIt/news/index.html","id":"weightit-090","dir":"Changelog","previous_headings":"","what":"WeightIt 0.9.0","title":"WeightIt 0.9.0","text":"CRAN release: 2020-02-11 Added support entropy balancing (method = \"ebal\") continuous treatments described Tübbicke (2020). Relies hand-written code contributed Stefan Tübbicke rather another R package. Sampling weights base weights supported binary multi-category treatments. Added support Balance SuperLearner described Pirracchio Carone (2018) method = \"super\". Rather using NNLS choose optimal combination predictions, can now optimize balance. , set SL.method = \"method.balance\". need set argument stop.method, works identically method = \"gbm\". example, stop.method = \"es.max\", predicted values given combination predicted values minimizes largest absolute standardized mean difference covariates sample weighted using predicted values propensity scores. Changed statistics displayed using summary(): weight ratio gone (weights can 0, problematic explode ratio), mean absolute deviation entropy weights now present. Added crayon prettier printing summary() output.","code":""},{"path":"https://ngreifer.github.io/WeightIt/news/index.html","id":"weightit-080","dir":"Changelog","previous_headings":"","what":"WeightIt 0.8.0","title":"WeightIt 0.8.0","text":"CRAN release: 2020-01-12 Formula interfaces now accept poly(x, .) matrix-generating functions variables, including rms-class-generating functions rms package (e.g., pol(), rcs(), etc.) (rms package must loaded use latter ones) basis-class-generating functions splines package (.e., bs() ns()). bug early version found @ahinton-mmc. Added support marginal mean weighting stratification (MMWS) described Hong (2010, 2012) weightit() get_w_from_ps() subclass argument (see References ?get_w_from_ps). method, subclasses formed based propensity score weights computed based number units subclass. MMWS can used method produces propensity score. implementation ensures subclasses least one member filling empty subclasses neighboring units. Added stabilize option get_w_from_ps(). new missing argument added weightit() choose missing data covariates handled. methods, \"ind\" (.e., missing indicators single-value imputation) allowed, \"ps\", \"gbm\", \"twang\", methods possible. method = \"ps\", stochastic approximation EM algorithm (SAEM) can used misaem package setting missing = \"saem\". continuous treatments \"ps\", \"gbm\", \"super\" methods (.e., conditional density treatment needs estimated), user can now supply density string function rather using normal density kernel density estimation. example, use density t-distribution 3 degrees freedom, one can set density = \"dt_3\". T-distributions often work better normal distributions extreme values treatment. methods now info component output object. contains information might useful diagnosing reporting method. example, method = \"gbm\", info contains tree used compute weights balance resulting trees, can plotted using plot(). method = \"super\", info contains coefficients stacking model cross-validation risk component methods. method = \"gbm\", best tree can chosen using cross validation rather balance setting stop.method = \"cv5\", e.g., 5-fold cross-validation. method = \"gbm\", new optional argument start.tree can set select tree balance begins computed. can speed things know best tree within first 100 trees, example. using method = \"gbm\" multi-category treatments estimands ATE, ATT, ATC used standardized mean differences stopping rule, mean differences weighted overall sample treatment group. Otherwise, efficiency improvements. using method = \"ps\" multi-category treatments, use use.mlogit = FALSE request multiple binary regressions instead multinomial regression now documented associated bug now fixed, thanks @ahinton-mmc. use method = \"super\", one can now set discrete = TRUE use discrete SuperLearner instead stacked SuperLearner, probably shouldn’t. moments int can now used method = \"npcbps\". Performance enhancements.","code":""},{"path":"https://ngreifer.github.io/WeightIt/news/index.html","id":"weightit-071","dir":"Changelog","previous_headings":"","what":"WeightIt 0.7.1","title":"WeightIt 0.7.1","text":"CRAN release: 2019-10-30 Fixed bug using weightit() inside another function passed argument explicitly. Also changed syntax ; must now either string (always possible) one-sided formula stratifying variable right-hand side. use variable data, must use formula interface. Fixed bug trying use ps weightit().","code":""},{"path":"https://ngreifer.github.io/WeightIt/news/index.html","id":"weightit-070","dir":"Changelog","previous_headings":"","what":"WeightIt 0.7.0","title":"WeightIt 0.7.0","text":"CRAN release: 2019-10-16 Added new sbps() function estimating subgroup balancing propensity score weights, including standard method new smooth version. Setting method = \"gbm\" method = \"twang\" now two different things. method = \"gbm\" uses gbm cobalt functions estimate weights much faster, method = \"twang\" uses twang functions estimate weights. results similar two methods. Prior version, method = \"gbm\" method = \"twang\" method = \"twang\" now. Bug fixes stabilize = TRUE, thanks @ulriksartipy Sven Rieger. Fixes using base.weight argument method = \"ebal\". Now supplied vector length equal number units dataset (contrast use ebalance, requires length equal number control units). Restored dependency cobalt examples vignette. method = \"ps\" treatment ordered (.e., ordinal), MASS::polr() used fit ordinal regression. Make treatment un-ordered use multinomial regression instead. Added support using bias-reduced fitting functions method = \"ps\" provided brglm2 package. can accessed changing link , example, \"br.logit\" \"br.probit\". multinomial treatments, setting link = \"br.logit\" fits bias-reduced multinomial regression model using brglm2::brmultinom(). can helpful regular maximum likelihood models fail converge, though may also sign lack overlap.","code":""},{"path":"https://ngreifer.github.io/WeightIt/news/index.html","id":"weightit-060","dir":"Changelog","previous_headings":"","what":"WeightIt 0.6.0","title":"WeightIt 0.6.0","text":"CRAN release: 2019-09-05 Bug fixes. Functions now work better used inside functions (e.g., lapply). Behavior weightit() presence non-NULL focal changed. focal specified, estimand assumed ATT. Previously, focal ignored unless estimand = \"ATT\". Processing estimand focal improved. Functions smarter guessing group focal group one isn’t specified, especially non-numeric treatments. focal can now used estimand = \"ATC\" indicate group control group, \"ATC\" \"ATT\" now function similarly. Added function get_w_from_ps() transform propensity scores weights (instead go weightit()). Added functions .weightit() .weightitMSM() convert weights treatments components weightit objects summary.weightit() can used . Updated documentation describe missing data covariates handled. bugs related missing data fixed well, thanks Yong Hao Pua. ps.cont() “z-transformed correlation” options removed simplify output. function supporting functions deprecated soon new version twang released. using method = \"ps\" method = \"super\" continuous treatments, setting use.kernel = TRUE plot = TRUE, plot now made ggplot2 rather base R plots. Added plot.summary.weightit() plot distribution weights (feature also optweight). Removed dependency cobalt temporarily, means examples vignette won’t run. Added ggplot2 Imports.","code":""},{"path":"https://ngreifer.github.io/WeightIt/news/index.html","id":"weightit-051","dir":"Changelog","previous_headings":"","what":"WeightIt 0.5.1","title":"WeightIt 0.5.1","text":"CRAN release: 2019-01-16 Fixed bug using ps argument weightit(). Fixed bug setting include.obj = TRUE weightitMSM(). Added warnings using certain methods longitudinal treatments validated may lead incorrect inferences.","code":""},{"path":"https://ngreifer.github.io/WeightIt/news/index.html","id":"weightit-050","dir":"Changelog","previous_headings":"","what":"WeightIt 0.5.0","title":"WeightIt 0.5.0","text":"CRAN release: 2018-11-22 Added super method estimate propensity scores using SuperLearner package. Added optweight method estimate weights using optimization (probably just use optweight package). weightit() now uses correct formula estimate weights ATO multinomial treatments described Li & Li (2018). Added include.obj option weightit() weightitMSM() include fitted object output object inspection. example, method = \"ps\", glm object containing propensity score model included output. Rearranged help pages. method now documentation page, linked weightit help page. Propensity scores now included output binary treatments gbm cbps methods. Thanks @Blanch-Font suggestion. bug fixes minor changes.","code":""},{"path":"https://ngreifer.github.io/WeightIt/news/index.html","id":"weightit-040","dir":"Changelog","previous_headings":"","what":"WeightIt 0.4.0","title":"WeightIt 0.4.0","text":"CRAN release: 2018-06-25 Added trim() function trim weights. Added ps.cont() function, estimates generalized propensity score weights continuous treatments using generalized boosted modeling, twang. function uses syntax ps() twang, can also accessed using weightit() method = \"gbm\". Support functions added make compatible twang functions assessing balance (e.g., summary, bal.table, plot). Thanks Donna Coffman enlightening method providing code implement . input formula now much forgiving, allowing objects environment included. data argument weightit() now optional. simplify things, output object longer contains data field. --hood changes facilitate adding new features debugging. aspects output objects slightly changed, shouldn’t affect use users. Fixed bug variables thrown method = \"ebal\".","code":""},{"path":"https://ngreifer.github.io/WeightIt/news/index.html","id":"weightit-032","dir":"Changelog","previous_headings":"","what":"WeightIt 0.3.2","title":"WeightIt 0.3.2","text":"CRAN release: 2018-03-14 Added new moments int options weightit() methods easily specify moments interactions covariates. Fixed bug using objects data set weightit(). Behavior changed include transformed covariates entered formula weightit() output. Fixed bug resulting potentially colinearity using ebal ebcw. Added vignette.","code":""},{"path":"https://ngreifer.github.io/WeightIt/news/index.html","id":"weightit-031","dir":"Changelog","previous_headings":"","what":"WeightIt 0.3.1","title":"WeightIt 0.3.1","text":"CRAN release: 2018-03-03 Edits code help files protect missing CBPS package. Corrected sampling weights functionality work correctly. Also expanded sampling weights able used methods, including natively allow sampling weights (e.g., ATE). Minor bug fixes spelling corrections.","code":""},{"path":"https://ngreifer.github.io/WeightIt/news/index.html","id":"weightit-030","dir":"Changelog","previous_headings":"","what":"WeightIt 0.3.0","title":"WeightIt 0.3.0","text":"CRAN release: 2018-01-14 Added weightitMSM() function (supporting print() summary() functions) estimate weights marginal structural models time-varying treatments covariates. Fixed bugs, including using CBPS continuous treatments, using focal incorrectly.","code":""},{"path":"https://ngreifer.github.io/WeightIt/news/index.html","id":"weightit-020","dir":"Changelog","previous_headings":"","what":"WeightIt 0.2.0","title":"WeightIt 0.2.0","text":"CRAN release: 2017-11-12 Added method = \"sbw\" stable balancing weights (now removed replaced method = \"optweight\") Allowed estimation multinomial propensity scores using multiple binary regressions mlogit installed Allowed estimation multinomial CBPS using multiple binary CBPS 4 groups Added README NEWS","code":""},{"path":"https://ngreifer.github.io/WeightIt/news/index.html","id":"weightit-010","dir":"Changelog","previous_headings":"","what":"WeightIt 0.1.0","title":"WeightIt 0.1.0","text":"CRAN release: 2017-10-17 First version!","code":""}]
